
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.0.5">
    
    
      
        <title>API reference - slp Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.77f3fd56.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.7fa14f5b.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#api-reference" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="slp Documentation" class="md-header__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            slp Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              API reference
            
          </span>
        </div>
      </div>
    </div>
    <div class="md-header__options">
      
    </div>
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="slp Documentation" class="md-nav__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    slp Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        slp
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../get-started/" class="md-nav__link">
        Getting started
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../tuning/" class="md-nav__link">
        Hyperparameter tuning
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../config/" class="md-nav__link">
        Configuration
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../data-utils/" class="md-nav__link">
        Data manipulation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../plbind/" class="md-nav__link">
        Pytorch Lightning Bindings
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../modules/" class="md-nav__link">
        Generic Modules
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../multimodal/" class="md-nav__link">
        Multimodal Modules
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        SLP utility functions
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          API reference
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        API reference
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slp.config.config_parser" class="md-nav__link">
    slp.config.config_parser
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.config_parser.generate_example_config" class="md-nav__link">
    generate_example_config()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.config_parser.make_cli_parser" class="md-nav__link">
    make_cli_parser()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.config_parser.parse_config" class="md-nav__link">
    parse_config()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.nlp" class="md-nav__link">
    slp.config.nlp
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.nlp.SPECIAL_TOKENS" class="md-nav__link">
    SPECIAL_TOKENS
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.omegaconf" class="md-nav__link">
    slp.config.omegaconf
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.omegaconf.OmegaConfExtended" class="md-nav__link">
    OmegaConfExtended
  </a>
  
    <nav class="md-nav" aria-label="OmegaConfExtended">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.config.omegaconf.OmegaConfExtended.from_argparse" class="md-nav__link">
    from_argparse()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.config.omegaconf.OmegaConfExtended.from_yaml" class="md-nav__link">
    from_yaml()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators" class="md-nav__link">
    slp.data.collators
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator" class="md-nav__link">
    MultimodalSequenceClassificationCollator
  </a>
  
    <nav class="md-nav" aria-label="MultimodalSequenceClassificationCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator" class="md-nav__link">
    Seq2SeqCollator
  </a>
  
    <nav class="md-nav" aria-label="Seq2SeqCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator" class="md-nav__link">
    SequenceClassificationCollator
  </a>
  
    <nav class="md-nav" aria-label="SequenceClassificationCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus" class="md-nav__link">
    slp.data.corpus
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader" class="md-nav__link">
    EmbeddingsLoader
  </a>
  
    <nav class="md-nav" aria-label="EmbeddingsLoader">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.augment_embeddings" class="md-nav__link">
    augment_embeddings()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.in_accepted_vocab" class="md-nav__link">
    in_accepted_vocab()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.load" class="md-nav__link">
    load()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus" class="md-nav__link">
    HfCorpus
  </a>
  
    <nav class="md-nav" aria-label="HfCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus" class="md-nav__link">
    TokenizedCorpus
  </a>
  
    <nav class="md-nav" aria-label="TokenizedCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus" class="md-nav__link">
    WordCorpus
  </a>
  
    <nav class="md-nav" aria-label="WordCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.create_vocab" class="md-nav__link">
    create_vocab()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets" class="md-nav__link">
    slp.data.datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset" class="md-nav__link">
    CorpusDataset
  </a>
  
    <nav class="md-nav" aria-label="CorpusDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.map" class="md-nav__link">
    map()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset" class="md-nav__link">
    CorpusLMDataset
  </a>
  
    <nav class="md-nav" aria-label="CorpusLMDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.map" class="md-nav__link">
    map()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms" class="md-nav__link">
    slp.data.transforms
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer" class="md-nav__link">
    HuggingFaceTokenizer
  </a>
  
    <nav class="md-nav" aria-label="HuggingFaceTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.detokenize" class="md-nav__link">
    detokenize()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken" class="md-nav__link">
    ReplaceUnknownToken
  </a>
  
    <nav class="md-nav" aria-label="ReplaceUnknownToken">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer" class="md-nav__link">
    SentencepieceTokenizer
  </a>
  
    <nav class="md-nav" aria-label="SentencepieceTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer" class="md-nav__link">
    SpacyTokenizer
  </a>
  
    <nav class="md-nav" aria-label="SpacyTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.get_nlp" class="md-nav__link">
    get_nlp()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor" class="md-nav__link">
    ToTensor
  </a>
  
    <nav class="md-nav" aria-label="ToTensor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds" class="md-nav__link">
    ToTokenIds
  </a>
  
    <nav class="md-nav" aria-label="ToTokenIds">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention" class="md-nav__link">
    slp.modules.attention
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention" class="md-nav__link">
    Attention
  </a>
  
    <nav class="md-nav" aria-label="Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention" class="md-nav__link">
    MultiheadAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention" class="md-nav__link">
    MultiheadSelfAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadSelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention" class="md-nav__link">
    MultiheadTwowayAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadTwowayAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention" class="md-nav__link">
    SelfAttention
  </a>
  
    <nav class="md-nav" aria-label="SelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.attention" class="md-nav__link">
    attention()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.attention_scores" class="md-nav__link">
    attention_scores()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.merge_heads" class="md-nav__link">
    merge_heads()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.nystrom_attention" class="md-nav__link">
    nystrom_attention()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.pad_for_nystrom" class="md-nav__link">
    pad_for_nystrom()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.reset_parameters" class="md-nav__link">
    reset_parameters()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.split_heads" class="md-nav__link">
    split_heads()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier" class="md-nav__link">
    slp.modules.classifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier" class="md-nav__link">
    Classifier
  </a>
  
    <nav class="md-nav" aria-label="Classifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.MOSEITextClassifier" class="md-nav__link">
    MOSEITextClassifier
  </a>
  
    <nav class="md-nav" aria-label="MOSEITextClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.MOSEITextClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.RNNLateFusionClassifier" class="md-nav__link">
    RNNLateFusionClassifier
  </a>
  
    <nav class="md-nav" aria-label="RNNLateFusionClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.RNNLateFusionClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.TransformerLateFusionClassifier" class="md-nav__link">
    TransformerLateFusionClassifier
  </a>
  
    <nav class="md-nav" aria-label="TransformerLateFusionClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.TransformerLateFusionClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed" class="md-nav__link">
    slp.modules.embed
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed" class="md-nav__link">
    Embed
  </a>
  
    <nav class="md-nav" aria-label="Embed">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.init_embeddings" class="md-nav__link">
    init_embeddings()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding" class="md-nav__link">
    PositionalEncoding
  </a>
  
    <nav class="md-nav" aria-label="PositionalEncoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward" class="md-nav__link">
    slp.modules.feedforward
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF" class="md-nav__link">
    PositionwiseFF
  </a>
  
    <nav class="md-nav" aria-label="PositionwiseFF">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward.TwoLayer" class="md-nav__link">
    TwoLayer
  </a>
  
    <nav class="md-nav" aria-label="TwoLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.TwoLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm" class="md-nav__link">
    slp.modules.norm
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf" class="md-nav__link">
    LayerNormTf
  </a>
  
    <nav class="md-nav" aria-label="LayerNormTf">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm.ScaleNorm" class="md-nav__link">
    ScaleNorm
  </a>
  
    <nav class="md-nav" aria-label="ScaleNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.ScaleNorm.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.regularization" class="md-nav__link">
    slp.modules.regularization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise" class="md-nav__link">
    GaussianNoise
  </a>
  
    <nav class="md-nav" aria-label="GaussianNoise">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn" class="md-nav__link">
    slp.modules.rnn
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN" class="md-nav__link">
    AttentiveRNN
  </a>
  
    <nav class="md-nav" aria-label="AttentiveRNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN" class="md-nav__link">
    RNN
  </a>
  
    <nav class="md-nav" aria-label="RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN" class="md-nav__link">
    TokenRNN
  </a>
  
    <nav class="md-nav" aria-label="TokenRNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer" class="md-nav__link">
    slp.modules.transformer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Decoder" class="md-nav__link">
    Decoder
  </a>
  
    <nav class="md-nav" aria-label="Decoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Decoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.DecoderLayer" class="md-nav__link">
    DecoderLayer
  </a>
  
    <nav class="md-nav" aria-label="DecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.DecoderLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Encoder" class="md-nav__link">
    Encoder
  </a>
  
    <nav class="md-nav" aria-label="Encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Encoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderDecoder" class="md-nav__link">
    EncoderDecoder
  </a>
  
    <nav class="md-nav" aria-label="EncoderDecoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderDecoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderLayer" class="md-nav__link">
    EncoderLayer
  </a>
  
    <nav class="md-nav" aria-label="EncoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer1" class="md-nav__link">
    Sublayer1
  </a>
  
    <nav class="md-nav" aria-label="Sublayer1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer1.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer2" class="md-nav__link">
    Sublayer2
  </a>
  
    <nav class="md-nav" aria-label="Sublayer2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer2.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer3" class="md-nav__link">
    Sublayer3
  </a>
  
    <nav class="md-nav" aria-label="Sublayer3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer3.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Transformer" class="md-nav__link">
    Transformer
  </a>
  
    <nav class="md-nav" aria-label="Transformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Transformer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerSequenceEncoder" class="md-nav__link">
    TransformerSequenceEncoder
  </a>
  
    <nav class="md-nav" aria-label="TransformerSequenceEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerSequenceEncoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerTokenSequenceEncoder" class="md-nav__link">
    TransformerTokenSequenceEncoder
  </a>
  
    <nav class="md-nav" aria-label="TransformerTokenSequenceEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerTokenSequenceEncoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.reset_parameters" class="md-nav__link">
    reset_parameters()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.dm" class="md-nav__link">
    slp.plbind.dm
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromCorpus" class="md-nav__link">
    PLDataModuleFromCorpus
  </a>
  
    <nav class="md-nav" aria-label="PLDataModuleFromCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromCorpus.add_argparse_args" class="md-nav__link">
    add_argparse_args()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets" class="md-nav__link">
    PLDataModuleFromDatasets
  </a>
  
    <nav class="md-nav" aria-label="PLDataModuleFromDatasets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.add_argparse_args" class="md-nav__link">
    add_argparse_args()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.prepare_data" class="md-nav__link">
    prepare_data()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.test_dataloader" class="md-nav__link">
    test_dataloader()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.train_dataloader" class="md-nav__link">
    train_dataloader()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.val_dataloader" class="md-nav__link">
    val_dataloader()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.dm.split_data" class="md-nav__link">
    split_data()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.helpers" class="md-nav__link">
    slp.plbind.helpers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FixedWandbLogger" class="md-nav__link">
    FixedWandbLogger
  </a>
  
    <nav class="md-nav" aria-label="FixedWandbLogger">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FixedWandbLogger.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FixedWandbLogger.finalize" class="md-nav__link">
    finalize()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FromLogits" class="md-nav__link">
    FromLogits
  </a>
  
    <nav class="md-nav" aria-label="FromLogits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FromLogits.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FromLogits.compute" class="md-nav__link">
    compute()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FromLogits.update" class="md-nav__link">
    update()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module" class="md-nav__link">
    slp.plbind.module
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.AutoEncoderPLModule" class="md-nav__link">
    AutoEncoderPLModule
  </a>
  
    <nav class="md-nav" aria-label="AutoEncoderPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.AutoEncoderPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.BertPLModule" class="md-nav__link">
    BertPLModule
  </a>
  
    <nav class="md-nav" aria-label="BertPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.BertPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.MultimodalTransformerClassificationPLModule" class="md-nav__link">
    MultimodalTransformerClassificationPLModule
  </a>
  
    <nav class="md-nav" aria-label="MultimodalTransformerClassificationPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.MultimodalTransformerClassificationPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.PLModule" class="md-nav__link">
    PLModule
  </a>
  
    <nav class="md-nav" aria-label="PLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.PLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.RnnPLModule" class="md-nav__link">
    RnnPLModule
  </a>
  
    <nav class="md-nav" aria-label="RnnPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.RnnPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule" class="md-nav__link">
    SimplePLModule
  </a>
  
    <nav class="md-nav" aria-label="SimplePLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.aggregate_epoch_metrics" class="md-nav__link">
    aggregate_epoch_metrics()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.configure_optimizers" class="md-nav__link">
    configure_optimizers()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.log_to_console" class="md-nav__link">
    log_to_console()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.test_epoch_end" class="md-nav__link">
    test_epoch_end()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.test_step" class="md-nav__link">
    test_step()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.training_epoch_end" class="md-nav__link">
    training_epoch_end()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.training_step" class="md-nav__link">
    training_step()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.validation_epoch_end" class="md-nav__link">
    validation_epoch_end()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.validation_step" class="md-nav__link">
    validation_step()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.TransformerClassificationPLModule" class="md-nav__link">
    TransformerClassificationPLModule
  </a>
  
    <nav class="md-nav" aria-label="TransformerClassificationPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.TransformerClassificationPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.TransformerPLModule" class="md-nav__link">
    TransformerPLModule
  </a>
  
    <nav class="md-nav" aria-label="TransformerPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.TransformerPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer" class="md-nav__link">
    slp.plbind.trainer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer.add_optimizer_args" class="md-nav__link">
    add_optimizer_args()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer.add_trainer_args" class="md-nav__link">
    add_trainer_args()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer.make_trainer" class="md-nav__link">
    make_trainer()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer.make_trainer_for_ray_tune" class="md-nav__link">
    make_trainer_for_ray_tune()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer.watch_model" class="md-nav__link">
    watch_model()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.log" class="md-nav__link">
    slp.util.log
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.log.configure_logging" class="md-nav__link">
    configure_logging()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.log.log_to_file" class="md-nav__link">
    log_to_file()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch" class="md-nav__link">
    slp.util.pytorch
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.NoOp" class="md-nav__link">
    NoOp
  </a>
  
    <nav class="md-nav" aria-label="NoOp">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.util.pytorch.NoOp.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.PackSequence" class="md-nav__link">
    PackSequence
  </a>
  
    <nav class="md-nav" aria-label="PackSequence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.util.pytorch.PackSequence.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.util.pytorch.PackSequence.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.PadPackedSequence" class="md-nav__link">
    PadPackedSequence
  </a>
  
    <nav class="md-nav" aria-label="PadPackedSequence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.util.pytorch.PadPackedSequence.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.util.pytorch.PadPackedSequence.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.from_checkpoint" class="md-nav__link">
    from_checkpoint()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.mktensor" class="md-nav__link">
    mktensor()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.moore_penrose_pinv" class="md-nav__link">
    moore_penrose_pinv()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.pad_mask" class="md-nav__link">
    pad_mask()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.pad_sequence" class="md-nav__link">
    pad_sequence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.repeat_layer" class="md-nav__link">
    repeat_layer()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.rotate_tensor" class="md-nav__link">
    rotate_tensor()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.shift_tensor" class="md-nav__link">
    shift_tensor()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.sort_sequences" class="md-nav__link">
    sort_sequences()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.subsequent_mask" class="md-nav__link">
    subsequent_mask()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.t" class="md-nav__link">
    t()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.t_" class="md-nav__link">
    t_()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.to_device" class="md-nav__link">
    to_device()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system" class="md-nav__link">
    slp.util.system
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.date_fname" class="md-nav__link">
    date_fname()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.download_url" class="md-nav__link">
    download_url()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.has_internet_connection" class="md-nav__link">
    has_internet_connection()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.is_file" class="md-nav__link">
    is_file()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.is_subpath" class="md-nav__link">
    is_subpath()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.is_url" class="md-nav__link">
    is_url()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.json_dump" class="md-nav__link">
    json_dump()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.json_load" class="md-nav__link">
    json_load()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.pickle_dump" class="md-nav__link">
    pickle_dump()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.pickle_load" class="md-nav__link">
    pickle_load()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.print_separator" class="md-nav__link">
    print_separator()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.read_wav" class="md-nav__link">
    read_wav()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.run_cmd" class="md-nav__link">
    run_cmd()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.run_cmd_silent" class="md-nav__link">
    run_cmd_silent()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.safe_mkdirs" class="md-nav__link">
    safe_mkdirs()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.suppress_print" class="md-nav__link">
    suppress_print()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.timethis" class="md-nav__link">
    timethis()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.write_wav" class="md-nav__link">
    write_wav()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.yaml_dump" class="md-nav__link">
    yaml_dump()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.yaml_load" class="md-nav__link">
    yaml_load()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.types" class="md-nav__link">
    slp.util.types
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.types.dir_path" class="md-nav__link">
    dir_path()
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slp.config.config_parser" class="md-nav__link">
    slp.config.config_parser
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.config_parser.generate_example_config" class="md-nav__link">
    generate_example_config()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.config_parser.make_cli_parser" class="md-nav__link">
    make_cli_parser()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.config_parser.parse_config" class="md-nav__link">
    parse_config()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.nlp" class="md-nav__link">
    slp.config.nlp
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.nlp.SPECIAL_TOKENS" class="md-nav__link">
    SPECIAL_TOKENS
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.omegaconf" class="md-nav__link">
    slp.config.omegaconf
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.config.omegaconf.OmegaConfExtended" class="md-nav__link">
    OmegaConfExtended
  </a>
  
    <nav class="md-nav" aria-label="OmegaConfExtended">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.config.omegaconf.OmegaConfExtended.from_argparse" class="md-nav__link">
    from_argparse()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.config.omegaconf.OmegaConfExtended.from_yaml" class="md-nav__link">
    from_yaml()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators" class="md-nav__link">
    slp.data.collators
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator" class="md-nav__link">
    MultimodalSequenceClassificationCollator
  </a>
  
    <nav class="md-nav" aria-label="MultimodalSequenceClassificationCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator" class="md-nav__link">
    Seq2SeqCollator
  </a>
  
    <nav class="md-nav" aria-label="Seq2SeqCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator" class="md-nav__link">
    SequenceClassificationCollator
  </a>
  
    <nav class="md-nav" aria-label="SequenceClassificationCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus" class="md-nav__link">
    slp.data.corpus
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader" class="md-nav__link">
    EmbeddingsLoader
  </a>
  
    <nav class="md-nav" aria-label="EmbeddingsLoader">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.augment_embeddings" class="md-nav__link">
    augment_embeddings()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.in_accepted_vocab" class="md-nav__link">
    in_accepted_vocab()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.load" class="md-nav__link">
    load()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus" class="md-nav__link">
    HfCorpus
  </a>
  
    <nav class="md-nav" aria-label="HfCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus" class="md-nav__link">
    TokenizedCorpus
  </a>
  
    <nav class="md-nav" aria-label="TokenizedCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus" class="md-nav__link">
    WordCorpus
  </a>
  
    <nav class="md-nav" aria-label="WordCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.create_vocab" class="md-nav__link">
    create_vocab()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets" class="md-nav__link">
    slp.data.datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset" class="md-nav__link">
    CorpusDataset
  </a>
  
    <nav class="md-nav" aria-label="CorpusDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.map" class="md-nav__link">
    map()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset" class="md-nav__link">
    CorpusLMDataset
  </a>
  
    <nav class="md-nav" aria-label="CorpusLMDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.map" class="md-nav__link">
    map()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms" class="md-nav__link">
    slp.data.transforms
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer" class="md-nav__link">
    HuggingFaceTokenizer
  </a>
  
    <nav class="md-nav" aria-label="HuggingFaceTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.detokenize" class="md-nav__link">
    detokenize()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken" class="md-nav__link">
    ReplaceUnknownToken
  </a>
  
    <nav class="md-nav" aria-label="ReplaceUnknownToken">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer" class="md-nav__link">
    SentencepieceTokenizer
  </a>
  
    <nav class="md-nav" aria-label="SentencepieceTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer" class="md-nav__link">
    SpacyTokenizer
  </a>
  
    <nav class="md-nav" aria-label="SpacyTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.get_nlp" class="md-nav__link">
    get_nlp()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor" class="md-nav__link">
    ToTensor
  </a>
  
    <nav class="md-nav" aria-label="ToTensor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds" class="md-nav__link">
    ToTokenIds
  </a>
  
    <nav class="md-nav" aria-label="ToTokenIds">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention" class="md-nav__link">
    slp.modules.attention
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention" class="md-nav__link">
    Attention
  </a>
  
    <nav class="md-nav" aria-label="Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention" class="md-nav__link">
    MultiheadAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention" class="md-nav__link">
    MultiheadSelfAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadSelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention" class="md-nav__link">
    MultiheadTwowayAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadTwowayAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention" class="md-nav__link">
    SelfAttention
  </a>
  
    <nav class="md-nav" aria-label="SelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.attention" class="md-nav__link">
    attention()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.attention_scores" class="md-nav__link">
    attention_scores()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.merge_heads" class="md-nav__link">
    merge_heads()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.nystrom_attention" class="md-nav__link">
    nystrom_attention()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.pad_for_nystrom" class="md-nav__link">
    pad_for_nystrom()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.reset_parameters" class="md-nav__link">
    reset_parameters()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.split_heads" class="md-nav__link">
    split_heads()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier" class="md-nav__link">
    slp.modules.classifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier" class="md-nav__link">
    Classifier
  </a>
  
    <nav class="md-nav" aria-label="Classifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.MOSEITextClassifier" class="md-nav__link">
    MOSEITextClassifier
  </a>
  
    <nav class="md-nav" aria-label="MOSEITextClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.MOSEITextClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.RNNLateFusionClassifier" class="md-nav__link">
    RNNLateFusionClassifier
  </a>
  
    <nav class="md-nav" aria-label="RNNLateFusionClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.RNNLateFusionClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.TransformerLateFusionClassifier" class="md-nav__link">
    TransformerLateFusionClassifier
  </a>
  
    <nav class="md-nav" aria-label="TransformerLateFusionClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.TransformerLateFusionClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed" class="md-nav__link">
    slp.modules.embed
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed" class="md-nav__link">
    Embed
  </a>
  
    <nav class="md-nav" aria-label="Embed">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.init_embeddings" class="md-nav__link">
    init_embeddings()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding" class="md-nav__link">
    PositionalEncoding
  </a>
  
    <nav class="md-nav" aria-label="PositionalEncoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward" class="md-nav__link">
    slp.modules.feedforward
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF" class="md-nav__link">
    PositionwiseFF
  </a>
  
    <nav class="md-nav" aria-label="PositionwiseFF">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward.TwoLayer" class="md-nav__link">
    TwoLayer
  </a>
  
    <nav class="md-nav" aria-label="TwoLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.TwoLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm" class="md-nav__link">
    slp.modules.norm
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf" class="md-nav__link">
    LayerNormTf
  </a>
  
    <nav class="md-nav" aria-label="LayerNormTf">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm.ScaleNorm" class="md-nav__link">
    ScaleNorm
  </a>
  
    <nav class="md-nav" aria-label="ScaleNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.ScaleNorm.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.regularization" class="md-nav__link">
    slp.modules.regularization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise" class="md-nav__link">
    GaussianNoise
  </a>
  
    <nav class="md-nav" aria-label="GaussianNoise">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn" class="md-nav__link">
    slp.modules.rnn
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN" class="md-nav__link">
    AttentiveRNN
  </a>
  
    <nav class="md-nav" aria-label="AttentiveRNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN" class="md-nav__link">
    RNN
  </a>
  
    <nav class="md-nav" aria-label="RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN" class="md-nav__link">
    TokenRNN
  </a>
  
    <nav class="md-nav" aria-label="TokenRNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer" class="md-nav__link">
    slp.modules.transformer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Decoder" class="md-nav__link">
    Decoder
  </a>
  
    <nav class="md-nav" aria-label="Decoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Decoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.DecoderLayer" class="md-nav__link">
    DecoderLayer
  </a>
  
    <nav class="md-nav" aria-label="DecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.DecoderLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Encoder" class="md-nav__link">
    Encoder
  </a>
  
    <nav class="md-nav" aria-label="Encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Encoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderDecoder" class="md-nav__link">
    EncoderDecoder
  </a>
  
    <nav class="md-nav" aria-label="EncoderDecoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderDecoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderLayer" class="md-nav__link">
    EncoderLayer
  </a>
  
    <nav class="md-nav" aria-label="EncoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer1" class="md-nav__link">
    Sublayer1
  </a>
  
    <nav class="md-nav" aria-label="Sublayer1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer1.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer2" class="md-nav__link">
    Sublayer2
  </a>
  
    <nav class="md-nav" aria-label="Sublayer2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer2.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer3" class="md-nav__link">
    Sublayer3
  </a>
  
    <nav class="md-nav" aria-label="Sublayer3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer3.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Transformer" class="md-nav__link">
    Transformer
  </a>
  
    <nav class="md-nav" aria-label="Transformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Transformer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerSequenceEncoder" class="md-nav__link">
    TransformerSequenceEncoder
  </a>
  
    <nav class="md-nav" aria-label="TransformerSequenceEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerSequenceEncoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerTokenSequenceEncoder" class="md-nav__link">
    TransformerTokenSequenceEncoder
  </a>
  
    <nav class="md-nav" aria-label="TransformerTokenSequenceEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerTokenSequenceEncoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.reset_parameters" class="md-nav__link">
    reset_parameters()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.dm" class="md-nav__link">
    slp.plbind.dm
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromCorpus" class="md-nav__link">
    PLDataModuleFromCorpus
  </a>
  
    <nav class="md-nav" aria-label="PLDataModuleFromCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromCorpus.add_argparse_args" class="md-nav__link">
    add_argparse_args()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets" class="md-nav__link">
    PLDataModuleFromDatasets
  </a>
  
    <nav class="md-nav" aria-label="PLDataModuleFromDatasets">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.add_argparse_args" class="md-nav__link">
    add_argparse_args()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.prepare_data" class="md-nav__link">
    prepare_data()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.test_dataloader" class="md-nav__link">
    test_dataloader()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.train_dataloader" class="md-nav__link">
    train_dataloader()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.dm.PLDataModuleFromDatasets.val_dataloader" class="md-nav__link">
    val_dataloader()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.dm.split_data" class="md-nav__link">
    split_data()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.helpers" class="md-nav__link">
    slp.plbind.helpers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FixedWandbLogger" class="md-nav__link">
    FixedWandbLogger
  </a>
  
    <nav class="md-nav" aria-label="FixedWandbLogger">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FixedWandbLogger.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FixedWandbLogger.finalize" class="md-nav__link">
    finalize()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FromLogits" class="md-nav__link">
    FromLogits
  </a>
  
    <nav class="md-nav" aria-label="FromLogits">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FromLogits.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FromLogits.compute" class="md-nav__link">
    compute()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.helpers.FromLogits.update" class="md-nav__link">
    update()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module" class="md-nav__link">
    slp.plbind.module
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.AutoEncoderPLModule" class="md-nav__link">
    AutoEncoderPLModule
  </a>
  
    <nav class="md-nav" aria-label="AutoEncoderPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.AutoEncoderPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.BertPLModule" class="md-nav__link">
    BertPLModule
  </a>
  
    <nav class="md-nav" aria-label="BertPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.BertPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.MultimodalTransformerClassificationPLModule" class="md-nav__link">
    MultimodalTransformerClassificationPLModule
  </a>
  
    <nav class="md-nav" aria-label="MultimodalTransformerClassificationPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.MultimodalTransformerClassificationPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.PLModule" class="md-nav__link">
    PLModule
  </a>
  
    <nav class="md-nav" aria-label="PLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.PLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.RnnPLModule" class="md-nav__link">
    RnnPLModule
  </a>
  
    <nav class="md-nav" aria-label="RnnPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.RnnPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule" class="md-nav__link">
    SimplePLModule
  </a>
  
    <nav class="md-nav" aria-label="SimplePLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.aggregate_epoch_metrics" class="md-nav__link">
    aggregate_epoch_metrics()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.configure_optimizers" class="md-nav__link">
    configure_optimizers()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.log_to_console" class="md-nav__link">
    log_to_console()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.test_epoch_end" class="md-nav__link">
    test_epoch_end()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.test_step" class="md-nav__link">
    test_step()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.training_epoch_end" class="md-nav__link">
    training_epoch_end()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.training_step" class="md-nav__link">
    training_step()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.validation_epoch_end" class="md-nav__link">
    validation_epoch_end()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.SimplePLModule.validation_step" class="md-nav__link">
    validation_step()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.TransformerClassificationPLModule" class="md-nav__link">
    TransformerClassificationPLModule
  </a>
  
    <nav class="md-nav" aria-label="TransformerClassificationPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.TransformerClassificationPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.module.TransformerPLModule" class="md-nav__link">
    TransformerPLModule
  </a>
  
    <nav class="md-nav" aria-label="TransformerPLModule">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.plbind.module.TransformerPLModule.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer" class="md-nav__link">
    slp.plbind.trainer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer.add_optimizer_args" class="md-nav__link">
    add_optimizer_args()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer.add_trainer_args" class="md-nav__link">
    add_trainer_args()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer.make_trainer" class="md-nav__link">
    make_trainer()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer.make_trainer_for_ray_tune" class="md-nav__link">
    make_trainer_for_ray_tune()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.plbind.trainer.watch_model" class="md-nav__link">
    watch_model()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.log" class="md-nav__link">
    slp.util.log
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.log.configure_logging" class="md-nav__link">
    configure_logging()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.log.log_to_file" class="md-nav__link">
    log_to_file()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch" class="md-nav__link">
    slp.util.pytorch
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.NoOp" class="md-nav__link">
    NoOp
  </a>
  
    <nav class="md-nav" aria-label="NoOp">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.util.pytorch.NoOp.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.PackSequence" class="md-nav__link">
    PackSequence
  </a>
  
    <nav class="md-nav" aria-label="PackSequence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.util.pytorch.PackSequence.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.util.pytorch.PackSequence.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.PadPackedSequence" class="md-nav__link">
    PadPackedSequence
  </a>
  
    <nav class="md-nav" aria-label="PadPackedSequence">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.util.pytorch.PadPackedSequence.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.util.pytorch.PadPackedSequence.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.from_checkpoint" class="md-nav__link">
    from_checkpoint()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.mktensor" class="md-nav__link">
    mktensor()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.moore_penrose_pinv" class="md-nav__link">
    moore_penrose_pinv()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.pad_mask" class="md-nav__link">
    pad_mask()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.pad_sequence" class="md-nav__link">
    pad_sequence()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.repeat_layer" class="md-nav__link">
    repeat_layer()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.rotate_tensor" class="md-nav__link">
    rotate_tensor()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.shift_tensor" class="md-nav__link">
    shift_tensor()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.sort_sequences" class="md-nav__link">
    sort_sequences()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.subsequent_mask" class="md-nav__link">
    subsequent_mask()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.t" class="md-nav__link">
    t()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.t_" class="md-nav__link">
    t_()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.pytorch.to_device" class="md-nav__link">
    to_device()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system" class="md-nav__link">
    slp.util.system
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.date_fname" class="md-nav__link">
    date_fname()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.download_url" class="md-nav__link">
    download_url()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.has_internet_connection" class="md-nav__link">
    has_internet_connection()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.is_file" class="md-nav__link">
    is_file()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.is_subpath" class="md-nav__link">
    is_subpath()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.is_url" class="md-nav__link">
    is_url()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.json_dump" class="md-nav__link">
    json_dump()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.json_load" class="md-nav__link">
    json_load()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.pickle_dump" class="md-nav__link">
    pickle_dump()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.pickle_load" class="md-nav__link">
    pickle_load()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.print_separator" class="md-nav__link">
    print_separator()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.read_wav" class="md-nav__link">
    read_wav()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.run_cmd" class="md-nav__link">
    run_cmd()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.run_cmd_silent" class="md-nav__link">
    run_cmd_silent()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.safe_mkdirs" class="md-nav__link">
    safe_mkdirs()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.suppress_print" class="md-nav__link">
    suppress_print()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.timethis" class="md-nav__link">
    timethis()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.write_wav" class="md-nav__link">
    write_wav()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.yaml_dump" class="md-nav__link">
    yaml_dump()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.system.yaml_load" class="md-nav__link">
    yaml_load()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.types" class="md-nav__link">
    slp.util.types
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.types.dir_path" class="md-nav__link">
    dir_path()
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="api-reference">API reference</h1>


  <div class="doc doc-object doc-module">

<a id="slp.config.config_parser"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-function">



<h2 id="slp.config.config_parser.generate_example_config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">generate_example_config</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="n">output_file</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>parse_config Parse a provided YAML config file and command line args and merge them</p>
<p>During experimentation we want ideally to have a configuration file with the model and training configuration,
but also be able to run quick experiments using command line args.
This function allows you to double dip, by overriding values in a YAML config file through user provided command line arguments.</p>
<p>The precedence for merging is as follows
   * default cli args values &lt; config file values &lt; user provided cli args</p>
<p>E.g.:</p>
<ul>
<li>if you don't include a value in your configuration it will take the default value from the argparse arguments</li>
<li>if you provide a cli arg (e.g. run the script with --bsz 64) it will override the value in the config file</li>
</ul>
<p>Note we use an extended OmegaConf istance to achieve this (see slp.config.omegaconf.OmegaConf)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>parser</code></td>
        <td><code>ArgumentParser</code></td>
        <td><p>The argument parser you want to use</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>output_file</code></td>
        <td><code>str</code></td>
        <td><p>Configuration file name or file descriptor to save example configuration</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>args</code></td>
        <td><code>Optional[List[str]]</code></td>
        <td><p>Optional input sys.argv style args. Useful for testing.
Use this only for testing. By default it uses sys.argv[1:]</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/config/config_parser.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">generate_example_config</span><span class="p">(</span>
    <span class="n">parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">,</span>
    <span class="n">output_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;parse_config Parse a provided YAML config file and command line args and merge them</span>

<span class="sd">    During experimentation we want ideally to have a configuration file with the model and training configuration,</span>
<span class="sd">    but also be able to run quick experiments using command line args.</span>
<span class="sd">    This function allows you to double dip, by overriding values in a YAML config file through user provided command line arguments.</span>

<span class="sd">    The precedence for merging is as follows</span>
<span class="sd">       * default cli args values &lt; config file values &lt; user provided cli args</span>

<span class="sd">    E.g.:</span>

<span class="sd">       * if you don&#39;t include a value in your configuration it will take the default value from the argparse arguments</span>
<span class="sd">       * if you provide a cli arg (e.g. run the script with --bsz 64) it will override the value in the config file</span>

<span class="sd">    Note we use an extended OmegaConf istance to achieve this (see slp.config.omegaconf.OmegaConf)</span>

<span class="sd">    Args:</span>
<span class="sd">        parser (argparse.ArgumentParser): The argument parser you want to use</span>
<span class="sd">        output_file (Union[str, IO]): Configuration file name or file descriptor to save example configuration</span>
<span class="sd">        args (Optional[List[str]]): Optional input sys.argv style args. Useful for testing.</span>
<span class="sd">            Use this only for testing. By default it uses sys.argv[1:]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">parse_config</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="n">include_none</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">OmegaConf</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">output_file</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.config.config_parser.make_cli_parser" class="doc doc-heading">
<code class="highlight language-python"><span class="n">make_cli_parser</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="n">datamodule_cls</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>make_cli_parser Augment an argument parser for slp with the default arguments</p>
<p>Default arguments for training, logging, optimization etc. are added to the input {parser}.
If you use make_cli_parser, the following command line arguments will be included</p>
<pre><code>!!! usage "my_script.py [-h] [--hidden MODEL.INTERMEDIATE_HIDDEN]"
                                [--optimizer {Adam,AdamW,SGD,Adadelta,Adagrad,Adamax,ASGD,RMSprop}]
                                [--lr OPTIM.LR] [--weight-decay OPTIM.WEIGHT_DECAY]
                                [--lr-scheduler] [--lr-factor LR_SCHEDULE.FACTOR]
                                [--lr-patience LR_SCHEDULE.PATIENCE]
                                [--lr-cooldown LR_SCHEDULE.COOLDOWN]
                                [--min-lr LR_SCHEDULE.MIN_LR] [--seed SEED] [--config CONFIG]
                                [--experiment-name TRAINER.EXPERIMENT_NAME]
                                [--run-id TRAINER.RUN_ID]
                                [--experiment-group TRAINER.EXPERIMENT_GROUP]
                                [--experiments-folder TRAINER.EXPERIMENTS_FOLDER]
                                [--save-top-k TRAINER.SAVE_TOP_K]
                                [--patience TRAINER.PATIENCE]
                                [--wandb-project TRAINER.WANDB_PROJECT]
                                [--tags [TRAINER.TAGS [TRAINER.TAGS ...]]]
                                [--stochastic_weight_avg] [--gpus TRAINER.GPUS]
                                [--val-interval TRAINER.CHECK_VAL_EVERY_N_EPOCH]
                                [--clip-grad-norm TRAINER.GRADIENT_CLIP_VAL]
                                [--epochs TRAINER.MAX_EPOCHS] [--steps TRAINER.MAX_STEPS]
                                [--tbtt_steps TRAINER.TRUNCATED_BPTT_STEPS] [--debug]
                                [--offline] [--early-stop-on TRAINER.EARLY_STOP_ON]
                                [--early-stop-mode {min,max}] [--num-trials TUNE.NUM_TRIALS]
                                [--gpus-per-trial TUNE.GPUS_PER_TRIAL]
                                [--cpus-per-trial TUNE.CPUS_PER_TRIAL]
                                [--tune-metric TUNE.METRIC] [--tune-mode {max,min}]
                                [--val-percent DATA.VAL_PERCENT]
                                [--test-percent DATA.TEST_PERCENT] [--bsz DATA.BATCH_SIZE]
                                [--bsz-eval DATA.BATCH_SIZE_EVAL]
                                [--num-workers DATA.NUM_WORKERS] [--no-pin-memory]
                                [--drop-last] [--no-shuffle-eval]

optional arguments:
  -h, --help            show this help message and exit
  --hidden MODEL.INTERMEDIATE_HIDDEN
                                                Intermediate hidden layers for linear module
  --optimizer {Adam,AdamW,SGD,Adadelta,Adagrad,Adamax,ASGD,RMSprop}
                                                Which optimizer to use
  --lr OPTIM.LR         Learning rate
  --weight-decay OPTIM.WEIGHT_DECAY
                                                Learning rate
  --lr-scheduler        Use learning rate scheduling. Currently only
                                                ReduceLROnPlateau is supported out of the box
  --lr-factor LR_SCHEDULE.FACTOR
                                                Multiplicative factor by which LR is reduced. Used if
                                                --lr-scheduler is provided.
  --lr-patience LR_SCHEDULE.PATIENCE
                                                Number of epochs with no improvement after which
                                                learning rate will be reduced. Used if --lr-scheduler
                                                is provided.
  --lr-cooldown LR_SCHEDULE.COOLDOWN
                                                Number of epochs to wait before resuming normal
                                                operation after lr has been reduced. Used if --lr-
                                                scheduler is provided.
  --min-lr LR_SCHEDULE.MIN_LR
                                                Minimum lr for LR scheduling. Used if --lr-scheduler
                                                is provided.
  --seed SEED           Seed for reproducibility
  --config CONFIG       Path to YAML configuration file
  --experiment-name TRAINER.EXPERIMENT_NAME
                                                Name of the running experiment
  --run-id TRAINER.RUN_ID
                                                Unique identifier for the current run. If not provided
                                                it is inferred from datetime.now()
  --experiment-group TRAINER.EXPERIMENT_GROUP
                                                Group of current experiment. Useful when evaluating
                                                for different seeds / cross-validation etc.
  --experiments-folder TRAINER.EXPERIMENTS_FOLDER
                                                Top-level folder where experiment results &amp;
                                                checkpoints are saved
  --save-top-k TRAINER.SAVE_TOP_K
                                                Save checkpoints for top k models
  --patience TRAINER.PATIENCE
                                                Number of epochs to wait before early stopping
  --wandb-project TRAINER.WANDB_PROJECT
                                                Wandb project under which results are saved
  --tags [TRAINER.TAGS [TRAINER.TAGS ...]]
                                                Tags for current run to make results searchable.
  --stochastic_weight_avg
                                                Use Stochastic weight averaging.
  --gpus TRAINER.GPUS   Number of GPUs to use
  --val-interval TRAINER.CHECK_VAL_EVERY_N_EPOCH
                                                Run validation every n epochs
  --clip-grad-norm TRAINER.GRADIENT_CLIP_VAL
                                                Clip gradients with ||grad(w)|| &gt;= args.clip_grad_norm
  --epochs TRAINER.MAX_EPOCHS
                                                Maximum number of training epochs
  --steps TRAINER.MAX_STEPS
                                                Maximum number of training steps
  --tbtt_steps TRAINER.TRUNCATED_BPTT_STEPS
                                                Truncated Back-propagation-through-time steps.
  --debug               If true, we run a full run on a small subset of the
                                                input data and overfit 10 training batches
  --offline             If true, forces offline execution of wandb logger
  --early-stop-on TRAINER.EARLY_STOP_ON
                                                Metric for early stopping
  --early-stop-mode {min,max}
                                                Minimize or maximize early stopping metric
  --num-trials TUNE.NUM_TRIALS
                                                Number of trials to run for hyperparameter tuning
  --gpus-per-trial TUNE.GPUS_PER_TRIAL
                                                How many gpus to use for each trial. If gpus_per_trial
                                                &lt; 1 multiple trials are packed in the same gpu
  --cpus-per-trial TUNE.CPUS_PER_TRIAL
                                                How many cpus to use for each trial.
  --tune-metric TUNE.METRIC
                                                Tune this metric. Need to be one of the keys of
                                                metrics_map passed into make_trainer_for_ray_tune.
  --tune-mode {max,min}
                                                Maximize or minimize metric
  --val-percent DATA.VAL_PERCENT
                                                Percent of validation data to be randomly split from
                                                the training set, if no validation set is provided
  --test-percent DATA.TEST_PERCENT
                                                Percent of test data to be randomly split from the
                                                training set, if no test set is provided
  --bsz DATA.BATCH_SIZE
                                                Training batch size
  --bsz-eval DATA.BATCH_SIZE_EVAL
                                                Evaluation batch size
  --num-workers DATA.NUM_WORKERS
                                                Number of workers to be used in the DataLoader
  --no-pin-memory       Don't pin data to GPU memory when transferring
  --drop-last           Drop last incomplete batch
  --no-shuffle-eval     Don't shuffle val &amp; test sets
</code></pre>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>parser</code></td>
        <td><code>ArgumentParser</code></td>
        <td><p>A parent argument to be augmented</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>datamodule_cls</code></td>
        <td><code>LightningDataModule</code></td>
        <td><p>A data module class that injects arguments through the add_argparse_args method</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>ArgumentParser</code></td>
      <td><p>argparse.ArgumentParser: The augmented command line parser</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">argparse</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">slp.plbind.dm</span> <span class="kn">import</span> <span class="n">PLDataModuleFromDatasets</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="s2">&quot;My cool model&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--hidden&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;model.hidden&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">)</span>  <span class="c1"># Create parser with model arguments and anything else you need</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">make_cli_parser</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="n">PLDataModuleFromDatasets</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">args</span> <span class="o">=</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;--bsz&quot;</span><span class="p">,</span> <span class="s2">&quot;64&quot;</span><span class="p">,</span> <span class="s2">&quot;--lr&quot;</span><span class="p">,</span> <span class="s2">&quot;0.01&quot;</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">args</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">batch_size</span>
<span class="mi">64</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">args</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">lr</span>
<span class="mf">0.01</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/config/config_parser.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">make_cli_parser</span><span class="p">(</span>
    <span class="n">parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">,</span> <span class="n">datamodule_cls</span><span class="p">:</span> <span class="n">pl</span><span class="o">.</span><span class="n">LightningDataModule</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;make_cli_parser Augment an argument parser for slp with the default arguments</span>

<span class="sd">    Default arguments for training, logging, optimization etc. are added to the input {parser}.</span>
<span class="sd">    If you use make_cli_parser, the following command line arguments will be included</span>

<span class="sd">        usage: my_script.py [-h] [--hidden MODEL.INTERMEDIATE_HIDDEN]</span>
<span class="sd">                                        [--optimizer {Adam,AdamW,SGD,Adadelta,Adagrad,Adamax,ASGD,RMSprop}]</span>
<span class="sd">                                        [--lr OPTIM.LR] [--weight-decay OPTIM.WEIGHT_DECAY]</span>
<span class="sd">                                        [--lr-scheduler] [--lr-factor LR_SCHEDULE.FACTOR]</span>
<span class="sd">                                        [--lr-patience LR_SCHEDULE.PATIENCE]</span>
<span class="sd">                                        [--lr-cooldown LR_SCHEDULE.COOLDOWN]</span>
<span class="sd">                                        [--min-lr LR_SCHEDULE.MIN_LR] [--seed SEED] [--config CONFIG]</span>
<span class="sd">                                        [--experiment-name TRAINER.EXPERIMENT_NAME]</span>
<span class="sd">                                        [--run-id TRAINER.RUN_ID]</span>
<span class="sd">                                        [--experiment-group TRAINER.EXPERIMENT_GROUP]</span>
<span class="sd">                                        [--experiments-folder TRAINER.EXPERIMENTS_FOLDER]</span>
<span class="sd">                                        [--save-top-k TRAINER.SAVE_TOP_K]</span>
<span class="sd">                                        [--patience TRAINER.PATIENCE]</span>
<span class="sd">                                        [--wandb-project TRAINER.WANDB_PROJECT]</span>
<span class="sd">                                        [--tags [TRAINER.TAGS [TRAINER.TAGS ...]]]</span>
<span class="sd">                                        [--stochastic_weight_avg] [--gpus TRAINER.GPUS]</span>
<span class="sd">                                        [--val-interval TRAINER.CHECK_VAL_EVERY_N_EPOCH]</span>
<span class="sd">                                        [--clip-grad-norm TRAINER.GRADIENT_CLIP_VAL]</span>
<span class="sd">                                        [--epochs TRAINER.MAX_EPOCHS] [--steps TRAINER.MAX_STEPS]</span>
<span class="sd">                                        [--tbtt_steps TRAINER.TRUNCATED_BPTT_STEPS] [--debug]</span>
<span class="sd">                                        [--offline] [--early-stop-on TRAINER.EARLY_STOP_ON]</span>
<span class="sd">                                        [--early-stop-mode {min,max}] [--num-trials TUNE.NUM_TRIALS]</span>
<span class="sd">                                        [--gpus-per-trial TUNE.GPUS_PER_TRIAL]</span>
<span class="sd">                                        [--cpus-per-trial TUNE.CPUS_PER_TRIAL]</span>
<span class="sd">                                        [--tune-metric TUNE.METRIC] [--tune-mode {max,min}]</span>
<span class="sd">                                        [--val-percent DATA.VAL_PERCENT]</span>
<span class="sd">                                        [--test-percent DATA.TEST_PERCENT] [--bsz DATA.BATCH_SIZE]</span>
<span class="sd">                                        [--bsz-eval DATA.BATCH_SIZE_EVAL]</span>
<span class="sd">                                        [--num-workers DATA.NUM_WORKERS] [--no-pin-memory]</span>
<span class="sd">                                        [--drop-last] [--no-shuffle-eval]</span>

<span class="sd">        optional arguments:</span>
<span class="sd">          -h, --help            show this help message and exit</span>
<span class="sd">          --hidden MODEL.INTERMEDIATE_HIDDEN</span>
<span class="sd">                                                        Intermediate hidden layers for linear module</span>
<span class="sd">          --optimizer {Adam,AdamW,SGD,Adadelta,Adagrad,Adamax,ASGD,RMSprop}</span>
<span class="sd">                                                        Which optimizer to use</span>
<span class="sd">          --lr OPTIM.LR         Learning rate</span>
<span class="sd">          --weight-decay OPTIM.WEIGHT_DECAY</span>
<span class="sd">                                                        Learning rate</span>
<span class="sd">          --lr-scheduler        Use learning rate scheduling. Currently only</span>
<span class="sd">                                                        ReduceLROnPlateau is supported out of the box</span>
<span class="sd">          --lr-factor LR_SCHEDULE.FACTOR</span>
<span class="sd">                                                        Multiplicative factor by which LR is reduced. Used if</span>
<span class="sd">                                                        --lr-scheduler is provided.</span>
<span class="sd">          --lr-patience LR_SCHEDULE.PATIENCE</span>
<span class="sd">                                                        Number of epochs with no improvement after which</span>
<span class="sd">                                                        learning rate will be reduced. Used if --lr-scheduler</span>
<span class="sd">                                                        is provided.</span>
<span class="sd">          --lr-cooldown LR_SCHEDULE.COOLDOWN</span>
<span class="sd">                                                        Number of epochs to wait before resuming normal</span>
<span class="sd">                                                        operation after lr has been reduced. Used if --lr-</span>
<span class="sd">                                                        scheduler is provided.</span>
<span class="sd">          --min-lr LR_SCHEDULE.MIN_LR</span>
<span class="sd">                                                        Minimum lr for LR scheduling. Used if --lr-scheduler</span>
<span class="sd">                                                        is provided.</span>
<span class="sd">          --seed SEED           Seed for reproducibility</span>
<span class="sd">          --config CONFIG       Path to YAML configuration file</span>
<span class="sd">          --experiment-name TRAINER.EXPERIMENT_NAME</span>
<span class="sd">                                                        Name of the running experiment</span>
<span class="sd">          --run-id TRAINER.RUN_ID</span>
<span class="sd">                                                        Unique identifier for the current run. If not provided</span>
<span class="sd">                                                        it is inferred from datetime.now()</span>
<span class="sd">          --experiment-group TRAINER.EXPERIMENT_GROUP</span>
<span class="sd">                                                        Group of current experiment. Useful when evaluating</span>
<span class="sd">                                                        for different seeds / cross-validation etc.</span>
<span class="sd">          --experiments-folder TRAINER.EXPERIMENTS_FOLDER</span>
<span class="sd">                                                        Top-level folder where experiment results &amp;</span>
<span class="sd">                                                        checkpoints are saved</span>
<span class="sd">          --save-top-k TRAINER.SAVE_TOP_K</span>
<span class="sd">                                                        Save checkpoints for top k models</span>
<span class="sd">          --patience TRAINER.PATIENCE</span>
<span class="sd">                                                        Number of epochs to wait before early stopping</span>
<span class="sd">          --wandb-project TRAINER.WANDB_PROJECT</span>
<span class="sd">                                                        Wandb project under which results are saved</span>
<span class="sd">          --tags [TRAINER.TAGS [TRAINER.TAGS ...]]</span>
<span class="sd">                                                        Tags for current run to make results searchable.</span>
<span class="sd">          --stochastic_weight_avg</span>
<span class="sd">                                                        Use Stochastic weight averaging.</span>
<span class="sd">          --gpus TRAINER.GPUS   Number of GPUs to use</span>
<span class="sd">          --val-interval TRAINER.CHECK_VAL_EVERY_N_EPOCH</span>
<span class="sd">                                                        Run validation every n epochs</span>
<span class="sd">          --clip-grad-norm TRAINER.GRADIENT_CLIP_VAL</span>
<span class="sd">                                                        Clip gradients with ||grad(w)|| &gt;= args.clip_grad_norm</span>
<span class="sd">          --epochs TRAINER.MAX_EPOCHS</span>
<span class="sd">                                                        Maximum number of training epochs</span>
<span class="sd">          --steps TRAINER.MAX_STEPS</span>
<span class="sd">                                                        Maximum number of training steps</span>
<span class="sd">          --tbtt_steps TRAINER.TRUNCATED_BPTT_STEPS</span>
<span class="sd">                                                        Truncated Back-propagation-through-time steps.</span>
<span class="sd">          --debug               If true, we run a full run on a small subset of the</span>
<span class="sd">                                                        input data and overfit 10 training batches</span>
<span class="sd">          --offline             If true, forces offline execution of wandb logger</span>
<span class="sd">          --early-stop-on TRAINER.EARLY_STOP_ON</span>
<span class="sd">                                                        Metric for early stopping</span>
<span class="sd">          --early-stop-mode {min,max}</span>
<span class="sd">                                                        Minimize or maximize early stopping metric</span>
<span class="sd">          --num-trials TUNE.NUM_TRIALS</span>
<span class="sd">                                                        Number of trials to run for hyperparameter tuning</span>
<span class="sd">          --gpus-per-trial TUNE.GPUS_PER_TRIAL</span>
<span class="sd">                                                        How many gpus to use for each trial. If gpus_per_trial</span>
<span class="sd">                                                        &lt; 1 multiple trials are packed in the same gpu</span>
<span class="sd">          --cpus-per-trial TUNE.CPUS_PER_TRIAL</span>
<span class="sd">                                                        How many cpus to use for each trial.</span>
<span class="sd">          --tune-metric TUNE.METRIC</span>
<span class="sd">                                                        Tune this metric. Need to be one of the keys of</span>
<span class="sd">                                                        metrics_map passed into make_trainer_for_ray_tune.</span>
<span class="sd">          --tune-mode {max,min}</span>
<span class="sd">                                                        Maximize or minimize metric</span>
<span class="sd">          --val-percent DATA.VAL_PERCENT</span>
<span class="sd">                                                        Percent of validation data to be randomly split from</span>
<span class="sd">                                                        the training set, if no validation set is provided</span>
<span class="sd">          --test-percent DATA.TEST_PERCENT</span>
<span class="sd">                                                        Percent of test data to be randomly split from the</span>
<span class="sd">                                                        training set, if no test set is provided</span>
<span class="sd">          --bsz DATA.BATCH_SIZE</span>
<span class="sd">                                                        Training batch size</span>
<span class="sd">          --bsz-eval DATA.BATCH_SIZE_EVAL</span>
<span class="sd">                                                        Evaluation batch size</span>
<span class="sd">          --num-workers DATA.NUM_WORKERS</span>
<span class="sd">                                                        Number of workers to be used in the DataLoader</span>
<span class="sd">          --no-pin-memory       Don&#39;t pin data to GPU memory when transferring</span>
<span class="sd">          --drop-last           Drop last incomplete batch</span>
<span class="sd">          --no-shuffle-eval     Don&#39;t shuffle val &amp; test sets</span>

<span class="sd">    Args:</span>
<span class="sd">        parser (argparse.ArgumentParser): A parent argument to be augmented</span>
<span class="sd">        datamodule_cls (pytorch_lightning.LightningDataModule): A data module class that injects arguments through the add_argparse_args method</span>

<span class="sd">    Returns:</span>
<span class="sd">        argparse.ArgumentParser: The augmented command line parser</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import argparse</span>
<span class="sd">        &gt;&gt;&gt; from slp.plbind.dm import PLDataModuleFromDatasets</span>
<span class="sd">        &gt;&gt;&gt; parser = argparse.ArgumentParser(&quot;My cool model&quot;)</span>
<span class="sd">        &gt;&gt;&gt; parser.add_argument(&quot;--hidden&quot;, dest=&quot;model.hidden&quot;, type=int)  # Create parser with model arguments and anything else you need</span>
<span class="sd">        &gt;&gt;&gt; parser = make_cli_parser(parser, PLDataModuleFromDatasets)</span>
<span class="sd">        &gt;&gt;&gt; args = parser.parse_args(args=[&quot;--bsz&quot;, &quot;64&quot;, &quot;--lr&quot;, &quot;0.01&quot;])</span>
<span class="sd">        &gt;&gt;&gt; args.data.batch_size</span>
<span class="sd">        64</span>
<span class="sd">        &gt;&gt;&gt; args.optim.lr</span>
<span class="sd">        0.01</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">add_optimizer_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">add_trainer_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">add_tune_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">datamodule_cls</span><span class="o">.</span><span class="n">add_argparse_args</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.config.config_parser.parse_config" class="doc doc-heading">
<code class="highlight language-python"><span class="n">parse_config</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="n">config_file</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">include_none</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>parse_config Parse a provided YAML config file and command line args and merge them</p>
<p>During experimentation we want ideally to have a configuration file with the model and training configuration,
but also be able to run quick experiments using command line args.
This function allows you to double dip, by overriding values in a YAML config file through user provided command line arguments.</p>
<p>The precedence for merging is as follows
   * default cli args values &lt; config file values &lt; user provided cli args</p>
<p>E.g.:</p>
<ul>
<li>if you don't include a value in your configuration it will take the default value from the argparse arguments</li>
<li>if you provide a cli arg (e.g. run the script with --bsz 64) it will override the value in the config file</li>
</ul>
<p>Note we use an extended OmegaConf istance to achieve this (see slp.config.omegaconf.OmegaConf)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>parser</code></td>
        <td><code>ArgumentParser</code></td>
        <td><p>The argument parser you want to use</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>config_file</code></td>
        <td><code>Union[str, IO]</code></td>
        <td><p>Configuration file name or file descriptor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>args</code></td>
        <td><code>Optional[List[str]]</code></td>
        <td><p>Optional input sys.argv style args. Useful for testing.
Use this only for testing. By default it uses sys.argv[1:]</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[omegaconf.listconfig.ListConfig, omegaconf.dictconfig.DictConfig]</code></td>
      <td><p>OmegaConf.DictConfig: The parsed configuration as an OmegaConf DictConfig object</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">io</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">slp.config.config_parser</span> <span class="kn">import</span> <span class="n">parse_config</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mock_config_file</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">StringIO</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">model:</span>
<span class="s1">  hidden: 100</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="s2">&quot;My cool model&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--hidden&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;model.hidden&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cfg</span> <span class="o">=</span> <span class="n">parse_config</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="n">mock_config_file</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hidden&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">}}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="nb">type</span><span class="p">(</span><span class="n">cfg</span><span class="p">)</span>
<span class="o">&lt;</span><span class="k">class</span> <span class="err">&#39;</span><span class="nc">omegaconf</span><span class="o">.</span><span class="n">dictconfig</span><span class="o">.</span><span class="n">DictConfig</span><span class="s1">&#39;&gt;</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cfg</span> <span class="o">=</span> <span class="n">parse_config</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="n">mock_config_file</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;--hidden&quot;</span><span class="p">,</span> <span class="s2">&quot;200&quot;</span><span class="p">])</span>
<span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hidden&#39;</span><span class="p">:</span> <span class="mi">200</span><span class="p">}}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">mock_config_file</span> <span class="o">=</span> <span class="n">io</span><span class="o">.</span><span class="n">StringIO</span><span class="p">(</span><span class="s1">&#39;&#39;&#39;</span>
<span class="s1">random_value: hello</span>
<span class="s1">&#39;&#39;&#39;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">cfg</span> <span class="o">=</span> <span class="n">parse_config</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="n">mock_config_file</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hidden&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">},</span> <span class="s1">&#39;random_value&#39;</span><span class="p">:</span> <span class="s1">&#39;hello&#39;</span><span class="p">}</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/config/config_parser.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">parse_config</span><span class="p">(</span>
    <span class="n">parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">,</span>
    <span class="n">config_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">IO</span><span class="p">]],</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">include_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">ListConfig</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;parse_config Parse a provided YAML config file and command line args and merge them</span>

<span class="sd">    During experimentation we want ideally to have a configuration file with the model and training configuration,</span>
<span class="sd">    but also be able to run quick experiments using command line args.</span>
<span class="sd">    This function allows you to double dip, by overriding values in a YAML config file through user provided command line arguments.</span>

<span class="sd">    The precedence for merging is as follows</span>
<span class="sd">       * default cli args values &lt; config file values &lt; user provided cli args</span>

<span class="sd">    E.g.:</span>

<span class="sd">       * if you don&#39;t include a value in your configuration it will take the default value from the argparse arguments</span>
<span class="sd">       * if you provide a cli arg (e.g. run the script with --bsz 64) it will override the value in the config file</span>

<span class="sd">    Note we use an extended OmegaConf istance to achieve this (see slp.config.omegaconf.OmegaConf)</span>

<span class="sd">    Args:</span>
<span class="sd">        parser (argparse.ArgumentParser): The argument parser you want to use</span>
<span class="sd">        config_file (Union[str, IO]): Configuration file name or file descriptor</span>
<span class="sd">        args (Optional[List[str]]): Optional input sys.argv style args. Useful for testing.</span>
<span class="sd">            Use this only for testing. By default it uses sys.argv[1:]</span>

<span class="sd">    Returns:</span>
<span class="sd">        OmegaConf.DictConfig: The parsed configuration as an OmegaConf DictConfig object</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import io</span>
<span class="sd">        &gt;&gt;&gt; from slp.config.config_parser import parse_config</span>
<span class="sd">        &gt;&gt;&gt; mock_config_file = io.StringIO(&#39;&#39;&#39;</span>
<span class="sd">        model:</span>
<span class="sd">          hidden: 100</span>
<span class="sd">        &#39;&#39;&#39;)</span>
<span class="sd">        &gt;&gt;&gt; parser = argparse.ArgumentParser(&quot;My cool model&quot;)</span>
<span class="sd">        &gt;&gt;&gt; parser.add_argument(&quot;--hidden&quot;, dest=&quot;model.hidden&quot;, type=int, default=20)</span>
<span class="sd">        &gt;&gt;&gt; cfg = parse_config(parser, mock_config_file)</span>
<span class="sd">        {&#39;model&#39;: {&#39;hidden&#39;: 100}}</span>
<span class="sd">        &gt;&gt;&gt; type(cfg)</span>
<span class="sd">        &lt;class &#39;omegaconf.dictconfig.DictConfig&#39;&gt;</span>
<span class="sd">        &gt;&gt;&gt; cfg = parse_config(parser, mock_config_file, args=[&quot;--hidden&quot;, &quot;200&quot;])</span>
<span class="sd">        {&#39;model&#39;: {&#39;hidden&#39;: 200}}</span>
<span class="sd">        &gt;&gt;&gt; mock_config_file = io.StringIO(&#39;&#39;&#39;</span>
<span class="sd">        random_value: hello</span>
<span class="sd">        &#39;&#39;&#39;)</span>
<span class="sd">        &gt;&gt;&gt; cfg = parse_config(parser, mock_config_file)</span>
<span class="sd">        {&#39;model&#39;: {&#39;hidden&#39;: 20}, &#39;random_value&#39;: &#39;hello&#39;}</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Merge Configurations Precedence: default kwarg values &lt; default argparse values &lt; config file values &lt; user provided CLI args values</span>

    <span class="k">if</span> <span class="n">config_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">dict_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">from_yaml</span><span class="p">(</span><span class="n">config_file</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">dict_config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">({})</span>

    <span class="n">user_cli</span><span class="p">,</span> <span class="n">default_cli</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">from_argparse</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="n">include_none</span><span class="o">=</span><span class="n">include_none</span><span class="p">)</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">merge</span><span class="p">(</span><span class="n">default_cli</span><span class="p">,</span> <span class="n">dict_config</span><span class="p">,</span> <span class="n">user_cli</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Running with the following configuration&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="se">\n</span><span class="si">{</span><span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_yaml</span><span class="p">(</span><span class="n">config</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">config</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.config.nlp"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.config.nlp.SPECIAL_TOKENS" class="doc doc-heading">
        <code>SPECIAL_TOKENS</code>



</h2>

    <div class="doc doc-contents ">

      <p>SPECIAL_TOKENS Special Tokens for NLP applications</p>
<p>Default special tokens values and indices (compatible with BERT):</p>
<pre><code>* [PAD]: 0
* [MASK]: 1
* [UNK]: 2
* [BOS]: 3
* [EOS]: 4
* [CLS]: 5
* [SEP]: 6
* [PAUSE]: 7
</code></pre>




  <div class="doc doc-children">



















  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.config.omegaconf"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.config.omegaconf.OmegaConfExtended" class="doc doc-heading">
        <code>OmegaConfExtended</code>



</h2>

    <div class="doc doc-contents ">

      <p>OmegaConfExtended Extended OmegaConf class, to include argparse style CLI arguments</p>
<p>Unfortunately the original authors are not interested into providing integration with argparse
(https://github.com/omry/omegaconf/issues/569), so we have to get by with this extension</p>




  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.config.omegaconf.OmegaConfExtended.from_argparse" class="doc doc-heading">
<code class="highlight language-python"><span class="n">from_argparse</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">include_none</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>from_argparse Static method to convert argparse arguments into OmegaConf DictConfig objects</p>
<p>We parse the command line arguments and separate the user provided values and the default values.
This is useful for merging with a config file.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>parser</code></td>
        <td><code>ArgumentParser</code></td>
        <td><p>Parser for argparse arguments</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>args</code></td>
        <td><code>Optional[List[str]]</code></td>
        <td><p>Optional input sys.argv style args. Useful for testing.
Use this only for testing. By default it uses sys.argv[1:]</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[omegaconf.dictconfig.DictConfig, omegaconf.dictconfig.DictConfig]</code></td>
      <td><p>Tuple[omegaconf.DictConfig, omegaconf.DictConfig]: (user provided cli args, default cli args) as a tuple of omegaconf.DictConfigs</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">argparse</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">slp.config.omegaconf</span> <span class="kn">import</span> <span class="n">OmegaConfExtended</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="s2">&quot;My cool model&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--hidden&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;model.hidden&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">20</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">user_provided_args</span><span class="p">,</span> <span class="n">default_args</span> <span class="o">=</span> <span class="n">OmegaConfExtended</span><span class="o">.</span><span class="n">from_argparse</span><span class="p">(</span><span class="n">parser</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;--hidden&quot;</span><span class="p">,</span> <span class="s2">&quot;100&quot;</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">user_provided_args</span>
<span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hidden&#39;</span><span class="p">:</span> <span class="mi">100</span><span class="p">}}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">default_args</span>
<span class="p">{}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">user_provided_args</span><span class="p">,</span> <span class="n">default_args</span> <span class="o">=</span> <span class="n">OmegaConfExtended</span><span class="o">.</span><span class="n">from_argparse</span><span class="p">(</span><span class="n">parser</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">user_provided_args</span>
<span class="p">{}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">default_args</span>
<span class="p">{</span><span class="s1">&#39;model&#39;</span><span class="p">:</span> <span class="p">{</span><span class="s1">&#39;hidden&#39;</span><span class="p">:</span> <span class="mi">20</span><span class="p">}}</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/config/omegaconf.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">from_argparse</span><span class="p">(</span>
    <span class="n">parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">,</span>
    <span class="n">args</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">include_none</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;from_argparse Static method to convert argparse arguments into OmegaConf DictConfig objects</span>

<span class="sd">    We parse the command line arguments and separate the user provided values and the default values.</span>
<span class="sd">    This is useful for merging with a config file.</span>

<span class="sd">    Args:</span>
<span class="sd">        parser (argparse.ArgumentParser): Parser for argparse arguments</span>
<span class="sd">        args (Optional[List[str]]): Optional input sys.argv style args. Useful for testing.</span>
<span class="sd">            Use this only for testing. By default it uses sys.argv[1:]</span>
<span class="sd">    Returns:</span>
<span class="sd">        Tuple[omegaconf.DictConfig, omegaconf.DictConfig]: (user provided cli args, default cli args) as a tuple of omegaconf.DictConfigs</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; import argparse</span>
<span class="sd">        &gt;&gt;&gt; from slp.config.omegaconf import OmegaConfExtended</span>
<span class="sd">        &gt;&gt;&gt; parser = argparse.ArgumentParser(&quot;My cool model&quot;)</span>
<span class="sd">        &gt;&gt;&gt; parser.add_argument(&quot;--hidden&quot;, dest=&quot;model.hidden&quot;, type=int, default=20)</span>
<span class="sd">        &gt;&gt;&gt; user_provided_args, default_args = OmegaConfExtended.from_argparse(parser, args=[&quot;--hidden&quot;, &quot;100&quot;])</span>
<span class="sd">        &gt;&gt;&gt; user_provided_args</span>
<span class="sd">        {&#39;model&#39;: {&#39;hidden&#39;: 100}}</span>
<span class="sd">        &gt;&gt;&gt; default_args</span>
<span class="sd">        {}</span>
<span class="sd">        &gt;&gt;&gt; user_provided_args, default_args = OmegaConfExtended.from_argparse(parser)</span>
<span class="sd">        &gt;&gt;&gt; user_provided_args</span>
<span class="sd">        {}</span>
<span class="sd">        &gt;&gt;&gt; default_args</span>
<span class="sd">        {&#39;model&#39;: {&#39;hidden&#39;: 20}}</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">dest_to_arg</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="o">.</span><span class="n">dest</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">parser</span><span class="o">.</span><span class="n">_option_string_actions</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="n">all_args</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="n">args</span><span class="p">))</span>
    <span class="n">provided_args</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">default_args</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">all_args</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">dest_to_arg</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="ow">in</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">:</span>
            <span class="n">provided_args</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">default_args</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

    <span class="n">provided</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">_nest</span><span class="p">(</span><span class="n">provided_args</span><span class="p">,</span> <span class="n">include_none</span><span class="o">=</span><span class="n">include_none</span><span class="p">))</span>
    <span class="n">defaults</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">_nest</span><span class="p">(</span><span class="n">default_args</span><span class="p">,</span> <span class="n">include_none</span><span class="o">=</span><span class="n">include_none</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">provided</span><span class="p">,</span> <span class="n">defaults</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.config.omegaconf.OmegaConfExtended.from_yaml" class="doc doc-heading">
<code class="highlight language-python"><span class="n">from_yaml</span><span class="p">(</span><span class="n">file_</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Alias for OmegaConf.load
OmegaConf.from_yaml got removed at some point. Bring it back</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>file_</code></td>
        <td><code>Union[str, pathlib.Path, IO[Any]]</code></td>
        <td><p>file to load or file descriptor</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[omegaconf.dictconfig.DictConfig, omegaconf.listconfig.ListConfig]</code></td>
      <td><p>Union[DictConfig, ListConfig]: The loaded configuration</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/config/omegaconf.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">from_yaml</span><span class="p">(</span>
    <span class="n">file_</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pathlib</span><span class="o">.</span><span class="n">Path</span><span class="p">,</span> <span class="n">IO</span><span class="p">[</span><span class="n">Any</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">DictConfig</span><span class="p">,</span> <span class="n">ListConfig</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Alias for OmegaConf.load</span>
<span class="sd">    OmegaConf.from_yaml got removed at some point. Bring it back</span>

<span class="sd">    Args:</span>
<span class="sd">        file_ (Union[str, pathlib.Path, IO[Any]]): file to load or file descriptor</span>

<span class="sd">    Returns:</span>
<span class="sd">        Union[DictConfig, ListConfig]: The loaded configuration</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">OmegaConfExtended</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file_</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.data.collators"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.data.collators.MultimodalSequenceClassificationCollator" class="doc doc-heading">
        <code>MultimodalSequenceClassificationCollator</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.MultimodalSequenceClassificationCollator.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call collate function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>List[Dict[str, torch.Tensor]]</code></td>
        <td><p>Batch of samples.
It expects a list of dictionaries from modalities to torch tensors</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Dict[str, torch.Tensor], torch.Tensor, Dict[str, torch.Tensor]]</code></td>
      <td><p>Tuple[Dict[str, torch.Tensor], torch.Tensor, Dict[str, torch.Tensor]]: tuple of
    (dict batched modality tensors, labels, dict of modality sequence lengths)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Call collate function</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (List[Dict[str, torch.Tensor]]): Batch of samples.</span>
<span class="sd">            It expects a list of dictionaries from modalities to torch tensors</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[Dict[str, torch.Tensor], torch.Tensor, Dict[str, torch.Tensor]]: tuple of</span>
<span class="sd">            (dict batched modality tensors, labels, dict of modality sequence lengths)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modalities</span><span class="p">:</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extract_sequence</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
        <span class="n">lengths</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">lengths</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">lengths</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">)</span>

        <span class="n">inputs</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
            <span class="n">seq</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">padding_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">targets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Label</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">label_key</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

    <span class="c1"># Pad and convert to tensor</span>
    <span class="n">ttargets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">mktensor</span><span class="p">(</span>
        <span class="n">targets</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">label_dtype</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">ttargets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">lengths</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.MultimodalSequenceClassificationCollator.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">modalities</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;audio&#39;</span><span class="p">,</span> <span class="s1">&#39;visual&#39;</span><span class="p">},</span> <span class="n">label_key</span><span class="o">=</span><span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">label_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Collate function for sequence classification tasks</p>
<ul>
<li>Perform padding</li>
<li>Calculate sequence lengths</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>pad_indx</code></td>
        <td><code>int</code></td>
        <td><p>Pad token index. Defaults to 0.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>modalities</code></td>
        <td><code>Set</code></td>
        <td><p>Which modalities are included in the batch dict</p></td>
        <td><code>{&#39;text&#39;, &#39;audio&#39;, &#39;visual&#39;}</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Pad sequences to a fixed maximum length</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>label_key</code></td>
        <td><code>str</code></td>
        <td><p>String to access the label in the batch dict</p></td>
        <td><code>&#39;label&#39;</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>device of returned tensors. Leave this as "cpu".
The LightningModule will handle the Conversion.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">MultimodalSequenceClassificationCollator</span><span class="p">())</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">modalities</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;visual&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;audio&quot;</span><span class="p">},</span>
    <span class="n">label_key</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Collate function for sequence classification tasks</span>

<span class="sd">    * Perform padding</span>
<span class="sd">    * Calculate sequence lengths</span>

<span class="sd">    Args:</span>
<span class="sd">        pad_indx (int): Pad token index. Defaults to 0.</span>
<span class="sd">        modalities (Set): Which modalities are included in the batch dict</span>
<span class="sd">        max_length (int): Pad sequences to a fixed maximum length</span>
<span class="sd">        label_key (str): String to access the label in the batch dict</span>
<span class="sd">        device (str): device of returned tensors. Leave this as &quot;cpu&quot;.</span>
<span class="sd">            The LightningModule will handle the Conversion.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = torch.utils.DataLoader(my_dataset, collate_fn=MultimodalSequenceClassificationCollator())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span> <span class="o">=</span> <span class="n">pad_indx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_key</span> <span class="o">=</span> <span class="n">label_key</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">modalities</span> <span class="o">=</span> <span class="n">modalities</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_dtype</span> <span class="o">=</span> <span class="n">label_dtype</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.collators.Seq2SeqCollator" class="doc doc-heading">
        <code>Seq2SeqCollator</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.Seq2SeqCollator.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call collate function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>List[Tuple[torch.Tensor, torch.Tensor]]</code></td>
        <td><p>Batch of samples.
It expects a list of tuples (source, target)
Each source and target are a sequences of features or ids.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: Returns tuple of batched tensors
    (inputs, labels, lengths_inputs, lengths_targets)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Call collate function</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (List[Tuple[torch.Tensor, torch.Tensor]]): Batch of samples.</span>
<span class="sd">            It expects a list of tuples (source, target)</span>
<span class="sd">            Each source and target are a sequences of features or ids.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: Returns tuple of batched tensors</span>
<span class="sd">            (inputs, labels, lengths_inputs, lengths_targets)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">targets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">lengths_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">lengths_targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">lengths_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">lengths_inputs</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">)</span>
        <span class="n">lengths_targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">lengths_targets</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">)</span>

    <span class="n">inputs_padded</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">targets_padded</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
        <span class="n">targets</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inputs_padded</span><span class="p">,</span> <span class="n">targets_padded</span><span class="p">,</span> <span class="n">lengths_inputs</span><span class="p">,</span> <span class="n">lengths_targets</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.Seq2SeqCollator.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Collate function for seq2seq tasks</p>
<ul>
<li>Perform padding</li>
<li>Calculate sequence lengths</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>pad_indx</code></td>
        <td><code>int</code></td>
        <td><p>Pad token index. Defaults to 0.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Pad sequences to a fixed maximum length</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>device of returned tensors. Leave this as "cpu".
The LightningModule will handle the Conversion.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">Seq2SeqClassificationCollator</span><span class="p">())</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Collate function for seq2seq tasks</span>

<span class="sd">    * Perform padding</span>
<span class="sd">    * Calculate sequence lengths</span>

<span class="sd">    Args:</span>
<span class="sd">        pad_indx (int): Pad token index. Defaults to 0.</span>
<span class="sd">        max_length (int): Pad sequences to a fixed maximum length</span>
<span class="sd">        device (str): device of returned tensors. Leave this as &quot;cpu&quot;.</span>
<span class="sd">            The LightningModule will handle the Conversion.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = torch.utils.DataLoader(my_dataset, collate_fn=Seq2SeqClassificationCollator())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span> <span class="o">=</span> <span class="n">pad_indx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.collators.SequenceClassificationCollator" class="doc doc-heading">
        <code>SequenceClassificationCollator</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.SequenceClassificationCollator.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call collate function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>List[Tuple[torch.Tensor, Union[numpy.ndarray, torch.Tensor, List[~T], int]]]</code></td>
        <td><p>Batch of samples.
It expects a list of tuples (inputs, label).</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Returns tuple of batched tensors (inputs, labels, lengths)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Label</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Call collate function</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (List[Tuple[torch.Tensor, slp.util.types.Label]]): Batch of samples.</span>
<span class="sd">            It expects a list of tuples (inputs, label).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Returns tuple of batched tensors (inputs, labels, lengths)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">targets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Label</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="c1">#  targets: List[torch.tensor] = map(list, zip(*batch))</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">)</span>
    <span class="c1"># Pad and convert to tensor</span>
    <span class="n">inputs_padded</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">ttargets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">mktensor</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inputs_padded</span><span class="p">,</span> <span class="n">ttargets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">lengths</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.SequenceClassificationCollator.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Collate function for sequence classification tasks</p>
<ul>
<li>Perform padding</li>
<li>Calculate sequence lengths</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>pad_indx</code></td>
        <td><code>int</code></td>
        <td><p>Pad token index. Defaults to 0.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Pad sequences to a fixed maximum length</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>device of returned tensors. Leave this as "cpu".
The LightningModule will handle the Conversion.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">SequenceClassificationCollator</span><span class="p">())</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Collate function for sequence classification tasks</span>

<span class="sd">    * Perform padding</span>
<span class="sd">    * Calculate sequence lengths</span>

<span class="sd">    Args:</span>
<span class="sd">        pad_indx (int): Pad token index. Defaults to 0.</span>
<span class="sd">        max_length (int): Pad sequences to a fixed maximum length</span>
<span class="sd">        device (str): device of returned tensors. Leave this as &quot;cpu&quot;.</span>
<span class="sd">            The LightningModule will handle the Conversion.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = torch.utils.DataLoader(my_dataset, collate_fn=SequenceClassificationCollator())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span> <span class="o">=</span> <span class="n">pad_indx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.data.corpus"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.data.corpus.EmbeddingsLoader" class="doc doc-heading">
        <code>EmbeddingsLoader</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.EmbeddingsLoader.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings_file</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">extra_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Load word embeddings in text format</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>embeddings_file</code></td>
        <td><code>str</code></td>
        <td><p>File where embeddings are stored (e.g. glove.6B.50d.txt)</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dim</code></td>
        <td><code>int</code></td>
        <td><p>Dimensionality of embeddings</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>vocab</code></td>
        <td><code>Optional[Dict[str, int]]</code></td>
        <td><p>Load only embeddings in vocab. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>extra_tokens</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Create random embeddings for these special tokens.
Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embeddings_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">vocab</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Load word embeddings in text format</span>

<span class="sd">    Args:</span>
<span class="sd">        embeddings_file (str): File where embeddings are stored (e.g. glove.6B.50d.txt)</span>
<span class="sd">        dim (int): Dimensionality of embeddings</span>
<span class="sd">        vocab (Optional[Dict[str, int]]): Load only embeddings in vocab. Defaults to None.</span>
<span class="sd">        extra_tokens (Optional[slp.config.nlp.SPECIAL_TOKENS]): Create random embeddings for these special tokens.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span> <span class="o">=</span> <span class="n">embeddings_file</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_cache_name</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim_</span> <span class="o">=</span> <span class="n">dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">extra_tokens</span> <span class="o">=</span> <span class="n">extra_tokens</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.EmbeddingsLoader.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>String representation of class</p>

        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;String representation of class&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_</span><span class="si">}</span><span class="s2">)&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.EmbeddingsLoader.augment_embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">augment_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">token</span><span class="p">,</span> <span class="n">emb</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Create a random embedding for a special token and append it to the embeddings array</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>word2idx</code></td>
        <td><code>Dict[str, int]</code></td>
        <td><p>Current word2idx map</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>idx2word</code></td>
        <td><code>Dict[int, str]</code></td>
        <td><p>Current idx2word map</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>List[numpy.ndarray]</code></td>
        <td><p>Embeddings array as list of embeddings</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>token</code></td>
        <td><code>str</code></td>
        <td><p>The special token (e.g. [PAD])</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>emb</code></td>
        <td><code>Optional[numpy.ndarray]</code></td>
        <td><p>Optional value for the embedding to be appended.
Defaults to None, where a random embedding is created.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Dict[str, int], Dict[int, str], List[numpy.ndarray]]</code></td>
      <td><p>Tuple[Dict[str, int], Dict[int, str], List[np.ndarray]]: (word2idx, idx2word, embeddings) tuple</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">augment_embeddings</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">word2idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">idx2word</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
    <span class="n">embeddings</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
    <span class="n">token</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Create a random embedding for a special token and append it to the embeddings array</span>

<span class="sd">    Args:</span>
<span class="sd">        word2idx (Dict[str, int]): Current word2idx map</span>
<span class="sd">        idx2word (Dict[int, str]): Current idx2word map</span>
<span class="sd">        embeddings (List[np.ndarray]): Embeddings array as list of embeddings</span>
<span class="sd">        token (str): The special token (e.g. [PAD])</span>
<span class="sd">        emb (Optional[np.ndarray]): Optional value for the embedding to be appended.</span>
<span class="sd">            Defaults to None, where a random embedding is created.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[Dict[str, int], Dict[int, str], List[np.ndarray]]: (word2idx, idx2word, embeddings) tuple</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">word2idx</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="n">idx2word</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)]</span> <span class="o">=</span> <span class="n">token</span>

    <span class="k">if</span> <span class="n">emb</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_</span><span class="p">)</span>
    <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.EmbeddingsLoader.in_accepted_vocab" class="doc doc-heading">
<code class="highlight language-python"><span class="n">in_accepted_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Check if word exists in given vocabulary</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>word</code></td>
        <td><code>str</code></td>
        <td><p>word from embeddings file</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>bool</code></td>
      <td><p>bool: Word exists</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">in_accepted_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Check if word exists in given vocabulary</span>

<span class="sd">    Args:</span>
<span class="sd">        word (str): word from embeddings file</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: Word exists</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.EmbeddingsLoader.load" class="doc doc-heading">
<code class="highlight language-python"><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Read the word vectors from a text file</p>
<ul>
<li>Read embeddings</li>
<li>Filter with given vocabulary</li>
<li>Augment with special tokens</li>
</ul>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Dict[str, int], Dict[int, str], numpy.ndarray]</code></td>
      <td><p>types.Embeddings: (word2idx, idx2word, embeddings) tuple</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@system</span><span class="o">.</span><span class="n">timethis</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">types</span><span class="o">.</span><span class="n">Embeddings</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Read the word vectors from a text file</span>

<span class="sd">    * Read embeddings</span>
<span class="sd">    * Filter with given vocabulary</span>
<span class="sd">    * Augment with special tokens</span>

<span class="sd">    Returns:</span>
<span class="sd">        types.Embeddings: (word2idx, idx2word, embeddings) tuple</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># in order to avoid this time consuming operation, cache the results</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_cache</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loaded word embeddings from cache.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">cache</span>
    <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Didn&#39;t find embeddings cache file </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Loading embeddings from file.&quot;</span><span class="p">)</span>

    <span class="c1"># create the necessary dictionaries and the word embeddings matrix</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">critical</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="si">}</span><span class="s2"> not found!&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">OSError</span><span class="p">(</span><span class="n">errno</span><span class="o">.</span><span class="n">ENOENT</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">strerror</span><span class="p">(</span><span class="n">errno</span><span class="o">.</span><span class="n">ENOENT</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Indexing file </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="si">}</span><span class="s2"> ...&quot;</span><span class="p">)</span>

    <span class="c1"># create the 2D array, which will be used for initializing</span>
    <span class="c1"># the Embedding layer of a NN.</span>
    <span class="c1"># We reserve the first row (idx=0), as the word embedding,</span>
    <span class="c1"># which will be used for zero padding (word with id = 0).</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">augment_embeddings</span><span class="p">(</span>
            <span class="p">{},</span>
            <span class="p">{},</span>
            <span class="p">[],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extra_tokens</span><span class="o">.</span><span class="n">PAD</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">emb</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_tokens</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adding token </span><span class="si">{</span><span class="n">token</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2"> to embeddings matrix&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_tokens</span><span class="o">.</span><span class="n">PAD</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">augment_embeddings</span><span class="p">(</span>
                <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">value</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">augment_embeddings</span><span class="p">(</span>
            <span class="p">{},</span> <span class="p">{},</span> <span class="p">[],</span> <span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="n">emb</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="c1"># read file, line by line</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">num_lines</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
            <span class="n">f</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">num_lines</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Loading word embeddings...&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">):</span>
            <span class="c1"># skip the first row if it is a header</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word2idx</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_accepted_vocab</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
                <span class="k">continue</span>

            <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">idx2word</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
            <span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
            <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
            <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="si">}</span><span class="s2"> word vectors.&quot;</span><span class="p">)</span>
    <span class="n">embeddings_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>

    <span class="c1"># write the data to a cache file</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dump_cache</span><span class="p">((</span><span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings_out</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings_out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.corpus.HfCorpus" class="doc doc-heading">
        <code>HfCorpus</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">embeddings</span><span class="p">:</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Unused. Defined for compatibility</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.frequencies" class="doc doc-heading">
<code class="highlight language-python"><span class="n">frequencies</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve wordpieces occurence counts</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: wordpieces occurence counts</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.idx2word" class="doc doc-heading">
<code class="highlight language-python"><span class="n">idx2word</span><span class="p">:</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Unused. Defined for compatibility</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.indices" class="doc doc-heading">
<code class="highlight language-python"><span class="n">indices</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve corpus as token indices</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[List[int]]</code></td>
      <td><p>List[List[int]]: Token indices for corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.raw" class="doc doc-heading">
<code class="highlight language-python"><span class="n">raw</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve raw corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>List[str]: Raw Corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.tokenized" class="doc doc-heading">
<code class="highlight language-python"><span class="n">tokenized</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve tokenized corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[List[str]]</code></td>
      <td><p>List[List[str]]: tokenized corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.vocab" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve set of words in vocabulary</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Set[str]</code></td>
      <td><p>Set[str]: set of words in vocabulary</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.vocab_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve vocabulary size</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Vocabulary size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.word2idx" class="doc doc-heading">
<code class="highlight language-python"><span class="n">word2idx</span><span class="p">:</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Unused. Defined for compatibility</p>
    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.HfCorpus.__getitem__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Get ith element in corpus as token indices</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>idx</code></td>
        <td><code>List[int]</code></td>
        <td><p>index in corpus</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of token indices for sentence</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Get ith element in corpus as token indices</span>

<span class="sd">    Args:</span>
<span class="sd">        idx (List[int]): index in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of token indices for sentence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&lt;=</span> <span class="mi">0</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">][:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.HfCorpus.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tokenizer_model</span><span class="o">=</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Process a corpus using hugging face tokenizers</p>
<p>Select one of hugging face tokenizers and process corpus</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>List[str]</code></td>
        <td><p>List of sentences</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lower</code></td>
        <td><code>bool</code></td>
        <td><p>Convert strings to lower case. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>tokenizer_model</code></td>
        <td><code>str</code></td>
        <td><p>Hugging face model to use. Defaults to "bert-base-uncased".</p></td>
        <td><code>&#39;bert-base-uncased&#39;</code></td>
      </tr>
      <tr>
        <td><code>add_special_tokens</code></td>
        <td><code>bool</code></td>
        <td><p>Add special tokens in sentence during tokenization. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>special_tokens</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens to include in the vocabulary.
 Defaults to slp.config.nlp.SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Crop sequences above this length. Defaults to -1 where sequences are left unaltered.</p></td>
        <td><code>-1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">corpus</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">lower</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">tokenizer_model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span>
    <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Process a corpus using hugging face tokenizers</span>

<span class="sd">    Select one of hugging face tokenizers and process corpus</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (List[str]): List of sentences</span>
<span class="sd">        lower (bool): Convert strings to lower case. Defaults to True.</span>
<span class="sd">        tokenizer_model (str): Hugging face model to use. Defaults to &quot;bert-base-uncased&quot;.</span>
<span class="sd">        add_special_tokens (bool): Add special tokens in sentence during tokenization. Defaults to True.</span>
<span class="sd">        special_tokens (Optional[SPECIAL_TOKENS]): Special tokens to include in the vocabulary.</span>
<span class="sd">             Defaults to slp.config.nlp.SPECIAL_TOKENS.</span>
<span class="sd">        max_length (int): Crop sequences above this length. Defaults to -1 where sequences are left unaltered.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">corpus_</span> <span class="o">=</span> <span class="n">corpus</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Tokenizing corpus using hugging face tokenizer from </span><span class="si">{</span><span class="n">tokenizer_model</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">HuggingFaceTokenizer</span><span class="p">(</span>
        <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">tokenizer_model</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">corpus_</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Converting tokens to indices...&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
    <span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">,</span>
            <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Mapping indices to tokens...&quot;</span><span class="p">,</span>
            <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span> <span class="o">=</span> <span class="n">create_vocab</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.HfCorpus.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Number of samples in corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Corpus length</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Number of samples in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Corpus length</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.corpus.TokenizedCorpus" class="doc doc-heading">
        <code>TokenizedCorpus</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">embeddings</span><span class="p">:</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Unused. Kept for compatibility</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.frequencies" class="doc doc-heading">
<code class="highlight language-python"><span class="n">frequencies</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve wordpieces occurence counts</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: wordpieces occurence counts</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.idx2word" class="doc doc-heading">
<code class="highlight language-python"><span class="n">idx2word</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve idx2word mapping</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[int, str]</code></td>
      <td><p>Dict[str, int]: idx2word mapping</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.indices" class="doc doc-heading">
<code class="highlight language-python"><span class="n">indices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve corpus as token indices</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[List[int], List[List[int]]]</code></td>
      <td><p>List[List[int]]: Token indices for corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.raw" class="doc doc-heading">
<code class="highlight language-python"><span class="n">raw</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve raw corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[List[str], List[List[str]]]</code></td>
      <td><p>List[str]: Raw Corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.tokenized" class="doc doc-heading">
<code class="highlight language-python"><span class="n">tokenized</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve tokenized corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[List[str], List[List[str]]]</code></td>
      <td><p>List[List[str]]: Tokenized corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.vocab" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve set of words in vocabulary</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Set[str]</code></td>
      <td><p>Set[str]: set of words in vocabulary</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.vocab_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve vocabulary size</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Vocabulary size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.word2idx" class="doc doc-heading">
<code class="highlight language-python"><span class="n">word2idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve word2idx mapping</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: word2idx mapping</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.TokenizedCorpus.__getitem__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Get ith element in corpus as token indices</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>idx</code></td>
        <td><code>List[int]</code></td>
        <td><p>index in corpus</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of token indices for sentence</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Get ith element in corpus as token indices</span>

<span class="sd">    Args:</span>
<span class="sd">        idx (List[int]): index in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of token indices for sentence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&lt;=</span> <span class="mi">0</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">][:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.TokenizedCorpus.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">word2idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Wrap a corpus that's already tokenized</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>Union[List[str], List[List[str]]]</code></td>
        <td><p>List of tokens or List of lists of tokens</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>word2idx</code></td>
        <td><code>Dict[str, int]</code></td>
        <td><p>Token to index mapping. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>special_tokens</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special Tokens. Defaults to SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">corpus</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]],</span>
    <span class="n">word2idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrap a corpus that&#39;s already tokenized</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (Union[List[str], List[List[str]]]): List of tokens or List of lists of tokens</span>
<span class="sd">        word2idx (Dict[str, int], optional): Token to index mapping. Defaults to None.</span>
<span class="sd">        special_tokens (Optional[SPECIAL_TOKENS], optional): Special Tokens. Defaults to SPECIAL_TOKENS.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">corpus_</span> <span class="o">=</span> <span class="n">corpus</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span> <span class="o">=</span> <span class="n">corpus</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span> <span class="o">=</span> <span class="n">create_vocab</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">word2idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Converting tokens to ids using word2idx.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span> <span class="o">=</span> <span class="n">word2idx</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;No word2idx provided. Will convert tokens to ids using an iterative counter.&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">()))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">idx2word_</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">to_token_ids</span> <span class="o">=</span> <span class="n">ToTokenIds</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span><span class="p">,</span>
        <span class="n">specials</span><span class="o">=</span><span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_token_ids</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">,</span>
                <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Converting tokens to token ids...&quot;</span><span class="p">,</span>
                <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_token_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.TokenizedCorpus.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Number of samples in corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Corpus length</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Number of samples in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Corpus length</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.corpus.WordCorpus" class="doc doc-heading">
        <code>WordCorpus</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">embeddings</span><span class="p">:</span> <span class="n">ndarray</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve embeddings array</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>ndarray</code></td>
      <td><p>np.ndarray: Array of pretrained word embeddings</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.frequencies" class="doc doc-heading">
<code class="highlight language-python"><span class="n">frequencies</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve word occurence counts</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: word occurence counts</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.idx2word" class="doc doc-heading">
<code class="highlight language-python"><span class="n">idx2word</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve idx2word mapping</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[int, str]</code></td>
      <td><p>Dict[str, int]: idx2word mapping</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.indices" class="doc doc-heading">
<code class="highlight language-python"><span class="n">indices</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve corpus as token indices</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[List[int]]</code></td>
      <td><p>List[List[int]]: Token indices for corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.raw" class="doc doc-heading">
<code class="highlight language-python"><span class="n">raw</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve raw corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>List[str]: Raw Corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.tokenized" class="doc doc-heading">
<code class="highlight language-python"><span class="n">tokenized</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve tokenized corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[List[str]]</code></td>
      <td><p>List[List[str]]: Tokenized corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.vocab" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve set of words in vocabulary</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Set[str]</code></td>
      <td><p>Set[str]: set of words in vocabulary</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.vocab_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve vocabulary size for corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: vocabulary size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.word2idx" class="doc doc-heading">
<code class="highlight language-python"><span class="n">word2idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve word2idx mapping</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: word2idx mapping</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.WordCorpus.__getitem__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Get ith element in corpus as token indices</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>idx</code></td>
        <td><code>List[int]</code></td>
        <td><p>index in corpus</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of token indices for sentence</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Get ith element in corpus as token indices</span>

<span class="sd">    Args:</span>
<span class="sd">        idx (List[int]): index in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of token indices for sentence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&lt;=</span> <span class="mi">0</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">][:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.WordCorpus.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">limit_vocab_size</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">word2idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">idx2word</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings_dim</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">append_eos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;en_core_web_md&#39;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Load corpus embeddings, tokenize in words using spacy and convert to ids</p>
<p>This class handles the handling of a raw corpus. It handles:</p>
<ul>
<li>Tokenization into words (spacy)</li>
<li>Loading of pretrained word embedding</li>
<li>Calculation of word frequencies / corpus statistics</li>
<li>Conversion to token ids</li>
</ul>
<p>You can pass either:</p>
<ul>
<li>Pass an embeddings file to load pretrained embeddings and create the word2idx mapping</li>
<li>Pass already loaded embeddings array and word2idx. This is useful for the dev / test splits
  where we want to pass the train split embeddings / word2idx.</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>List[str]</code></td>
        <td><p>Corpus as a list of sentences</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>limit_vocab_size</code></td>
        <td><code>int</code></td>
        <td><p>Upper bound for number of most frequent tokens to keep. Defaults to 30000.</p></td>
        <td><code>30000</code></td>
      </tr>
      <tr>
        <td><code>word2idx</code></td>
        <td><code>Optional[Dict[str, int]]</code></td>
        <td><p>Mapping of word to indices. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>idx2word</code></td>
        <td><code>Optional[Dict[int, str]]</code></td>
        <td><p>Mapping of indices to words. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>Optional[numpy.ndarray]</code></td>
        <td><p>Embeddings array. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings_file</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Embeddings file to read. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings_dim</code></td>
        <td><code>int</code></td>
        <td><p>Dimension of embeddings. Defaults to 300.</p></td>
        <td><code>300</code></td>
      </tr>
      <tr>
        <td><code>lower</code></td>
        <td><code>bool</code></td>
        <td><p>Convert strings to lower case. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>special_tokens</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens to include in the vocabulary.
 Defaults to slp.config.nlp.SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
      <tr>
        <td><code>prepend_bos</code></td>
        <td><code>bool</code></td>
        <td><p>Prepend Beginning of Sequence token for seq2seq tasks. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>append_eos</code></td>
        <td><code>bool</code></td>
        <td><p>Append End of Sequence token for seq2seq tasks. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>lang</code></td>
        <td><code>str</code></td>
        <td><p>Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to "en_core_web_md".</p></td>
        <td><code>&#39;en_core_web_md&#39;</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Crop sequences above this length. Defaults to -1 where sequences are left unaltered.</p></td>
        <td><code>-1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">corpus</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">limit_vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">30000</span><span class="p">,</span>
    <span class="n">word2idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">idx2word</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span>
    <span class="n">lower</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="n">prepend_bos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">append_eos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;en_core_web_md&quot;</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Load corpus embeddings, tokenize in words using spacy and convert to ids</span>

<span class="sd">    This class handles the handling of a raw corpus. It handles:</span>

<span class="sd">    * Tokenization into words (spacy)</span>
<span class="sd">    * Loading of pretrained word embedding</span>
<span class="sd">    * Calculation of word frequencies / corpus statistics</span>
<span class="sd">    * Conversion to token ids</span>

<span class="sd">    You can pass either:</span>

<span class="sd">    * Pass an embeddings file to load pretrained embeddings and create the word2idx mapping</span>
<span class="sd">    * Pass already loaded embeddings array and word2idx. This is useful for the dev / test splits</span>
<span class="sd">      where we want to pass the train split embeddings / word2idx.</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (List[List[str]]): Corpus as a list of sentences</span>
<span class="sd">        limit_vocab_size (int): Upper bound for number of most frequent tokens to keep. Defaults to 30000.</span>
<span class="sd">        word2idx (Optional[Dict[str, int]]): Mapping of word to indices. Defaults to None.</span>
<span class="sd">        idx2word (Optional[Dict[int, str]]): Mapping of indices to words. Defaults to None.</span>
<span class="sd">        embeddings (Optional[np.ndarray]): Embeddings array. Defaults to None.</span>
<span class="sd">        embeddings_file (Optional[str]): Embeddings file to read. Defaults to None.</span>
<span class="sd">        embeddings_dim (int): Dimension of embeddings. Defaults to 300.</span>
<span class="sd">        lower (bool): Convert strings to lower case. Defaults to True.</span>
<span class="sd">        special_tokens (Optional[SPECIAL_TOKENS]): Special tokens to include in the vocabulary.</span>
<span class="sd">             Defaults to slp.config.nlp.SPECIAL_TOKENS.</span>
<span class="sd">        prepend_bos (bool): Prepend Beginning of Sequence token for seq2seq tasks. Defaults to False.</span>
<span class="sd">        append_eos (bool): Append End of Sequence token for seq2seq tasks. Defaults to False.</span>
<span class="sd">        lang (str): Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to &quot;en_core_web_md&quot;.</span>
<span class="sd">        max_length (int): Crop sequences above this length. Defaults to -1 where sequences are left unaltered.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># FIXME: Extract super class to avoid repetition</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">corpus_</span> <span class="o">=</span> <span class="n">corpus</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SpacyTokenizer</span><span class="p">(</span>
        <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span>
        <span class="n">prepend_bos</span><span class="o">=</span><span class="n">prepend_bos</span><span class="p">,</span>
        <span class="n">append_eos</span><span class="o">=</span><span class="n">append_eos</span><span class="p">,</span>
        <span class="n">specials</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
        <span class="n">lang</span><span class="o">=</span><span class="n">lang</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokenizing corpus using spacy </span><span class="si">{</span><span class="n">lang</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus_</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Tokenizing corpus...&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span> <span class="o">=</span> <span class="n">create_vocab</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">limit_vocab_size</span> <span class="k">if</span> <span class="n">word2idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx2word_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="c1"># self.corpus_indices_ = self.tokenized_corpus_</span>

    <span class="k">if</span> <span class="n">word2idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Word2idx was already provided. Going to used it.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">embeddings_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">word2idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Going to load </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="p">)</span><span class="si">}</span><span class="s2"> embeddings from </span><span class="si">{</span><span class="n">embeddings_file</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">loader</span> <span class="o">=</span> <span class="n">EmbeddingsLoader</span><span class="p">(</span>
            <span class="n">embeddings_file</span><span class="p">,</span>
            <span class="n">embeddings_dim</span><span class="p">,</span>
            <span class="n">vocab</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="p">,</span>
            <span class="n">extra_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_</span> <span class="o">=</span> <span class="n">embeddings</span>

    <span class="k">if</span> <span class="n">idx2word</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx2word_</span> <span class="o">=</span> <span class="n">idx2word</span>

    <span class="k">if</span> <span class="n">word2idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span> <span class="o">=</span> <span class="n">word2idx</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Converting tokens to ids using word2idx.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_token_ids</span> <span class="o">=</span> <span class="n">ToTokenIds</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span><span class="p">,</span>
            <span class="n">specials</span><span class="o">=</span><span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_token_ids</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">,</span>
                <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Converting tokens to token ids...&quot;</span><span class="p">,</span>
                <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">]</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Filtering corpus vocabulary.&quot;</span><span class="p">)</span>

        <span class="n">updated_vocab</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span><span class="p">:</span>
                <span class="n">updated_vocab</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Out of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="p">)</span><span class="si">}</span><span class="s2"> tokens </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">updated_vocab</span><span class="p">)</span><span class="si">}</span><span class="s2"> were not found in the pretrained embeddings.&quot;</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span> <span class="o">=</span> <span class="n">updated_vocab</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.WordCorpus.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Number of samples in corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Corpus length</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Number of samples in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Corpus length</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="slp.data.corpus.create_vocab" class="doc doc-heading">
<code class="highlight language-python"><span class="n">create_vocab</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Create the vocabulary based on tokenized input corpus</p>
<ul>
<li>Injects special tokens in the vocabulary</li>
<li>Calculates the occurence count for each token</li>
<li>Limits vocabulary to vocab_size most common tokens</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>Union[List[str], List[List[str]]]</code></td>
        <td><p>The tokenized corpus as a list of sentences or a list of tokenized sentences</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>vocab_size</code></td>
        <td><code>int</code></td>
        <td><p>[description]. Limit vocabulary to vocab_size most common tokens.
Defaults to -1 which keeps all tokens.</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>special_tokens</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens to include in the vocabulary. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: Dictionary of all accepted tokens and their corresponding occurence counts</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">create_vocab</span><span class="p">([</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;galaxy&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;away&quot;</span><span class="p">])</span>
<span class="p">{</span><span class="s1">&#39;far&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;away&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;galaxy&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">create_vocab</span><span class="p">([</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;galaxy&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;away&quot;</span><span class="p">],</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;far&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">create_vocab</span><span class="p">([</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;galaxy&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;away&quot;</span><span class="p">],</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="n">slp</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">nlp</span><span class="o">.</span><span class="n">SPECIAL_TOKENS</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;[PAD]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[MASK]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[UNK]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[BOS]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[EOS]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[CLS]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;far&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">create_vocab</span><span class="p">(</span>
    <span class="n">corpus</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]],</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Create the vocabulary based on tokenized input corpus</span>

<span class="sd">    * Injects special tokens in the vocabulary</span>
<span class="sd">    * Calculates the occurence count for each token</span>
<span class="sd">    * Limits vocabulary to vocab_size most common tokens</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (Union[List[str], List[List[str]]]): The tokenized corpus as a list of sentences or a list of tokenized sentences</span>
<span class="sd">        vocab_size (int): [description]. Limit vocabulary to vocab_size most common tokens.</span>
<span class="sd">            Defaults to -1 which keeps all tokens.</span>
<span class="sd">        special_tokens Optional[SPECIAL_TOKENS]: Special tokens to include in the vocabulary. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, int]: Dictionary of all accepted tokens and their corresponding occurence counts</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; create_vocab([&quot;in&quot;, &quot;a&quot;, &quot;galaxy&quot;, &quot;far&quot;, &quot;far&quot;, &quot;away&quot;])</span>
<span class="sd">        {&#39;far&#39;: 2, &#39;away&#39;: 1, &#39;galaxy&#39;: 1, &#39;a&#39;: 1, &#39;in&#39;: 1}</span>
<span class="sd">        &gt;&gt;&gt; create_vocab([&quot;in&quot;, &quot;a&quot;, &quot;galaxy&quot;, &quot;far&quot;, &quot;far&quot;, &quot;away&quot;], vocab_size=3)</span>
<span class="sd">        {&#39;far&#39;: 2, &#39;a&#39;: 1, &#39;in&#39;: 1}</span>
<span class="sd">        &gt;&gt;&gt; create_vocab([&quot;in&quot;, &quot;a&quot;, &quot;galaxy&quot;, &quot;far&quot;, &quot;far&quot;, &quot;away&quot;], vocab_size=3, special_tokens=slp.config.nlp.SPECIAL_TOKENS)</span>
<span class="sd">        {&#39;[PAD]&#39;: 0, &#39;[MASK]&#39;: 0, &#39;[UNK]&#39;: 0, &#39;[BOS]&#39;: 0, &#39;[EOS]&#39;: 0, &#39;[CLS]&#39;: 0, &#39;[SEP]&#39;: 0, &#39;far&#39;: 2, &#39;a&#39;: 1, &#39;in&#39;: 1}</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">corpus</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">corpus</span><span class="p">))</span>
    <span class="n">freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">special_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">extra_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">extra_tokens</span> <span class="o">=</span> <span class="n">special_tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">vocab_size</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">freq</span><span class="p">)</span>
    <span class="n">take</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">freq</span><span class="p">))</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Keeping </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2"> most common tokens out of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">freq</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">take0</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Take first tuple element&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">common_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">take0</span><span class="p">,</span> <span class="n">freq</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">take</span><span class="p">)))</span>
    <span class="n">common_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">common_words</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">extra_tokens</span><span class="p">))</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">extra_tokens</span> <span class="o">+</span> <span class="n">common_words</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">vocab_size</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">words</span><span class="p">[:</span> <span class="n">vocab_size</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">extra_tokens</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">token_freq</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Token frequeny&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">extra_tokens</span> <span class="k">else</span> <span class="n">freq</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="nb">map</span><span class="p">(</span><span class="n">token_freq</span><span class="p">,</span> <span class="n">words</span><span class="p">)))</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary created with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s2"> tokens.&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The 10 most common tokens are:</span><span class="se">\n</span><span class="si">{</span><span class="n">freq</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">vocab</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.data.datasets"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.data.datasets.CorpusDataset" class="doc doc-heading">
        <code>CorpusDataset</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusDataset.__getitem__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Get a source and target token from the corpus</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>idx</code></td>
        <td><code>int</code></td>
        <td><p>Token position</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>(processed sentence, label)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a source and target token from the corpus</span>

<span class="sd">    Args:</span>
<span class="sd">        idx (int): Token position</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: (processed sentence, label)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">corpus</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">target</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text</span><span class="p">,</span> <span class="n">target</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusDataset.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Labeled corpus dataset</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>WordCorpus, HfCorpus etc..</code></td>
        <td><p>Input corpus</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>labels</code></td>
        <td><code>List[Any]</code></td>
        <td><p>Labels for examples</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Labeled corpus dataset</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (WordCorpus, HfCorpus etc..): Input corpus</span>
<span class="sd">        labels (List[Any]): Labels for examples</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus</span><span class="p">),</span> <span class="s2">&quot;Incompatible labels and corpus&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusDataset.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Length of corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>Corpus Length</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Length of corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Corpus Length</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusDataset.map" class="doc doc-heading">
<code class="highlight language-python"><span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Append a transform to self.transforms, in order to be applied to the data</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>t</code></td>
        <td><code>Callable[[str], Any]</code></td>
        <td><p>Transform of input token</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>CorpusDataset</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Append a transform to self.transforms, in order to be applied to the data</span>

<span class="sd">    Args:</span>
<span class="sd">        t (Callable[[str], Any]): Transform of input token</span>

<span class="sd">    Returns:</span>
<span class="sd">        CorpusDataset: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.datasets.CorpusLMDataset" class="doc doc-heading">
        <code>CorpusLMDataset</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusLMDataset.__getitem__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Get a source and target token from the corpus</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>idx</code></td>
        <td><code>int</code></td>
        <td><p>Token position</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>source=coprus[idx], target=corpus[idx+1]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a source and target token from the corpus</span>

<span class="sd">    Args:</span>
<span class="sd">        idx (int): Token position</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: source=coprus[idx], target=corpus[idx+1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="p">:</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusLMDataset.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Wraps a tokenized dataset which is provided as a list of tokens</p>
<p>Targets = source shifted one token to the left (next token prediction)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>List[str] or WordCorpus</code></td>
        <td><p>List of tokens</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wraps a tokenized dataset which is provided as a list of tokens</span>

<span class="sd">    Targets = source shifted one token to the left (next token prediction)</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (List[str] or WordCorpus): List of tokens</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusLMDataset.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Length of corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>Corpus Length</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Length of corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Corpus Length</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusLMDataset.map" class="doc doc-heading">
<code class="highlight language-python"><span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Append a transform to self.transforms, in order to be applied to the data</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>t</code></td>
        <td><code>Callable[[str], Any]</code></td>
        <td><p>Transform of input token</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>CorpusLMDataset</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Append a transform to self.transforms, in order to be applied to the data</span>

<span class="sd">    Args:</span>
<span class="sd">        t (Callable[[str], Any]): Transform of input token</span>

<span class="sd">    Returns:</span>
<span class="sd">        CorpusLMDataset: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.data.transforms"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.HuggingFaceTokenizer" class="doc doc-heading">
        <code>HuggingFaceTokenizer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.HuggingFaceTokenizer.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call to tokenize function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>str</code></td>
        <td><p>Input string</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of token ids</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Call to tokenize function</span>

<span class="sd">    Args:</span>
<span class="sd">        x (str): Input string</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of token ids</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">add_special_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">65536</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.HuggingFaceTokenizer.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Apply one of huggingface tokenizers to a string</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>lower</code></td>
        <td><code>bool</code></td>
        <td><p>Lowercase string. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>model</code></td>
        <td><code>str</code></td>
        <td><p>Select transformer model. Defaults to "bert-base-uncased".</p></td>
        <td><code>&#39;bert-base-uncased&#39;</code></td>
      </tr>
      <tr>
        <td><code>add_special_tokens</code></td>
        <td><code>bool</code></td>
        <td><p>Insert special tokens to tokenized string. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">lower</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span>
    <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply one of huggingface tokenizers to a string</span>

<span class="sd">    Args:</span>
<span class="sd">        lower (bool): Lowercase string. Defaults to True.</span>
<span class="sd">        model (str): Select transformer model. Defaults to &quot;bert-base-uncased&quot;.</span>
<span class="sd">        add_special_tokens (bool): Insert special tokens to tokenized string. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="n">lower</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_special_tokens</span> <span class="o">=</span> <span class="n">add_special_tokens</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.HuggingFaceTokenizer.detokenize" class="doc doc-heading">
<code class="highlight language-python"><span class="n">detokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Convert list of token ids to list of tokens</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>List[int]</code></td>
        <td><p>List of token ids</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>List[str]: List of tokens</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">detokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Convert list of token ids to list of tokens</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[int]): List of token ids</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[str]: List of tokens</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.ReplaceUnknownToken" class="doc doc-heading">
        <code>ReplaceUnknownToken</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ReplaceUnknownToken.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Convert <unk> in list of tokens to [UNK]</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>List[str]</code></td>
        <td><p>List of tokens</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>List[str]: List of tokens</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Convert &lt;unk&gt; in list of tokens to [UNK]</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[str]): List of tokens</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[str]: List of tokens</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">w</span> <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">old_unk</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_unk</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ReplaceUnknownToken.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">old_unk</span><span class="o">=</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">,</span> <span class="n">new_unk</span><span class="o">=</span><span class="s1">&#39;[UNK]&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Replace existing unknown tokens in the vocab to [UNK]. Useful for wikitext</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>old_unk</code></td>
        <td><code>str</code></td>
        <td><p>Unk token in corpus. Defaults to "<unk>".</p></td>
        <td><code>&#39;&lt;unk&gt;&#39;</code></td>
      </tr>
      <tr>
        <td><code>new_unk</code></td>
        <td><code>str</code></td>
        <td><p>Desired unk value. Defaults to SPECIAL_TOKENS.UNK.value.</p></td>
        <td><code>&#39;[UNK]&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">old_unk</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
    <span class="n">new_unk</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="o">.</span><span class="n">UNK</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Replace existing unknown tokens in the vocab to [UNK]. Useful for wikitext</span>

<span class="sd">    Args:</span>
<span class="sd">        old_unk (str): Unk token in corpus. Defaults to &quot;&lt;unk&gt;&quot;.</span>
<span class="sd">        new_unk (str): Desired unk value. Defaults to SPECIAL_TOKENS.UNK.value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">old_unk</span> <span class="o">=</span> <span class="n">old_unk</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">new_unk</span> <span class="o">=</span> <span class="n">new_unk</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.SentencepieceTokenizer" class="doc doc-heading">
        <code>SentencepieceTokenizer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.SentencepieceTokenizer.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call to tokenize function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>str</code></td>
        <td><p>Input string</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of tokens ids</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Call to tokenize function</span>

<span class="sd">    Args:</span>
<span class="sd">        x (str): Input string</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of tokens ids</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lower</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_as_ids</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span>
    <span class="k">return</span> <span class="n">ids</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.SentencepieceTokenizer.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">append_eos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">specials</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Tokenize sentence using pretrained sentencepiece model</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>lower</code></td>
        <td><code>bool</code></td>
        <td><p>Lowercase string. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>model</code></td>
        <td><code>Optional[Any]</code></td>
        <td><p>Sentencepiece model. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>prepend_bos</code></td>
        <td><code>bool</code></td>
        <td><p>Prepend BOS for seq2seq. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>append_eos</code></td>
        <td><code>bool</code></td>
        <td><p>Append EOS for seq2seq. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>specials</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens. Defaults to SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">lower</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prepend_bos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">append_eos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">specials</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tokenize sentence using pretrained sentencepiece model</span>

<span class="sd">    Args:</span>
<span class="sd">        lower (bool): Lowercase string. Defaults to True.</span>
<span class="sd">        model (Optional[Any]): Sentencepiece model. Defaults to None.</span>
<span class="sd">        prepend_bos (bool): Prepend BOS for seq2seq. Defaults to False.</span>
<span class="sd">        append_eos (bool): Append EOS for seq2seq. Defaults to False.</span>
<span class="sd">        specials (Optional[SPECIAL_TOKENS]): Special tokens. Defaults to SPECIAL_TOKENS.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">Load</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">specials</span> <span class="o">=</span> <span class="n">specials</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lower</span> <span class="o">=</span> <span class="n">lower</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_piece_size</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">prepend_bos</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">piece_to_id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">specials</span><span class="o">.</span><span class="n">BOS</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
    <span class="k">if</span> <span class="n">append_eos</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">piece_to_id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">specials</span><span class="o">.</span><span class="n">EOS</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.SpacyTokenizer" class="doc doc-heading">
        <code>SpacyTokenizer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.SpacyTokenizer.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call to tokenize function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>str</code></td>
        <td><p>Input string</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>List[str]: List of tokens</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Call to tokenize function</span>

<span class="sd">    Args:</span>
<span class="sd">        x (str): Input string</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[str]: List of tokens</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lower</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span> <span class="o">+</span> <span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">nlp</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.SpacyTokenizer.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">append_eos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">specials</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;en_core_web_sm&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Apply spacy tokenizer to str</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>lower</code></td>
        <td><code>bool</code></td>
        <td><p>Lowercase string. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>prepend_bos</code></td>
        <td><code>bool</code></td>
        <td><p>Prepend BOS for seq2seq. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>append_eos</code></td>
        <td><code>bool</code></td>
        <td><p>Append EOS for seq2seq. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>specials</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens. Defaults to SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
      <tr>
        <td><code>lang</code></td>
        <td><code>str</code></td>
        <td><p>Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to "en_core_web_md".</p></td>
        <td><code>&#39;en_core_web_sm&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">lower</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">prepend_bos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">append_eos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">specials</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="n">lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply spacy tokenizer to str</span>

<span class="sd">    Args:</span>
<span class="sd">        lower (bool): Lowercase string. Defaults to True.</span>
<span class="sd">        prepend_bos (bool): Prepend BOS for seq2seq. Defaults to False.</span>
<span class="sd">        append_eos (bool): Append EOS for seq2seq. Defaults to False.</span>
<span class="sd">        specials (Optional[SPECIAL_TOKENS]): Special tokens. Defaults to SPECIAL_TOKENS.</span>
<span class="sd">        lang (str): Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to &quot;en_core_web_md&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lower</span> <span class="o">=</span> <span class="n">lower</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">specials</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lang</span> <span class="o">=</span> <span class="n">lang</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">prepend_bos</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">specials</span><span class="o">.</span><span class="n">BOS</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">append_eos</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">specials</span><span class="o">.</span><span class="n">EOS</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nlp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_nlp</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">lang</span><span class="p">,</span> <span class="n">specials</span><span class="o">=</span><span class="n">specials</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.SpacyTokenizer.get_nlp" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_nlp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;en_core_web_sm&#39;</span><span class="p">,</span> <span class="n">specials</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Get spacy nlp object for given lang and add SPECIAL_TOKENS</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>name</code></td>
        <td><code>str</code></td>
        <td><p>Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to "en_core_web_md".</p></td>
        <td><code>&#39;en_core_web_sm&#39;</code></td>
      </tr>
      <tr>
        <td><code>specials</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens. Defaults to SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Language</code></td>
      <td><p>spacy.Language: spacy text-processing pipeline</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_nlp</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">,</span>
    <span class="n">specials</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">spacy</span><span class="o">.</span><span class="n">Language</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Get spacy nlp object for given lang and add SPECIAL_TOKENS</span>

<span class="sd">    Args:</span>
<span class="sd">        name (str): Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to &quot;en_core_web_md&quot;.</span>
<span class="sd">        specials (Optional[SPECIAL_TOKENS]): Special tokens. Defaults to SPECIAL_TOKENS.</span>

<span class="sd">    Returns:</span>
<span class="sd">        spacy.Language: spacy text-processing pipeline</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">specials</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">specials</span><span class="o">.</span><span class="n">to_list</span><span class="p">():</span>
            <span class="n">nlp</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">add_special_case</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="p">[{</span><span class="n">ORTH</span><span class="p">:</span> <span class="n">token</span><span class="p">}])</span>
    <span class="k">return</span> <span class="n">nlp</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.ToTensor" class="doc doc-heading">
        <code>ToTensor</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ToTensor.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Convert list of tokens or list of features to tensor</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>List[Any]</code></td>
        <td><p>List of tokens or features</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Resulting tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Convert list of tokens or list of features to tensor</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[Any]): List of tokens or features</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Resulting tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">mktensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ToTensor.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>To tensor convertor</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>Device to map the tensor. Defaults to "cpu".</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>dtype</code></td>
        <td><code>dtype</code></td>
        <td><p>Type of resulting tensor. Defaults to torch.long.</p></td>
        <td><code>torch.int64</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;To tensor convertor</span>

<span class="sd">    Args:</span>
<span class="sd">        device (str): Device to map the tensor. Defaults to &quot;cpu&quot;.</span>
<span class="sd">        dtype (torch.dtype): Type of resulting tensor. Defaults to torch.long.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.ToTokenIds" class="doc doc-heading">
        <code>ToTokenIds</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ToTokenIds.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Convert list of tokens to list of token ids</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>List[str]</code></td>
        <td><p>List of tokens</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of token ids</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Convert list of tokens to list of token ids</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[str]): List of tokens</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of token ids</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_value</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span>
    <span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ToTokenIds.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">specials</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Convert List of tokens to list of token ids</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>word2idx</code></td>
        <td><code>Dict[str, int]</code></td>
        <td><p>Word to index mapping</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>specials</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens. Defaults to SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">word2idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">specials</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert List of tokens to list of token ids</span>

<span class="sd">    Args:</span>
<span class="sd">        word2idx (Dict[str, int]): Word to index mapping</span>
<span class="sd">        specials (Optional[SPECIAL_TOKENS]): Special tokens. Defaults to SPECIAL_TOKENS.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span> <span class="o">=</span> <span class="n">word2idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unk_value</span> <span class="o">=</span> <span class="n">specials</span><span class="o">.</span><span class="n">UNK</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="n">specials</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;[UNK]&quot;</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.attention"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.attention.Attention" class="doc doc-heading">
        <code>Attention</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.Attention.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Single-Headed Dot-product attention module</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>attention_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>input_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Input features. Defaults to None.
If None input_size is set to attention_size.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attention_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Single-Headed Dot-product attention module</span>

<span class="sd">    Args:</span>
<span class="sd">        attention_size (int): Number of hidden features. Defaults to 512.</span>
<span class="sd">        input_size (Optional[int]): Input features. Defaults to None.</span>
<span class="sd">            If None input_size is set to attention_size.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Attention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dk</span> <span class="o">=</span> <span class="n">input_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.Attention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Single-head scaled dot-product attention forward pass</p>
<p>Outputs the values, where features for each sequence element are weighted by their respective attention scores</p>
<div class="arithmatex">\[a = softmax(\frac{Q}{K^T}){\sqrt{d}}) \dot V\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>keys</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, D] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>queries</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, M, D] Queries tensor. If None queries = keys. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, L] or [B, M, L] zero-one mask for sequence elements. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, M, L])</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">keys</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Single-head scaled dot-product attention forward pass</span>

<span class="sd">    Outputs the values, where features for each sequence element are weighted by their respective attention scores</span>

<span class="sd">    $$a = softmax(\frac{Q}{K^T}){\sqrt{d}}) \dot V$$</span>

<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>

<span class="sd">    Args:</span>
<span class="sd">        keys (torch.Tensor): [B, L, D] Keys tensor</span>
<span class="sd">        queries (Optional[torch.Tensor]): Optional [B, M, D] Queries tensor. If None queries = keys. Defaults to None.</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, L] or [B, M, L] zero-one mask for sequence elements. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, M, L])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">queries</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">keys</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">keys</span>

    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>  <span class="c1"># (B, L, A)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

    <span class="c1"># weights =&gt; (B, L, L)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">v</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.attention.MultiheadAttention" class="doc doc-heading">
        <code>MultiheadAttention</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadAttention.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">nystrom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Multi-Headed Dot-product attention module</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>attention_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>Number of attention heads</p></td>
        <td><code>8</code></td>
      </tr>
      <tr>
        <td><code>input_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Input features. Defaults to None.
If None input_size is set to attention_size.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>nystrom</code></td>
        <td><code>bool</code></td>
        <td><p>Use nystrom method for attention calculation. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark points for nystrom attention. Defaults to 64.</p></td>
        <td><code>64</code></td>
      </tr>
      <tr>
        <td><code>inverse_iterations</code></td>
        <td><code>int</code></td>
        <td><p>Number of iteration to calculate the inverse in nystrom attention. Defaults to 6.</p></td>
        <td><code>6</code></td>
      </tr>
      <tr>
        <td><code>kernel_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Use residual convolution in the output. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attention_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">nystrom</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multi-Headed Dot-product attention module</span>

<span class="sd">    Args:</span>
<span class="sd">        attention_size (int): Number of hidden features. Defaults to 512.</span>
<span class="sd">        num_heads (int): Number of attention heads</span>
<span class="sd">        input_size (Optional[int]): Input features. Defaults to None.</span>
<span class="sd">            If None input_size is set to attention_size.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.1.</span>
<span class="sd">        nystrom (bool, optional): Use nystrom method for attention calculation. Defaults to False.</span>
<span class="sd">        num_landmarks (int, optional): Number of landmark points for nystrom attention. Defaults to 64.</span>
<span class="sd">        inverse_iterations (int, optional): Number of iteration to calculate the inverse in nystrom attention. Defaults to 6.</span>
<span class="sd">        kernel_size (Optional[int], optional): Use residual convolution in the output. Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiheadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inverse_iterations</span> <span class="o">=</span> <span class="n">inverse_iterations</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span> <span class="o">=</span> <span class="n">num_landmarks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span> <span class="o">=</span> <span class="n">nystrom</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">attention_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">attention_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">kernel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadAttention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Multi-head scaled dot-product attention forward pass</p>
<p>Outputs the values, where features for each sequence element are weighted by their respective attention scores</p>
<p>Each head performs dot-product attention</p>
<div class="arithmatex">\[a_H = softmax(\frac{Q_H \cdot K_H^T}{\sqrt{d}}) \cdot V_H\]</div>
<p>The outputs of multiple heads are concatenated and passed through a feedforward layer.</p>
<div class="arithmatex">\[a = W (a^{(1)}_{H} \mathbin\Vert a^{(2)}_{H} \dots) + b\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>keys</code></td>
        <td><code>torch.Tensor</code></td>
        <td><p>[B, L, D] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>queries</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, M, D] Queries tensor. If None queries = keys. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, M, L] zero-one mask for sequence elements. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>(Reweighted values [B, L, D], attention scores [B, H, M, L])</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Multi-head scaled dot-product attention forward pass</span>

<span class="sd">    Outputs the values, where features for each sequence element are weighted by their respective attention scores</span>

<span class="sd">    Each head performs dot-product attention</span>

<span class="sd">    $$a_H = softmax(\frac{Q_H \cdot K_H^T}{\sqrt{d}}) \cdot V_H$$</span>

<span class="sd">    The outputs of multiple heads are concatenated and passed through a feedforward layer.</span>

<span class="sd">    $$a = W (a^{(1)}_{H} \mathbin\Vert a^{(2)}_{H} \dots) + b$$</span>


<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>


<span class="sd">    Args:</span>
<span class="sd">        keys (torch.Tensor): [B, L, D] Keys tensor</span>
<span class="sd">        queries (Optional[torch.Tensor]): Optional [B, M, D] Queries tensor. If None queries = keys. Defaults to None.</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, M, L] zero-one mask for sequence elements. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, H, M, L])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span><span class="p">:</span>
        <span class="n">keys</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">pad_for_nystrom</span><span class="p">(</span>
            <span class="n">keys</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">queries</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">keys</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">keys</span>

    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span><span class="p">:</span>
        <span class="c1"># out = (B, H, L, A/H)</span>
        <span class="c1"># scores = Tuple</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">nystrom_attention</span><span class="p">(</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">inverse_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inverse_iterations</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># out =&gt; (B, H, L, A/H)</span>
        <span class="c1"># scores =&gt; (B, H, L, L)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">v</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>

    <span class="c1"># out =&gt; (B, H, L, A/H)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">merge_heads</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="n">seq_length</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_length</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.attention.MultiheadSelfAttention" class="doc doc-heading">
        <code>MultiheadSelfAttention</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadSelfAttention.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">nystrom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Multi-Headed Dot-product attention module</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>attention_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>Number of attention heads</p></td>
        <td><code>8</code></td>
      </tr>
      <tr>
        <td><code>input_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Input features. Defaults to None.
If None input_size is set to attention_size.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attention_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">nystrom</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multi-Headed Dot-product attention module</span>

<span class="sd">    Args:</span>
<span class="sd">        attention_size (int): Number of hidden features. Defaults to 512.</span>
<span class="sd">        num_heads (int): Number of attention heads</span>
<span class="sd">        input_size (Optional[int]): Input features. Defaults to None.</span>
<span class="sd">            If None input_size is set to attention_size.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiheadSelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inverse_iterations</span> <span class="o">=</span> <span class="n">inverse_iterations</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span> <span class="o">=</span> <span class="n">num_landmarks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span> <span class="o">=</span> <span class="n">nystrom</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">attention_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kqv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">attention_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">kernel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadSelfAttention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Multi-head scaled dot-product attention forward pass</p>
<p>Outputs the values, where features for each sequence element are weighted by their respective attention scores</p>
<p>Each head performs dot-product attention</p>
<div class="arithmatex">\[a_H = softmax(\frac{Q_H \cdot K_H^T}{\sqrt{d}}) \cdot V_H\]</div>
<p>The outputs of multiple heads are concatenated and passed through a feedforward layer.</p>
<div class="arithmatex">\[a = W (a^{(1)}_{H} \mathbin\Vert a^{(2)}_{H} \dots) + b\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>torch.Tensor</code></td>
        <td><p>[B, L, D] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, M, L] zero-one mask for sequence elements. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>(Reweighted values [B, L, D], attention scores [B, H, M, L])</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Multi-head scaled dot-product attention forward pass</span>

<span class="sd">    Outputs the values, where features for each sequence element are weighted by their respective attention scores</span>

<span class="sd">    Each head performs dot-product attention</span>

<span class="sd">    $$a_H = softmax(\frac{Q_H \cdot K_H^T}{\sqrt{d}}) \cdot V_H$$</span>

<span class="sd">    The outputs of multiple heads are concatenated and passed through a feedforward layer.</span>

<span class="sd">    $$a = W (a^{(1)}_{H} \mathbin\Vert a^{(2)}_{H} \dots) + b$$</span>


<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>


<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, D] Keys tensor</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, M, L] zero-one mask for sequence elements. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, H, M, L])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">pad_for_nystrom</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span>
        <span class="p">)</span>

    <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kqv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span><span class="p">:</span>
        <span class="c1"># out = (B, H, L, A/H)</span>
        <span class="c1"># scores = Tuple</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">nystrom_attention</span><span class="p">(</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">inverse_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inverse_iterations</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># out =&gt; (B, H, L, A/H)</span>
        <span class="c1"># scores =&gt; (B, H, L, L)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">v</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>

    <span class="c1"># out =&gt; (B, H, L, A/H)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">merge_heads</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="n">seq_length</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="n">seq_length</span><span class="p">:,</span> <span class="p">:]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.attention.MultiheadTwowayAttention" class="doc doc-heading">
        <code>MultiheadTwowayAttention</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadTwowayAttention.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nystrom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Multihead twoway attention for multimodal fusion</p>
<p>This module performs two way attention for two input modality feature sequences.
If att is the MultiheadAttention operation and x, y the input modality sequences,
the operation is summarized as</p>
<div class="arithmatex">\[out = (att(x \rightarrow y), att(y \rightarrow x))\]</div>
<p>If residual is True then a Vilbert-like residual connection is applied</p>
<div class="arithmatex">\[out = (att(x \rightarrow y) + x, att(y \rightarrow x) + y)\]</div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>attention_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>Number of attention heads</p></td>
        <td><code>8</code></td>
      </tr>
      <tr>
        <td><code>input_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Input features. Defaults to None.
If None input_size is set to attention_size.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>nystrom</code></td>
        <td><code>bool</code></td>
        <td><p>Use nystrom method for attention calculation. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark points for nystrom attention. Defaults to 64.</p></td>
        <td><code>64</code></td>
      </tr>
      <tr>
        <td><code>inverse_iterations</code></td>
        <td><code>int</code></td>
        <td><p>Number of iteration to calculate the inverse in nystrom attention. Defaults to 6.</p></td>
        <td><code>6</code></td>
      </tr>
      <tr>
        <td><code>kernel_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Use residual convolution in the output. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>residual</code></td>
        <td><code>bool</code></td>
        <td><p>Use vilbert-like residual connections for fusion. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attention_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">nystrom</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Multihead twoway attention for multimodal fusion</span>

<span class="sd">    This module performs two way attention for two input modality feature sequences.</span>
<span class="sd">    If att is the MultiheadAttention operation and x, y the input modality sequences,</span>
<span class="sd">    the operation is summarized as</span>

<span class="sd">    $$out = (att(x \rightarrow y), att(y \rightarrow x))$$</span>

<span class="sd">    If residual is True then a Vilbert-like residual connection is applied</span>

<span class="sd">    $$out = (att(x \rightarrow y) + x, att(y \rightarrow x) + y)$$</span>


<span class="sd">    Args:</span>
<span class="sd">        attention_size (int): Number of hidden features. Defaults to 512.</span>
<span class="sd">        num_heads (int): Number of attention heads</span>
<span class="sd">        input_size (Optional[int]): Input features. Defaults to None.</span>
<span class="sd">            If None input_size is set to attention_size.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.1.</span>
<span class="sd">        nystrom (bool, optional): Use nystrom method for attention calculation. Defaults to False.</span>
<span class="sd">        num_landmarks (int, optional): Number of landmark points for nystrom attention. Defaults to 64.</span>
<span class="sd">        inverse_iterations (int, optional): Number of iteration to calculate the inverse in nystrom attention. Defaults to 6.</span>
<span class="sd">        kernel_size (Optional[int], optional): Use residual convolution in the output. Defaults to None.</span>
<span class="sd">        residual (bool, optional): Use vilbert-like residual connections for fusion. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiheadTwowayAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">xy</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span>
        <span class="n">attention_size</span><span class="o">=</span><span class="n">attention_size</span><span class="p">,</span>
        <span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">nystrom</span><span class="o">=</span><span class="n">nystrom</span><span class="p">,</span>
        <span class="n">num_landmarks</span><span class="o">=</span><span class="n">num_landmarks</span><span class="p">,</span>
        <span class="n">inverse_iterations</span><span class="o">=</span><span class="n">inverse_iterations</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">yx</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span>
        <span class="n">attention_size</span><span class="o">=</span><span class="n">attention_size</span><span class="p">,</span>
        <span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">nystrom</span><span class="o">=</span><span class="n">nystrom</span><span class="p">,</span>
        <span class="n">num_landmarks</span><span class="o">=</span><span class="n">num_landmarks</span><span class="p">,</span>
        <span class="n">inverse_iterations</span><span class="o">=</span><span class="n">inverse_iterations</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadTwowayAttention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod1</span><span class="p">,</span> <span class="n">mod2</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>x : (B, L, D)
queries : (B, L, D)
values : (B, L, D)</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod1</span><span class="p">,</span> <span class="n">mod2</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    x : (B, L, D)</span>
<span class="sd">    queries : (B, L, D)</span>
<span class="sd">    values : (B, L, D)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out_mod1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xy</span><span class="p">(</span><span class="n">mod1</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">mod2</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
    <span class="n">out_mod2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">yx</span><span class="p">(</span><span class="n">mod2</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">mod1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out_mod1</span><span class="p">,</span> <span class="n">out_mod2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># vilbert cross residual</span>

        <span class="c1"># v + attention(v-&gt;a)</span>
        <span class="c1"># a + attention(a-&gt;v)</span>
        <span class="n">out_mod1</span> <span class="o">+=</span> <span class="n">mod2</span>
        <span class="n">out_mod2</span> <span class="o">+=</span> <span class="n">mod1</span>

        <span class="k">return</span> <span class="n">out_mod1</span><span class="p">,</span> <span class="n">out_mod2</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.attention.SelfAttention" class="doc doc-heading">
        <code>SelfAttention</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.SelfAttention.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Single-Headed Dot-product self attention module</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>attention_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>input_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Input features. Defaults to None.
If None input_size is set to attention_size.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attention_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Single-Headed Dot-product self attention module</span>

<span class="sd">    Args:</span>
<span class="sd">        attention_size (int): Number of hidden features. Defaults to 512.</span>
<span class="sd">        input_size (Optional[int]): Input features. Defaults to None.</span>
<span class="sd">            If None input_size is set to attention_size.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dk</span> <span class="o">=</span> <span class="n">input_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kqv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.SelfAttention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Single-head scaled dot-product attention forward pass</p>
<p>Outputs the values, where features for each sequence element are weighted by their respective attention scores</p>
<div class="arithmatex">\[a = softmax(\frac{Q}{K^T}){\sqrt{d}}) \dot V\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, D] Input tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, L] or [B, M, L] zero-one mask for sequence elements. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, M, L])</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Single-head scaled dot-product attention forward pass</span>

<span class="sd">    Outputs the values, where features for each sequence element are weighted by their respective attention scores</span>

<span class="sd">    $$a = softmax(\frac{Q}{K^T}){\sqrt{d}}) \dot V$$</span>

<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, D] Input tensor</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, L] or [B, M, L] zero-one mask for sequence elements. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, M, L])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kqv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, L, A)</span>

    <span class="c1"># weights =&gt; (B, L, L)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">v</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.attention" class="doc doc-heading">
<code class="highlight language-python"><span class="n">attention</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Reweight values using scaled dot product attention</p>
<div class="arithmatex">\[s = softmax(\frac{Q \cdot K^T}{\sqrt{d}}) V\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>k</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>q</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>v</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, M, A] or multi-head [B, H, M, A/H] Values tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dk</code></td>
        <td><code>int</code></td>
        <td><p>Model dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask
tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be
preserved. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>training</code></td>
        <td><code>bool</code></td>
        <td><p>Is module in training phase? Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>[B, M, L] or [B, H, M, L] attention scores</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">dk</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reweight values using scaled dot product attention</span>

<span class="sd">    $$s = softmax(\frac{Q \cdot K^T}{\sqrt{d}}) V$$</span>

<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>

<span class="sd">    Args:</span>
<span class="sd">        k (torch.Tensor): Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</span>
<span class="sd">        q (torch.Tensor): Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</span>
<span class="sd">        v (torch.Tensor): Single head [B, M, A] or multi-head [B, H, M, A/H] Values tensor</span>
<span class="sd">        dk (int): Model dimension</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask</span>
<span class="sd">            tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be</span>
<span class="sd">            preserved. Defaults to None.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.2.</span>
<span class="sd">        training (bool): Is module in training phase? Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, M, L] or [B, H, M, L] attention scores</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">(</span>
        <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span>
    <span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.attention_scores" class="doc doc-heading">
<code class="highlight language-python"><span class="n">attention_scores</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Calculate attention scores for scaled dot product attention</p>
<div class="arithmatex">\[s = softmax(\frac{Q \cdot K^T}{\sqrt{d}})\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>k</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>q</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dk</code></td>
        <td><code>int</code></td>
        <td><p>Model dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask
tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be
preserved. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>training</code></td>
        <td><code>bool</code></td>
        <td><p>Is module in training phase? Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: [B, M, L] or [B, H, M, L] attention scores</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">attention_scores</span><span class="p">(</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">dk</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculate attention scores for scaled dot product attention</span>

<span class="sd">    $$s = softmax(\frac{Q \cdot K^T}{\sqrt{d}})$$</span>

<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>

<span class="sd">    Args:</span>
<span class="sd">        k (torch.Tensor): Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</span>
<span class="sd">        q (torch.Tensor): Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</span>
<span class="sd">        dk (int): Model dimension</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask</span>
<span class="sd">            tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be</span>
<span class="sd">            preserved. Defaults to None.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.2.</span>
<span class="sd">        training (bool): Is module in training phase? Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, M, L] or [B, H, M, L] attention scores</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e5</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.merge_heads" class="doc doc-heading">
<code class="highlight language-python"><span class="n">merge_heads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Merge multiple attention heads into output tensor</p>
<p>(Batch size, Heads, Lengths, Attention size / Heads) =&gt; (Batch size, Length, Attention size)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, H, L, A/H] multi-head tensor</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor:  [B, L, A] merged / reshaped tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">merge_heads</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Merge multiple attention heads into output tensor</span>

<span class="sd">    (Batch size, Heads, Lengths, Attention size / Heads) =&gt; (Batch size, Length, Attention size)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, H, L, A/H] multi-head tensor</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor:  [B, L, A] merged / reshaped tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="c1"># x =&gt; (B, L, H, A/H)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.nystrom_attention" class="doc doc-heading">
<code class="highlight language-python"><span class="n">nystrom_attention</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Calculate attention using nystrom approximation</p>
<p>Implementation heavily based on: https://github.com/lucidrains/nystrom-attention</p>
<p>Reference: https://arxiv.org/abs/2102.03902
* B: Batch size
* L: Keys Sequence length
* M: Queries Sequence length
* H: Number of heads
* A: Feature dimension</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>k</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>q</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>v</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, M, A] or multi-head [B, H, M, A/H] Values tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dk</code></td>
        <td><code>int</code></td>
        <td><p>Model dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark points</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask
tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be
preserved. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>inverse_iterations</code></td>
        <td><code>int</code></td>
        <td><p>Number of iterations for Moore Penrose iterative inverse
approximation</p></td>
        <td><code>6</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>training</code></td>
        <td><code>bool</code></td>
        <td><p>Is module in training phase? Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>[B, M, L] or [B, H, M, L] attention scores</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">nystrom_attention</span><span class="p">(</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">dk</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate attention using nystrom approximation</span>

<span class="sd">    Implementation heavily based on: https://github.com/lucidrains/nystrom-attention</span>

<span class="sd">    Reference: https://arxiv.org/abs/2102.03902</span>
<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>

<span class="sd">    Args:</span>
<span class="sd">        k (torch.Tensor): Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</span>
<span class="sd">        q (torch.Tensor): Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</span>
<span class="sd">        v (torch.Tensor): Single head [B, M, A] or multi-head [B, H, M, A/H] Values tensor</span>
<span class="sd">        dk (int): Model dimension</span>
<span class="sd">        num_landmarks (int): Number of landmark points</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask</span>
<span class="sd">            tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be</span>
<span class="sd">            preserved. Defaults to None.</span>
<span class="sd">        inverse_iterations (int): Number of iterations for Moore Penrose iterative inverse</span>
<span class="sd">            approximation</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.2.</span>
<span class="sd">        training (bool): Is module in training phase? Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, M, L] or [B, H, M, L] attention scores</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">head_size</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="n">masked_mean_denom</span> <span class="o">=</span> <span class="n">seq_length</span> <span class="o">//</span> <span class="n">num_landmarks</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">masked_mean_denom</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">attention_mask</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">//</span> <span class="n">num_landmarks</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>  <span class="c1"># (B, 1, Landmarks)</span>
        <span class="n">mask_landmarks</span> <span class="o">=</span> <span class="p">(</span><span class="n">masked_mean_denom</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="n">masked_mean_denom</span> <span class="o">=</span> <span class="n">masked_mean_denom</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">attention_mask</span>  <span class="c1"># (B, H, L, A/H)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">attention_mask</span>  <span class="c1"># (B, H, L, A/H)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">*</span> <span class="n">attention_mask</span>  <span class="c1"># (B, H, L, A/H)</span>

        <span class="n">scores_1_mask</span> <span class="o">=</span> <span class="n">attention_mask</span> <span class="o">*</span> <span class="n">mask_landmarks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">scores_2_mask</span> <span class="o">=</span> <span class="n">mask_landmarks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask_landmarks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">scores_3_mask</span> <span class="o">=</span> <span class="n">scores_1_mask</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>

    <span class="n">q_landmarks</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>  <span class="c1"># batch_size</span>
        <span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># num_heads</span>
        <span class="n">num_landmarks</span><span class="p">,</span>  <span class="c1"># landmarks</span>
        <span class="n">seq_length</span> <span class="o">//</span> <span class="n">num_landmarks</span><span class="p">,</span>  <span class="c1"># reduced length</span>
        <span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># head_size</span>
    <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span>
    <span class="p">)</span>  <span class="c1"># (B, H, Landmarks, A/H)</span>

    <span class="n">k_landmarks</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>  <span class="c1"># batch_size</span>
        <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># num_heads</span>
        <span class="n">num_landmarks</span><span class="p">,</span>  <span class="c1"># landmarks</span>
        <span class="n">seq_length</span> <span class="o">//</span> <span class="n">num_landmarks</span><span class="p">,</span>  <span class="c1"># reduced length</span>
        <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># head size</span>
    <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span>
    <span class="p">)</span>  <span class="c1"># (B, H, Landmarks, A/H)</span>

    <span class="n">k_landmarks</span> <span class="o">=</span> <span class="n">k_landmarks</span> <span class="o">/</span> <span class="n">masked_mean_denom</span>
    <span class="n">q_landmarks</span> <span class="o">=</span> <span class="n">q_landmarks</span> <span class="o">/</span> <span class="n">masked_mean_denom</span>

    <span class="n">scores_1</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">(</span>
        <span class="n">k_landmarks</span><span class="p">,</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>  <span class="c1"># We have already accounted for dk</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">scores_1_mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">scores_2</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">(</span>
        <span class="n">k_landmarks</span><span class="p">,</span>
        <span class="n">q_landmarks</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>  <span class="c1"># We have already accounted for dk</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">scores_2_mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">scores_3</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">(</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">q_landmarks</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>  <span class="c1"># We have already accounted for dk</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">scores_3_mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">z_star</span> <span class="o">=</span> <span class="n">moore_penrose_pinv</span><span class="p">(</span><span class="n">scores_2</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="n">inverse_iterations</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores_1</span> <span class="o">@</span> <span class="n">z_star</span><span class="p">)</span> <span class="o">@</span> <span class="p">(</span><span class="n">scores_3</span> <span class="o">@</span> <span class="n">v</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="n">scores_1</span><span class="p">,</span> <span class="n">scores_2</span><span class="p">,</span> <span class="n">scores_3</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.pad_for_nystrom" class="doc doc-heading">
<code class="highlight language-python"><span class="n">pad_for_nystrom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Pad inputs and attention_mask to perform Nystrom Attention</p>
<p>Pad to nearest multiple of num_landmarks</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, A] Input tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark points</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>[B, L] Padding mask</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, Optional[torch.Tensor]]</code></td>
      <td><p>Tuple[torch.Tensor, Optional[torch.Tensor]]: Padded inputs and attention_mask</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">pad_for_nystrom</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Pad inputs and attention_mask to perform Nystrom Attention</span>

<span class="sd">    Pad to nearest multiple of num_landmarks</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, A] Input tensor</span>
<span class="sd">        num_landmarks (int): Number of landmark points</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): [B, L] Padding mask</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, Optional[torch.Tensor]]: Padded inputs and attention_mask</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">remainder</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">seq_length</span> <span class="o">/</span> <span class="n">num_landmarks</span><span class="p">),</span>
        <span class="n">seq_length</span> <span class="o">%</span> <span class="n">num_landmarks</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">remainder</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">num_landmarks</span> <span class="o">-</span> <span class="n">remainder</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.reset_parameters" class="doc doc-heading">
<code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Initialize parameters in the transformer model.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize parameters in the transformer model.&quot;&quot;&quot;</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;weight&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.split_heads" class="doc doc-heading">
<code class="highlight language-python"><span class="n">split_heads</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Split input tensor into multiple attention heads</p>
<p>(Batch size, Length, Attention size) =&gt; (Batch size, Heads, Lengths, Attention size / Heads)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, A] input tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>number of heads</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: [B, H, L, A/H] Splitted / reshaped tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Split input tensor into multiple attention heads</span>

<span class="sd">    (Batch size, Length, Attention size) =&gt; (Batch size, Heads, Lengths, Attention size / Heads)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, A] input tensor</span>
<span class="sd">        num_heads (int): number of heads</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, H, L, A/H] Splitted / reshaped tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">attention_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">head_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">attention_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.classifier"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.classifier.Classifier" class="doc doc-heading">
        <code>Classifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.classifier.Classifier.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">encoded_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Classifier wrapper module</p>
<p>Stores a Neural Network encoder and adds a classification layer on top.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>encoder</code></td>
        <td><code>Module</code></td>
        <td><p>[description]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>encoded_features</code></td>
        <td><code>int</code></td>
        <td><p>[description]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_classes</code></td>
        <td><code>int</code></td>
        <td><p>[description]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability</p></td>
        <td><code>0.2</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/classifier.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">encoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">encoded_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Classifier wrapper module</span>

<span class="sd">    Stores a Neural Network encoder and adds a classification layer on top.</span>

<span class="sd">    Args:</span>
<span class="sd">        encoder (nn.Module): [description]</span>
<span class="sd">        encoded_features (int): [description]</span>
<span class="sd">        num_classes (int): [description]</span>
<span class="sd">        dropout (float): Drop probability</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Classifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clf</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoded_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.classifier.Classifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Encode inputs using the encoder network and perform classification</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: [B, *, num_classes] Logits tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/classifier.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Encode inputs using the encoder network and perform classification</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, *, num_classes] Logits tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">encoded</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.classifier.MOSEITextClassifier" class="doc doc-heading">
        <code>MOSEITextClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.classifier.MOSEITextClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Encode inputs using the encoder network and perform classification</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>[B, *, num_classes] Logits tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/classifier.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.classifier.RNNLateFusionClassifier" class="doc doc-heading">
        <code>RNNLateFusionClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.classifier.RNNLateFusionClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/classifier.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modality_encoders</span><span class="p">[</span><span class="n">m</span><span class="p">](</span><span class="n">inputs</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="n">lengths</span><span class="p">[</span><span class="n">m</span><span class="p">])</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modalities</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span><span class="p">(</span><span class="o">*</span><span class="n">encoded</span><span class="p">)</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>





  <div class="doc doc-object doc-class">



<h2 id="slp.modules.classifier.TransformerLateFusionClassifier" class="doc doc-heading">
        <code>TransformerLateFusionClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.classifier.TransformerLateFusionClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attention_masks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/classifier.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attention_masks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">attention_masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_masks</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modalities</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modalities</span><span class="p">])</span>
        <span class="p">)</span>

    <span class="n">encoded</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modality_encoders</span><span class="p">[</span><span class="n">m</span><span class="p">](</span><span class="n">inputs</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_masks</span><span class="p">[</span><span class="n">m</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modalities</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span><span class="p">(</span><span class="o">*</span><span class="n">encoded</span><span class="p">)</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">modality_drop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">modality_drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>









  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.embed"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.embed.Embed" class="doc doc-heading">
        <code>Embed</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.embed.Embed.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Define the layer of the model and perform the initializations
of the layers (wherever it is necessary)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>num_embeddings</code></td>
        <td><code>int</code></td>
        <td><p>Total number of embeddings.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>embedding_dim</code></td>
        <td><code>int</code></td>
        <td><p>Embedding dimension.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>Optional[numpy.ndarray]</code></td>
        <td><p>the 2D ndarray with the word vectors.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>noise</code></td>
        <td><code>float</code></td>
        <td><p>Optional additive noise. Defaults to 0.0.</p></td>
        <td><code>0.0</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Embedding dropout probability. Defaults to 0.0.</p></td>
        <td><code>0.0</code></td>
      </tr>
      <tr>
        <td><code>scale</code></td>
        <td><code>float</code></td>
        <td><p>Scale word embeddings by a constant. Defaults to 1.0.</p></td>
        <td><code>1.0</code></td>
      </tr>
      <tr>
        <td><code>trainable</code></td>
        <td><code>bool</code></td>
        <td><p>Finetune embeddings. Defaults to False</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/embed.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define the layer of the model and perform the initializations</span>
<span class="sd">    of the layers (wherever it is necessary)</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings (int): Total number of embeddings.</span>
<span class="sd">        embedding_dim (int): Embedding dimension.</span>
<span class="sd">        embeddings (numpy.ndarray): the 2D ndarray with the word vectors.</span>
<span class="sd">        noise (float): Optional additive noise. Defaults to 0.0.</span>
<span class="sd">        dropout (float): Embedding dropout probability. Defaults to 0.0.</span>
<span class="sd">        scale (float): Scale word embeddings by a constant. Defaults to 1.0.</span>
<span class="sd">        trainable (bool): Finetune embeddings. Defaults to False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Embed</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>  <span class="c1"># scale embeddings by value. Needed for transformer</span>
    <span class="c1"># define the embedding layer, with the corresponding dimensions</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">num_embeddings</span><span class="o">=</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Initializing Embedding layer with pre-trained weights.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">trainable</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Embeddings are going to be finetuned&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Embeddings are frozen&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_embeddings</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">trainable</span><span class="p">)</span>

    <span class="c1"># the dropout &quot;layer&quot; for the word embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="c1"># the gaussian noise &quot;layer&quot; for the word embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">=</span> <span class="n">GaussianNoise</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.embed.Embed.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Embed input tokens</p>
<p>Assign embedding that corresponds to each token.
Optionally add Gaussian noise and embedding dropout and scale embeddings by a constant.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L] Input token ids.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>(torch.Tensor) -&gt; [B, L, E] Embedded tokens.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/embed.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Embed input tokens</span>

<span class="sd">    Assign embedding that corresponds to each token.</span>
<span class="sd">    Optionally add Gaussian noise and embedding dropout and scale embeddings by a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L] Input token ids.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (torch.Tensor) -&gt; [B, L, E] Embedded tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="o">.</span><span class="n">stddev</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">p</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">embeddings</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.embed.Embed.init_embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">init_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">trainable</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Initialize embeddings matrix with pretrained embeddings</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>weights</code></td>
        <td><code>ndarray</code></td>
        <td><p>pretrained embeddings</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>trainable</code></td>
        <td><code>bool</code></td>
        <td><p>Finetune embeddings?</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/embed.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">init_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">trainable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize embeddings matrix with pretrained embeddings</span>

<span class="sd">    Args:</span>
<span class="sd">        weights (np.ndarray): pretrained embeddings</span>
<span class="sd">        trainable (bool): Finetune embeddings?</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">trainable</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.embed.PositionalEncoding" class="doc doc-heading">
        <code>PositionalEncoding</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.embed.PositionalEncoding.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Inject some information about the relative or absolute position of the tokens in the sequence.</p>
<p>The positional encodings have the same dimension as
the embeddings, so that the two can be summed. Here, we use sine and cosine
functions of different frequencies.</p>
<p>PE for even positions:</p>
<div class="arithmatex">\[\text{PosEncoder}(pos, 2i) = sin(\frac{pos}{10000^{\frac{2i}{d}}})\]</div>
<p>PE for odd positions:</p>
<div class="arithmatex">\[\text{PosEncoder}(pos, 2i+1) = cos(\frac{pos}{10000^{\frac{2i}{d}}})\]</div>
<p>where <span class="arithmatex">\(pos\)</span> is the word position and <span class="arithmatex">\(i\)</span> is the embedding idx</p>
<p>Implementation modified from pytorch/examples/word_language_model.py</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>embedding_dim</code></td>
        <td><code>int</code></td>
        <td><p>Embedding / model dimension. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>max_len</code></td>
        <td><code>int</code></td>
        <td><p>Maximum sequence length that can be encoded. Defaults to 5000.</p></td>
        <td><code>5000</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/embed.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Inject some information about the relative or absolute position of the tokens in the sequence.</span>

<span class="sd">    The positional encodings have the same dimension as</span>
<span class="sd">    the embeddings, so that the two can be summed. Here, we use sine and cosine</span>
<span class="sd">    functions of different frequencies.</span>

<span class="sd">    PE for even positions:</span>

<span class="sd">    $$\text{PosEncoder}(pos, 2i) = sin(\frac{pos}{10000^{\frac{2i}{d}}})$$</span>

<span class="sd">    PE for odd positions:</span>

<span class="sd">    $$\text{PosEncoder}(pos, 2i+1) = cos(\frac{pos}{10000^{\frac{2i}{d}}})$$</span>

<span class="sd">    where $pos$ is the word position and $i$ is the embedding idx</span>

<span class="sd">    Implementation modified from pytorch/examples/word_language_model.py</span>

<span class="sd">    Args:</span>
<span class="sd">        embedding_dim (int): Embedding / model dimension. Defaults to 512.</span>
<span class="sd">        max_len (int): Maximum sequence length that can be encoded. Defaults to 5000.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">embedding_dim</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe&quot;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.embed.PositionalEncoding.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Calculate positional embeddings for input and add them to input tensor</p>
<div class="arithmatex">\[out = x + PosEmbed(x)\]</div>
<p>x is assumed to be batch first</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, D] input embeddings</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Embeddings + positional embeddings</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/embed.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Calculate positional embeddings for input and add them to input tensor</span>

<span class="sd">    $$out = x + PosEmbed(x)$$</span>

<span class="sd">    x is assumed to be batch first</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, D] input embeddings</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Embeddings + positional embeddings</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">:]</span>  <span class="c1"># type: ignore</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.feedforward"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-class">



<h2 id="slp.modules.feedforward.PositionwiseFF" class="doc doc-heading">
        <code>PositionwiseFF</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedforward.PositionwiseFF.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gelu</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Transformer Position-wise feed-forward layer</p>
<p>Linear -&gt; LayerNorm -&gt; ReLU -&gt; Linear</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>d_model</code></td>
        <td><code>int</code></td>
        <td><p>Model dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>d_ff</code></td>
        <td><code>int</code></td>
        <td><p>Hidden dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedforward.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">gelu</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer Position-wise feed-forward layer</span>

<span class="sd">    Linear -&gt; LayerNorm -&gt; ReLU -&gt; Linear</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): Model dimension</span>
<span class="sd">        d_ff (int): Hidden dimension</span>
<span class="sd">        dropout (float): Dropout probability. Defaults to 0.1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFF</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">gelu</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedforward.PositionwiseFF.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Position-wise FF forward pass</p>
<div class="arithmatex">\[out = W_2 \dot max(0, W_1 \dot x + b_1) + b_2\]</div>
<p>[B, <em>, D] -&gt; [B, </em>, H] -&gt; [B, *, D]</p>
<ul>
<li>B: Batch size</li>
<li>D: Model dim</li>
<li>H: Hidden size &gt; Model dim (Usually <span class="arithmatex">\(H = 2D\)</span>)</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, *, D] Input features</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: [B, *, D] Output features</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedforward.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Position-wise FF forward pass</span>

<span class="sd">    $$out = W_2 \dot max(0, W_1 \dot x + b_1) + b_2$$</span>

<span class="sd">    [B, *, D] -&gt; [B, *, H] -&gt; [B, *, D]</span>

<span class="sd">    * B: Batch size</span>
<span class="sd">    * D: Model dim</span>
<span class="sd">    * H: Hidden size &gt; Model dim (Usually $H = 2D$)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, *, D] Input features</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, *, D] Output features</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ff1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.feedforward.TwoLayer" class="doc doc-heading">
        <code>TwoLayer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedforward.TwoLayer.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/feedforward.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">out</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.norm"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-class">



<h2 id="slp.modules.norm.LayerNormTf" class="doc doc-heading">
        <code>LayerNormTf</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.norm.LayerNormTf.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Construct a layernorm module in the TF style (epsilon inside the square root).
Link: https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py#L234</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/norm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-12</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct a layernorm module in the TF style (epsilon inside the square root).</span>
<span class="sd">    Link: https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py#L234</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">LayerNormTf</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span> <span class="o">=</span> <span class="n">eps</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.norm.LayerNormTf.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Calculate Layernorm the tf way</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/norm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Calculate Layernorm the tf way&quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.norm.ScaleNorm" class="doc doc-heading">
        <code>ScaleNorm</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.norm.ScaleNorm.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/norm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">scaled_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">/</span> <span class="n">safe_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scaled_norm</span> <span class="o">*</span> <span class="n">x</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>








  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.regularization"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.regularization.GaussianNoise" class="doc doc-heading">
        <code>GaussianNoise</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.regularization.GaussianNoise.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stddev</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Additive Gaussian Noise layer</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>stddev</code></td>
        <td><code>float</code></td>
        <td><p>the standard deviation of the distribution</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>mean</code></td>
        <td><code>float</code></td>
        <td><p>the mean of the distribution</p></td>
        <td><code>0.0</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/regularization.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stddev</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">mean</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Additive Gaussian Noise layer</span>

<span class="sd">    Args:</span>
<span class="sd">        stddev (float): the standard deviation of the distribution</span>
<span class="sd">        mean (float): the mean of the distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stddev</span> <span class="o">=</span> <span class="n">stddev</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.regularization.GaussianNoise.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>String representation of class</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/regularization.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;String representation of class&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> (mean=</span><span class="si">{}</span><span class="s2">, stddev=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stddev</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.regularization.GaussianNoise.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Gaussian noise forward pass</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>Input features.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/regularization.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Gaussian noise forward pass</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): Input features.</span>

<span class="sd">    Returns:</span>
<span class="sd">        [type]: [description]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stddev</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.rnn"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.rnn.AttentiveRNN" class="doc doc-heading">
        <code>AttentiveRNN</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.AttentiveRNN.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">merge_bi</span><span class="o">=</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">,</span> <span class="n">packed_sequence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nystrom</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">33</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">return_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>RNN with embedding layer and optional attention mechanism</p>
<p>Single-headed scaled dot-product attention is used as an attention mechanism</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>input_size</code></td>
        <td><code>int</code></td>
        <td><p>Input features dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>Hidden features</p></td>
        <td><code>256</code></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Use batch first representation type. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>layers</code></td>
        <td><code>int</code></td>
        <td><p>Number of RNN layers. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use bidirectional RNNs. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>merge_bi</code></td>
        <td><code>str</code></td>
        <td><p>How bidirectional states are merged. Defaults to "cat".</p></td>
        <td><code>&#39;cat&#39;</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability. Defaults to 0.0.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>rnn_type</code></td>
        <td><code>str</code></td>
        <td><p>lstm or gru. Defaults to "lstm".</p></td>
        <td><code>&#39;lstm&#39;</code></td>
      </tr>
      <tr>
        <td><code>packed_sequence</code></td>
        <td><code>bool</code></td>
        <td><p>Use packed sequences. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Maximum sequence length for fixed length padding. If -1 takes the
largest sequence length in this batch</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>attention</code></td>
        <td><code>bool</code></td>
        <td><p>Use attention mechanism. Defaults to False</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>Number of attention heads. If 1 uses single headed attention</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>nystrom</code></td>
        <td><code>bool</code></td>
        <td><p>Use nystrom approximation for multihead attention</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark sequence elements for nystrom attention</p></td>
        <td><code>32</code></td>
      </tr>
      <tr>
        <td><code>kernel_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Kernel size for multihead attention output residual convolution</p></td>
        <td><code>33</code></td>
      </tr>
      <tr>
        <td><code>inverse_iterations</code></td>
        <td><code>int</code></td>
        <td><p>Number of iterations for moore-penrose inverse approximation
in nystrom attention. 6 is a good value</p></td>
        <td><code>6</code></td>
      </tr>
      <tr>
        <td><code>return_hidden</code></td>
        <td><code>bool</code></td>
        <td><p>Return all hidden states. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">merge_bi</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">packed_sequence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">nystrom</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">33</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">return_hidden</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;RNN with embedding layer and optional attention mechanism</span>

<span class="sd">    Single-headed scaled dot-product attention is used as an attention mechanism</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Input features dimension</span>
<span class="sd">        hidden_size (int): Hidden features</span>
<span class="sd">        batch_first (bool): Use batch first representation type. Defaults to True.</span>
<span class="sd">        layers (int): Number of RNN layers. Defaults to 1.</span>
<span class="sd">        bidirectional (bool): Use bidirectional RNNs. Defaults to False.</span>
<span class="sd">        merge_bi (str): How bidirectional states are merged. Defaults to &quot;cat&quot;.</span>
<span class="sd">        dropout (float): Dropout probability. Defaults to 0.0.</span>
<span class="sd">        rnn_type (str): lstm or gru. Defaults to &quot;lstm&quot;.</span>
<span class="sd">        packed_sequence (bool): Use packed sequences. Defaults to True.</span>
<span class="sd">        max_length (int): Maximum sequence length for fixed length padding. If -1 takes the</span>
<span class="sd">            largest sequence length in this batch</span>
<span class="sd">        attention (bool): Use attention mechanism. Defaults to False</span>
<span class="sd">        num_heads (int): Number of attention heads. If 1 uses single headed attention</span>
<span class="sd">        nystrom (bool): Use nystrom approximation for multihead attention</span>
<span class="sd">        num_landmarks (int): Number of landmark sequence elements for nystrom attention</span>
<span class="sd">        kernel_size (int): Kernel size for multihead attention output residual convolution</span>
<span class="sd">        inverse_iterations (int): Number of iterations for moore-penrose inverse approximation</span>
<span class="sd">            in nystrom attention. 6 is a good value</span>
<span class="sd">        return_hidden (bool): Return all hidden states. Defaults to False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AttentiveRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span>
        <span class="n">input_size</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">merge_bi</span><span class="o">=</span><span class="n">merge_bi</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">packed_sequence</span><span class="o">=</span><span class="n">packed_sequence</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">hidden_size</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">bidirectional</span> <span class="ow">and</span> <span class="n">merge_bi</span> <span class="o">==</span> <span class="s2">&quot;cat&quot;</span><span class="p">)</span>
        <span class="k">else</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">return_hidden</span> <span class="o">=</span> <span class="n">return_hidden</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">attention</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span>
                <span class="n">attention_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">attention_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_size</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">nystrom</span><span class="o">=</span><span class="n">nystrom</span><span class="p">,</span>
                <span class="n">num_landmarks</span><span class="o">=</span><span class="n">num_landmarks</span><span class="p">,</span>
                <span class="n">inverse_iterations</span><span class="o">=</span><span class="n">inverse_iterations</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.AttentiveRNN.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Attentive RNN forward pass</p>
<p>If self.attention=True then the outputs are the weighted sum of the RNN hidden states with the attention score weights
Else the output is the last hidden state of the RNN.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L] Input token ids</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B] Original sequence lengths</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]</code></td>
      <td><p>Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
    if return_hidden == False: Returns a tensor [B, H] or [B, 2<em>H] of output features to be used for classification
    if return_hidden == True: Returns a tensor [B, H] or [B, 2</em>H] of output features to
        be used for classification, and a tensor of all the hidden states</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Attentive RNN forward pass</span>

<span class="sd">    If self.attention=True then the outputs are the weighted sum of the RNN hidden states with the attention score weights</span>
<span class="sd">    Else the output is the last hidden state of the RNN.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L] Input token ids</span>
<span class="sd">        lengths (torch.Tensor): [B] Original sequence lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:</span>
<span class="sd">            if return_hidden == False: Returns a tensor [B, H] or [B, 2*H] of output features to be used for classification</span>
<span class="sd">            if return_hidden == True: Returns a tensor [B, H] or [B, 2*H] of output features to</span>
<span class="sd">                be used for classification, and a tensor of all the hidden states</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">last_hidden</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
            <span class="n">states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">pad_mask</span><span class="p">(</span>
                <span class="n">lengths</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">states</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="k">else</span> <span class="n">states</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">states</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_hidden</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">states</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.rnn.RNN" class="doc doc-heading">
        <code>RNN</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.rnn.RNN.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>RNN output features size</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: RNN output features size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.RNN.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">merge_bi</span><span class="o">=</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">,</span> <span class="n">packed_sequence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>LSTM - GRU wrapper with packed sequence support and handling for bidirectional / last output states</p>
<p>It is recommended to run with batch_first=True because the rest of the code is built with this assumption</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>input_size</code></td>
        <td><code>int</code></td>
        <td><p>Input features.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>Hidden features.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Use batch first representation type. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>layers</code></td>
        <td><code>int</code></td>
        <td><p>Number of RNN layers. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use bidirectional RNNs. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>merge_bi</code></td>
        <td><code>str</code></td>
        <td><p>How bidirectional states are merged. Defaults to "cat".</p></td>
        <td><code>&#39;cat&#39;</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability. Defaults to 0.0.</p></td>
        <td><code>0.0</code></td>
      </tr>
      <tr>
        <td><code>rnn_type</code></td>
        <td><code>str</code></td>
        <td><p>lstm or gru. Defaults to "lstm".</p></td>
        <td><code>&#39;lstm&#39;</code></td>
      </tr>
      <tr>
        <td><code>packed_sequence</code></td>
        <td><code>bool</code></td>
        <td><p>Use packed sequences. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">merge_bi</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">packed_sequence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;LSTM - GRU wrapper with packed sequence support and handling for bidirectional / last output states</span>

<span class="sd">    It is recommended to run with batch_first=True because the rest of the code is built with this assumption</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Input features.</span>
<span class="sd">        hidden_size (int): Hidden features.</span>
<span class="sd">        batch_first (bool): Use batch first representation type. Defaults to True.</span>
<span class="sd">        layers (int): Number of RNN layers. Defaults to 1.</span>
<span class="sd">        bidirectional (bool): Use bidirectional RNNs. Defaults to False.</span>
<span class="sd">        merge_bi (str): How bidirectional states are merged. Defaults to &quot;cat&quot;.</span>
<span class="sd">        dropout (float): Dropout probability. Defaults to 0.0.</span>
<span class="sd">        rnn_type (str): lstm or gru. Defaults to &quot;lstm&quot;.</span>
<span class="sd">        packed_sequence (bool): Use packed sequences. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">merge_bi</span> <span class="o">=</span> <span class="n">merge_bi</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">=</span> <span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_first</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;You are running RNN with batch_first=False. Make sure this is really what you want&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">packed_sequence</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;You have set packed_sequence=False. Running with packed_sequence=True will be much faster&quot;</span>
        <span class="p">)</span>

    <span class="n">rnn_cls</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">rnn_cls</span><span class="p">(</span>
        <span class="n">input_size</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">packed_sequence</span> <span class="o">=</span> <span class="n">packed_sequence</span>

    <span class="k">if</span> <span class="n">packed_sequence</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pack</span> <span class="o">=</span> <span class="n">PackSequence</span><span class="p">(</span><span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unpack</span> <span class="o">=</span> <span class="n">PadPackedSequence</span><span class="p">(</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span>
        <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.RNN.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>RNN forward pass</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, D] Input features</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B] Original sequence lengths</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor, Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: (
    merged forward and backward states [B, L, H] or [B, L, 2<em>H],
    merged last forward and backward state [B, H] or [B, 2</em>H],
    hidden states tuple of [num_layers * num_directions, B, H] for LSTM or tensor [num_layers * num_directions, B, H] for GRU
)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
<span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;RNN forward pass</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, D] Input features</span>
<span class="sd">        lengths (torch.Tensor): [B] Original sequence lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: (</span>
<span class="sd">            merged forward and backward states [B, L, H] or [B, L, 2*H],</span>
<span class="sd">            merged last forward and backward state [B, H] or [B, 2*H],</span>
<span class="sd">            hidden states tuple of [num_layers * num_directions, B, H] for LSTM or tensor [num_layers * num_directions, B, H] for GRU</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">flatten_parameters</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">packed_sequence</span><span class="p">:</span>
        <span class="c1"># Latest pytorch allows only cpu tensors for packed sequence</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">packed_sequence</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">out</span><span class="p">,</span> <span class="n">last_timestep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_final_output</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">last_timestep</span><span class="p">,</span> <span class="n">hidden</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.rnn.TokenRNN" class="doc doc-heading">
        <code>TokenRNN</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.TokenRNN.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">finetune_embeddings</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">merge_bi</span><span class="o">=</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">,</span> <span class="n">packed_sequence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nystrom</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">33</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">return_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>RNN with embedding layer and optional attention mechanism</p>
<p>Single-headed scaled dot-product attention is used as an attention mechanism</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>Hidden features</p></td>
        <td><code>256</code></td>
      </tr>
      <tr>
        <td><code>vocab_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Vocabulary size. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings_dim</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Embedding dimension. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>Optional[numpy.ndarray]</code></td>
        <td><p>Embedding matrix. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings_dropout</code></td>
        <td><code>float</code></td>
        <td><p>Embedding dropout probability. Defaults to 0.0.</p></td>
        <td><code>0.0</code></td>
      </tr>
      <tr>
        <td><code>finetune_embeddings</code></td>
        <td><code>bool</code></td>
        <td><p>Finetune embeddings? Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Use batch first representation type. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>layers</code></td>
        <td><code>int</code></td>
        <td><p>Number of RNN layers. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use bidirectional RNNs. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>merge_bi</code></td>
        <td><code>str</code></td>
        <td><p>How bidirectional states are merged. Defaults to "cat".</p></td>
        <td><code>&#39;cat&#39;</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability. Defaults to 0.0.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>rnn_type</code></td>
        <td><code>str</code></td>
        <td><p>lstm or gru. Defaults to "lstm".</p></td>
        <td><code>&#39;lstm&#39;</code></td>
      </tr>
      <tr>
        <td><code>packed_sequence</code></td>
        <td><code>bool</code></td>
        <td><p>Use packed sequences. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Maximum sequence length for fixed length padding. If -1 takes the
largest sequence length in this batch</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>attention</code></td>
        <td><code>bool</code></td>
        <td><p>Use attention mechanism. Defaults to False</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>Number of attention heads. If 1 uses single headed attention</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>nystrom</code></td>
        <td><code>bool</code></td>
        <td><p>Use nystrom approximation for multihead attention</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark sequence elements for nystrom attention</p></td>
        <td><code>32</code></td>
      </tr>
      <tr>
        <td><code>kernel_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Kernel size for multihead attention output residual convolution</p></td>
        <td><code>33</code></td>
      </tr>
      <tr>
        <td><code>inverse_iterations</code></td>
        <td><code>int</code></td>
        <td><p>Number of iterations for moore-penrose inverse approximation
in nystrom attention. 6 is a good value</p></td>
        <td><code>6</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">finetune_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">merge_bi</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">packed_sequence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">nystrom</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">33</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">return_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;RNN with embedding layer and optional attention mechanism</span>

<span class="sd">    Single-headed scaled dot-product attention is used as an attention mechanism</span>

<span class="sd">    Args:</span>
<span class="sd">        hidden_size (int): Hidden features</span>
<span class="sd">        vocab_size (Optional[int]): Vocabulary size. Defaults to None.</span>
<span class="sd">        embeddings_dim (Optional[int]): Embedding dimension. Defaults to None.</span>
<span class="sd">        embeddings (Optional[np.ndarray]): Embedding matrix. Defaults to None.</span>
<span class="sd">        embeddings_dropout (float): Embedding dropout probability. Defaults to 0.0.</span>
<span class="sd">        finetune_embeddings (bool): Finetune embeddings? Defaults to False.</span>
<span class="sd">        batch_first (bool): Use batch first representation type. Defaults to True.</span>
<span class="sd">        layers (int): Number of RNN layers. Defaults to 1.</span>
<span class="sd">        bidirectional (bool): Use bidirectional RNNs. Defaults to False.</span>
<span class="sd">        merge_bi (str): How bidirectional states are merged. Defaults to &quot;cat&quot;.</span>
<span class="sd">        dropout (float): Dropout probability. Defaults to 0.0.</span>
<span class="sd">        rnn_type (str): lstm or gru. Defaults to &quot;lstm&quot;.</span>
<span class="sd">        packed_sequence (bool): Use packed sequences. Defaults to True.</span>
<span class="sd">        max_length (int): Maximum sequence length for fixed length padding. If -1 takes the</span>
<span class="sd">            largest sequence length in this batch</span>
<span class="sd">        attention (bool): Use attention mechanism. Defaults to False</span>
<span class="sd">        num_heads (int): Number of attention heads. If 1 uses single headed attention</span>
<span class="sd">        nystrom (bool): Use nystrom approximation for multihead attention</span>
<span class="sd">        num_landmarks (int): Number of landmark sequence elements for nystrom attention</span>
<span class="sd">        kernel_size (int): Kernel size for multihead attention output residual convolution</span>
<span class="sd">        inverse_iterations (int): Number of iterations for moore-penrose inverse approximation</span>
<span class="sd">            in nystrom attention. 6 is a good value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TokenRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">finetune_embeddings</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;You should either pass an embeddings matrix or vocab size&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">embeddings_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;You should either pass an embeddings matrix or embeddings_dim&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">embeddings_dim</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">embeddings_dim</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">embeddings_dropout</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">hidden_size</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">finetune_embeddings</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">AttentiveRNN</span><span class="p">(</span>
        <span class="n">embeddings_dim</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">merge_bi</span><span class="o">=</span><span class="n">merge_bi</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">packed_sequence</span><span class="o">=</span><span class="n">packed_sequence</span><span class="p">,</span>
        <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">nystrom</span><span class="o">=</span><span class="n">nystrom</span><span class="p">,</span>
        <span class="n">num_landmarks</span><span class="o">=</span><span class="n">num_landmarks</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">inverse_iterations</span><span class="o">=</span><span class="n">inverse_iterations</span><span class="p">,</span>
        <span class="n">return_hidden</span><span class="o">=</span><span class="n">return_hidden</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">out_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">out_size</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.TokenRNN.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Token RNN forward pass</p>
<p>If self.attention=True then the outputs are the weighted sum of the RNN hidden states with the attention score weights
Else the output is the last hidden state of the RNN.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L] Input token ids</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B] Original sequence lengths</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]</code></td>
      <td><p>torch.Tensor: [B, H] or [B, 2*H] Output features to be used for classification</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Token RNN forward pass</span>

<span class="sd">    If self.attention=True then the outputs are the weighted sum of the RNN hidden states with the attention score weights</span>
<span class="sd">    Else the output is the last hidden state of the RNN.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L] Input token ids</span>
<span class="sd">        lengths (torch.Tensor): [B] Original sequence lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, H] or [B, 2*H] Output features to be used for classification</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.transformer"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Decoder" class="doc doc-heading">
        <code>Decoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Decoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span>
            <span class="n">target</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="n">target_mask</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">target</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.DecoderLayer" class="doc doc-heading">
        <code>DecoderLayer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.DecoderLayer.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">target_mask</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fuse_layer</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Encoder" class="doc doc-heading">
        <code>Encoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Encoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.EncoderDecoder" class="doc doc-heading">
        <code>EncoderDecoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.EncoderDecoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
        <span class="n">target</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="n">target_mask</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">decoded</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.EncoderLayer" class="doc doc-heading">
        <code>EncoderLayer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.EncoderLayer.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Sublayer1" class="doc doc-heading">
        <code>Sublayer1</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Sublayer1.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prenorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prenorm</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_postnorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Sublayer2" class="doc doc-heading">
        <code>Sublayer2</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Sublayer2.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prenorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prenorm</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_postnorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Sublayer3" class="doc doc-heading">
        <code>Sublayer3</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Sublayer3.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prenorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prenorm</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_postnorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Transformer" class="doc doc-heading">
        <code>Transformer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Transformer.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">source</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="c1"># Adding embeddings + pos embeddings</span>
    <span class="c1"># is done in PositionalEncoding class</span>
    <span class="n">source</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_block</span><span class="p">(</span>
        <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="n">target_mask</span>
    <span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.TransformerSequenceEncoder" class="doc doc-heading">
        <code>TransformerSequenceEncoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.TransformerSequenceEncoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_norm</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.TransformerTokenSequenceEncoder" class="doc doc-heading">
        <code>TransformerTokenSequenceEncoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.TransformerTokenSequenceEncoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="slp.modules.transformer.reset_parameters" class="doc doc-heading">
<code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Initialize parameters in the transformer model.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize parameters in the transformer model.&quot;&quot;&quot;</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;weight&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>

            <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.plbind.dm"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.dm.PLDataModuleFromCorpus" class="doc doc-heading">
        <code>PLDataModuleFromCorpus</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">







  <div class="doc doc-object doc-attribute">



<h3 id="slp.plbind.dm.PLDataModuleFromCorpus.embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">numpy</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Embeddings matrix</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Optional[numpy.ndarray]</code></td>
      <td><p>Optional[np.ndarray]: Embeddings matrix</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.plbind.dm.PLDataModuleFromCorpus.vocab_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Number of tokens in the vocabulary</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Number of tokens in the vocabulary</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.dm.PLDataModuleFromCorpus.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">train_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test_labels</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val_percent</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">test_percent</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">batch_size_eval</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">shuffle_eval</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sampler_train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sampler_val</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sampler_test</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_sampler_train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_sampler_val</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_sampler_test</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">language_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">tokenizer</span><span class="o">=</span><span class="s1">&#39;spacy&#39;</span><span class="p">,</span> <span class="n">no_test_set</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">corpus_args</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Wrap raw corpus in a LightningDataModule</p>
<ul>
<li>This handles the selection of the appropriate corpus class based on the tokenizer argument.</li>
<li>If language_model=True it uses the appropriate dataset from slp.data.datasets.</li>
<li>Uses the PLDataModuleFromDatasets to split the val and test sets if not provided</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>train</code></td>
        <td><code>List</code></td>
        <td><p>Raw train corpus</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>train_labels</code></td>
        <td><code>Optional[List]</code></td>
        <td><p>Train labels. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>val</code></td>
        <td><code>Optional[List]</code></td>
        <td><p>Raw validation corpus. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>val_labels</code></td>
        <td><code>Optional[List]</code></td>
        <td><p>Validation labels. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>test</code></td>
        <td><code>Optional[List]</code></td>
        <td><p>Raw test corpus. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>test_labels</code></td>
        <td><code>Optional[List]</code></td>
        <td><p>Test labels. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>val_percent</code></td>
        <td><code>float</code></td>
        <td><p>Percent of train to be used for validation if no validation set is given. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>test_percent</code></td>
        <td><code>float</code></td>
        <td><p>Percent of train to be used for test set if no test set is given. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Training batch size. Defaults to 1.</p></td>
        <td><code>64</code></td>
      </tr>
      <tr>
        <td><code>batch_size_eval</code></td>
        <td><code>int</code></td>
        <td><p>Validation and test batch size. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>seed</code></td>
        <td><code>int</code></td>
        <td><p>Seed for deterministic run. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>num_workers</code></td>
        <td><code>int</code></td>
        <td><p>Number of workers in the DataLoader. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>pin_memory</code></td>
        <td><code>bool</code></td>
        <td><p>Pin tensors to GPU memory. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>drop_last</code></td>
        <td><code>bool</code></td>
        <td><p>Drop last incomplete batch. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>sampler_train</code></td>
        <td><code>Sampler</code></td>
        <td><p>Sampler for train loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>sampler_val</code></td>
        <td><code>Sampler</code></td>
        <td><p>Sampler for validation loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>sampler_test</code></td>
        <td><code>Sampler</code></td>
        <td><p>Sampler for test loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>batch_sampler_train</code></td>
        <td><code>BatchSampler</code></td>
        <td><p>Batch sampler for train loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>batch_sampler_val</code></td>
        <td><code>BatchSampler</code></td>
        <td><p>Batch sampler for validation loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>batch_sampler_test</code></td>
        <td><code>BatchSampler</code></td>
        <td><p>Batch sampler for test loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>shuffle_eval</code></td>
        <td><code>bool</code></td>
        <td><p>Shuffle validation and test dataloaders. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>collate_fn</code></td>
        <td><code>Optional[Callable[..., Any]]</code></td>
        <td><p>Collator function. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>language_model</code></td>
        <td><code>bool</code></td>
        <td><p>Use corpus for Language Modeling. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>tokenizer</code></td>
        <td><code>str</code></td>
        <td><p>Select one of the cls.accepted_tokenizers. Defaults to "spacy".</p></td>
        <td><code>&#39;spacy&#39;</code></td>
      </tr>
      <tr>
        <td><code>no_test_set</code></td>
        <td><code>bool</code></td>
        <td><p>Do not create test set. Useful for tuning</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>**corpus_args</code></td>
        <td><code>kwargs</code></td>
        <td><p>Extra arguments to be passed to the corpus. See
slp/data/corpus.py</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Exceptions:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>ValueError</code></td>
        <td><p>[description]</p></td>
      </tr>
      <tr>
        <td><code>ValueError</code></td>
        <td><p>[description]</p></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/dm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train</span><span class="p">:</span> <span class="n">List</span><span class="p">,</span>
    <span class="n">train_labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">val</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">val_labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">test</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">test_labels</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">val_percent</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">test_percent</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">batch_size_eval</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">shuffle_eval</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sampler_train</span><span class="p">:</span> <span class="n">Sampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sampler_val</span><span class="p">:</span> <span class="n">Sampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sampler_test</span><span class="p">:</span> <span class="n">Sampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_sampler_train</span><span class="p">:</span> <span class="n">BatchSampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_sampler_val</span><span class="p">:</span> <span class="n">BatchSampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_sampler_test</span><span class="p">:</span> <span class="n">BatchSampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">language_model</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">tokenizer</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;spacy&quot;</span><span class="p">,</span>
    <span class="n">no_test_set</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">corpus_args</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrap raw corpus in a LightningDataModule</span>

<span class="sd">    * This handles the selection of the appropriate corpus class based on the tokenizer argument.</span>
<span class="sd">    * If language_model=True it uses the appropriate dataset from slp.data.datasets.</span>
<span class="sd">    * Uses the PLDataModuleFromDatasets to split the val and test sets if not provided</span>

<span class="sd">    Args:</span>
<span class="sd">        train (List): Raw train corpus</span>
<span class="sd">        train_labels (Optional[List]): Train labels. Defaults to None.</span>
<span class="sd">        val (Optional[List]): Raw validation corpus. Defaults to None.</span>
<span class="sd">        val_labels (Optional[List]): Validation labels. Defaults to None.</span>
<span class="sd">        test (Optional[List]): Raw test corpus. Defaults to None.</span>
<span class="sd">        test_labels (Optional[List]): Test labels. Defaults to None.</span>
<span class="sd">        val_percent (float): Percent of train to be used for validation if no validation set is given. Defaults to 0.2.</span>
<span class="sd">        test_percent (float): Percent of train to be used for test set if no test set is given. Defaults to 0.2.</span>
<span class="sd">        batch_size (int): Training batch size. Defaults to 1.</span>
<span class="sd">        batch_size_eval (Optional[int]): Validation and test batch size. Defaults to None.</span>
<span class="sd">        seed (Optional[int]): Seed for deterministic run. Defaults to None.</span>
<span class="sd">        num_workers (int): Number of workers in the DataLoader. Defaults to 1.</span>
<span class="sd">        pin_memory (bool): Pin tensors to GPU memory. Defaults to True.</span>
<span class="sd">        drop_last (bool): Drop last incomplete batch. Defaults to False.</span>
<span class="sd">        sampler_train (Sampler): Sampler for train loader. Defaults to None.</span>
<span class="sd">        sampler_val (Sampler): Sampler for validation loader. Defaults to None.</span>
<span class="sd">        sampler_test (Sampler): Sampler for test loader. Defaults to None.</span>
<span class="sd">        batch_sampler_train (BatchSampler): Batch sampler for train loader. Defaults to None.</span>
<span class="sd">        batch_sampler_val (BatchSampler): Batch sampler for validation loader. Defaults to None.</span>
<span class="sd">        batch_sampler_test (BatchSampler): Batch sampler for test loader. Defaults to None.</span>
<span class="sd">        shuffle_eval (bool): Shuffle validation and test dataloaders. Defaults to False.</span>
<span class="sd">        collate_fn (Callable[..., Any]): Collator function. Defaults to None.</span>
<span class="sd">        language_model (bool): Use corpus for Language Modeling. Defaults to False.</span>
<span class="sd">        tokenizer (str): Select one of the cls.accepted_tokenizers. Defaults to &quot;spacy&quot;.</span>
<span class="sd">        no_test_set (bool): Do not create test set. Useful for tuning</span>
<span class="sd">        **corpus_args (kwargs): Extra arguments to be passed to the corpus. See</span>
<span class="sd">            slp/data/corpus.py</span>
<span class="sd">    Raises:</span>
<span class="sd">        ValueError: [description]</span>
<span class="sd">        ValueError: [description]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">language_model</span> <span class="o">=</span> <span class="n">language_model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">tokenizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">corpus_args</span> <span class="o">=</span> <span class="n">corpus_args</span>

    <span class="n">train_data</span><span class="p">,</span> <span class="n">val_data</span><span class="p">,</span> <span class="n">test_data</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_zip_corpus_and_labels</span><span class="p">(</span>
        <span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="p">,</span> <span class="n">test</span><span class="p">,</span> <span class="n">train_labels</span><span class="p">,</span> <span class="n">val_labels</span><span class="p">,</span> <span class="n">test_labels</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">no_test_set</span> <span class="o">=</span> <span class="n">no_test_set</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PLDataModuleFromCorpus</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">train_data</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">val</span><span class="o">=</span><span class="n">val_data</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">test</span><span class="o">=</span><span class="n">test_data</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">val_percent</span><span class="o">=</span><span class="n">val_percent</span><span class="p">,</span>
        <span class="n">test_percent</span><span class="o">=</span><span class="n">test_percent</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span>
        <span class="n">batch_size_eval</span><span class="o">=</span><span class="n">batch_size_eval</span><span class="p">,</span>
        <span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="n">pin_memory</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="o">=</span><span class="n">drop_last</span><span class="p">,</span>
        <span class="n">shuffle_eval</span><span class="o">=</span><span class="n">shuffle_eval</span><span class="p">,</span>
        <span class="n">sampler_train</span><span class="o">=</span><span class="n">sampler_train</span><span class="p">,</span>
        <span class="n">sampler_val</span><span class="o">=</span><span class="n">sampler_val</span><span class="p">,</span>
        <span class="n">sampler_test</span><span class="o">=</span><span class="n">sampler_test</span><span class="p">,</span>
        <span class="n">batch_sampler_train</span><span class="o">=</span><span class="n">batch_sampler_train</span><span class="p">,</span>
        <span class="n">batch_sampler_val</span><span class="o">=</span><span class="n">batch_sampler_val</span><span class="p">,</span>
        <span class="n">batch_sampler_test</span><span class="o">=</span><span class="n">batch_sampler_test</span><span class="p">,</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="n">collate_fn</span><span class="p">,</span>
        <span class="n">no_test_set</span><span class="o">=</span><span class="n">no_test_set</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.dm.PLDataModuleFromCorpus.add_argparse_args" class="doc doc-heading">
<code class="highlight language-python"><span class="n">add_argparse_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-classmethod"><code>classmethod</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Augment input parser with arguments for data loading and corpus processing</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>parent_parser</code></td>
        <td><code>argparse.ArgumentParser</code></td>
        <td><p>Parser created by the user</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>argparse.ArgumentParser</code></td>
      <td><p>Augmented parser</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/dm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">add_argparse_args</span><span class="p">(</span><span class="bp">cls</span><span class="p">,</span> <span class="n">parent_parser</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Augment input parser with arguments for data loading and corpus processing</span>

<span class="sd">    Args:</span>
<span class="sd">        parent_parser (argparse.ArgumentParser): Parser created by the user</span>

<span class="sd">    Returns:</span>
<span class="sd">        argparse.ArgumentParser: Augmented parser</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="nb">super</span><span class="p">(</span><span class="n">PLDataModuleFromCorpus</span><span class="p">,</span> <span class="bp">cls</span><span class="p">)</span><span class="o">.</span><span class="n">add_argparse_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--tokenizer&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.tokenizer&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="o">.</span><span class="n">lower</span><span class="p">,</span>
        <span class="c1"># Corpus can already be tokenized, you can use spacy for word tokenization or any tokenizer from hugging face</span>
        <span class="n">choices</span><span class="o">=</span><span class="bp">cls</span><span class="o">.</span><span class="n">accepted_tokenizers</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;spacy&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Token type. The tokenization will happen at this level.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Only when tokenizer == spacy</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--limit-vocab&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.limit_vocab_size&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Limit vocab size. -1 means use the whole vocab. Applicable only when --tokenizer=spacy&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--embeddings-file&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.embeddings_file&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="n">dir_path</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Path to file with pretrained embeddings. Applicable only when --tokenizer=spacy&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--embeddings-dim&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.embeddings_dim&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Embedding dim of pretrained embeddings. Applicable only when --tokenizer=spacy&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lang&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.lang&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;en_core_web_md&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Language for spacy tokenizer, e.g. en_core_web_md. Applicable only when --tokenizer=spacy&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--no-add-specials&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.add_special_tokens&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_false&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Do not add special tokens for hugging face tokenizers&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="c1"># Generic args</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lower&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.lower&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Convert to lowercase.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--prepend-bos&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.prepend_bos&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Prepend [BOS] token&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--append-eos&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.append_eos&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Append [EOS] token&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--max-sentence-length&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.max_len&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Maximum allowed sentence length. -1 means use the whole sentence&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.dm.PLDataModuleFromDatasets" class="doc doc-heading">
        <code>PLDataModuleFromDatasets</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.dm.PLDataModuleFromDatasets.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">test</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val_percent</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">test_percent</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">batch_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">batch_size_eval</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">num_workers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">pin_memory</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">drop_last</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">sampler_train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sampler_val</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">sampler_test</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_sampler_train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_sampler_val</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_sampler_test</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">shuffle_eval</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">no_test_set</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>LightningDataModule wrapper for generic torch.utils.data.Dataset</p>
<p>If val or test Datasets are not provided, this class will split
val_pecent and test_percent of the train set respectively to create them</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>train</code></td>
        <td><code>Dataset</code></td>
        <td><p>Train set</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>val</code></td>
        <td><code>Dataset</code></td>
        <td><p>Validation set. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>test</code></td>
        <td><code>Dataset</code></td>
        <td><p>Test set. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>val_percent</code></td>
        <td><code>float</code></td>
        <td><p>Percent of train to be used for validation if no validation set is given. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>test_percent</code></td>
        <td><code>float</code></td>
        <td><p>Percent of train to be used for test set if no test set is given. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>batch_size</code></td>
        <td><code>int</code></td>
        <td><p>Training batch size. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>batch_size_eval</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Validation and test batch size. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>seed</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Seed for deterministic run. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>num_workers</code></td>
        <td><code>int</code></td>
        <td><p>Number of workers in the DataLoader. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>pin_memory</code></td>
        <td><code>bool</code></td>
        <td><p>Pin tensors to GPU memory. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>drop_last</code></td>
        <td><code>bool</code></td>
        <td><p>Drop last incomplete batch. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>sampler_train</code></td>
        <td><code>Sampler</code></td>
        <td><p>Sampler for train loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>sampler_val</code></td>
        <td><code>Sampler</code></td>
        <td><p>Sampler for validation loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>sampler_test</code></td>
        <td><code>Sampler</code></td>
        <td><p>Sampler for test loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>batch_sampler_train</code></td>
        <td><code>BatchSampler</code></td>
        <td><p>Batch sampler for train loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>batch_sampler_val</code></td>
        <td><code>BatchSampler</code></td>
        <td><p>Batch sampler for validation loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>batch_sampler_test</code></td>
        <td><code>BatchSampler</code></td>
        <td><p>Batch sampler for test loader. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>shuffle_eval</code></td>
        <td><code>bool</code></td>
        <td><p>Shuffle validation and test dataloaders. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>collate_fn</code></td>
        <td><code>Optional[Callable[..., Any]]</code></td>
        <td><p>Collator function. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>no_test_set</code></td>
        <td><code>bool</code></td>
        <td><p>Do not create test set. Useful for tuning</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Exceptions:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>ValueError</code></td>
        <td><p>If both mutually exclusive sampler_train and batch_sampler_train are provided</p></td>
      </tr>
      <tr>
        <td><code>ValueError</code></td>
        <td><p>If both mutually exclusive sampler_val and batch_sampler_val are provided</p></td>
      </tr>
      <tr>
        <td><code>ValueError</code></td>
        <td><p>If both mutually exclusive sampler_test and batch_sampler_test are provided</p></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/dm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">train</span><span class="p">:</span> <span class="n">Dataset</span><span class="p">,</span>
    <span class="n">val</span><span class="p">:</span> <span class="n">Dataset</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">test</span><span class="p">:</span> <span class="n">Dataset</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">val_percent</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">test_percent</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">batch_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">batch_size_eval</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">seed</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">num_workers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">pin_memory</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">drop_last</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">sampler_train</span><span class="p">:</span> <span class="n">Sampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sampler_val</span><span class="p">:</span> <span class="n">Sampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">sampler_test</span><span class="p">:</span> <span class="n">Sampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_sampler_train</span><span class="p">:</span> <span class="n">BatchSampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_sampler_val</span><span class="p">:</span> <span class="n">BatchSampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_sampler_test</span><span class="p">:</span> <span class="n">BatchSampler</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">shuffle_eval</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">collate_fn</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Callable</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="n">Any</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">no_test_set</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;LightningDataModule wrapper for generic torch.utils.data.Dataset</span>

<span class="sd">    If val or test Datasets are not provided, this class will split</span>
<span class="sd">    val_pecent and test_percent of the train set respectively to create them</span>

<span class="sd">    Args:</span>
<span class="sd">        train (Dataset): Train set</span>
<span class="sd">        val (Dataset): Validation set. Defaults to None.</span>
<span class="sd">        test (Dataset): Test set. Defaults to None.</span>
<span class="sd">        val_percent (float): Percent of train to be used for validation if no validation set is given. Defaults to 0.2.</span>
<span class="sd">        test_percent (float): Percent of train to be used for test set if no test set is given. Defaults to 0.2.</span>
<span class="sd">        batch_size (int): Training batch size. Defaults to 1.</span>
<span class="sd">        batch_size_eval (Optional[int]): Validation and test batch size. Defaults to None.</span>
<span class="sd">        seed (Optional[int]): Seed for deterministic run. Defaults to None.</span>
<span class="sd">        num_workers (int): Number of workers in the DataLoader. Defaults to 1.</span>
<span class="sd">        pin_memory (bool): Pin tensors to GPU memory. Defaults to True.</span>
<span class="sd">        drop_last (bool): Drop last incomplete batch. Defaults to False.</span>
<span class="sd">        sampler_train (Sampler): Sampler for train loader. Defaults to None.</span>
<span class="sd">        sampler_val (Sampler): Sampler for validation loader. Defaults to None.</span>
<span class="sd">        sampler_test (Sampler): Sampler for test loader. Defaults to None.</span>
<span class="sd">        batch_sampler_train (BatchSampler): Batch sampler for train loader. Defaults to None.</span>
<span class="sd">        batch_sampler_val (BatchSampler): Batch sampler for validation loader. Defaults to None.</span>
<span class="sd">        batch_sampler_test (BatchSampler): Batch sampler for test loader. Defaults to None.</span>
<span class="sd">        shuffle_eval (bool): Shuffle validation and test dataloaders. Defaults to False.</span>
<span class="sd">        collate_fn (Callable[..., Any]): Collator function. Defaults to None.</span>
<span class="sd">        no_test_set (bool): Do not create test set. Useful for tuning</span>

<span class="sd">    Raises:</span>
<span class="sd">        ValueError: If both mutually exclusive sampler_train and batch_sampler_train are provided</span>
<span class="sd">        ValueError: If both mutually exclusive sampler_val and batch_sampler_val are provided</span>
<span class="sd">        ValueError: If both mutually exclusive sampler_test and batch_sampler_test are provided</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PLDataModuleFromDatasets</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">setup_has_run</span> <span class="o">=</span> <span class="kc">False</span>
    <span class="k">if</span> <span class="n">batch_sampler_train</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sampler_train</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You provided both a sampler and a batch sampler for the train set. These are mutually exclusive&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">batch_sampler_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sampler_val</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You provided both a sampler and a batch sampler for the validation set. These are mutually exclusive&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">batch_sampler_test</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">sampler_test</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;You provided both a sampler and a batch sampler for the test set. These are mutually exclusive&quot;</span>
        <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">val_percent</span> <span class="o">=</span> <span class="n">val_percent</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">test_percent</span> <span class="o">=</span> <span class="n">test_percent</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sampler_train</span> <span class="o">=</span> <span class="n">sampler_train</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sampler_val</span> <span class="o">=</span> <span class="n">sampler_val</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">sampler_test</span> <span class="o">=</span> <span class="n">sampler_test</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_train</span> <span class="o">=</span> <span class="n">batch_sampler_train</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_val</span> <span class="o">=</span> <span class="n">batch_sampler_val</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_test</span> <span class="o">=</span> <span class="n">batch_sampler_test</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span> <span class="o">=</span> <span class="n">num_workers</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span> <span class="o">=</span> <span class="n">pin_memory</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="o">=</span> <span class="n">drop_last</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">shuffle_eval</span> <span class="o">=</span> <span class="n">shuffle_eval</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span> <span class="o">=</span> <span class="n">collate_fn</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="o">=</span> <span class="n">batch_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">seed</span> <span class="o">=</span> <span class="n">seed</span>

    <span class="k">if</span> <span class="n">batch_size_eval</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">batch_size_eval</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">no_test_set</span> <span class="o">=</span> <span class="n">no_test_set</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_size_eval</span> <span class="o">=</span> <span class="n">batch_size_eval</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">train</span> <span class="o">=</span> <span class="n">train</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">val</span> <span class="o">=</span> <span class="n">val</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">test</span> <span class="o">=</span> <span class="n">test</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.dm.PLDataModuleFromDatasets.add_argparse_args" class="doc doc-heading">
<code class="highlight language-python"><span class="n">add_argparse_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-classmethod"><code>classmethod</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Augment input parser with arguments for data loading</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>parent_parser</code></td>
        <td><code>ArgumentParser</code></td>
        <td><p>Parser created by the user</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>ArgumentParser</code></td>
      <td><p>argparse.ArgumentParser: Augmented parser</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/dm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@classmethod</span>
<span class="k">def</span> <span class="nf">add_argparse_args</span><span class="p">(</span>
    <span class="bp">cls</span><span class="p">,</span> <span class="n">parent_parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Augment input parser with arguments for data loading</span>

<span class="sd">    Args:</span>
<span class="sd">        parent_parser (argparse.ArgumentParser): Parser created by the user</span>

<span class="sd">    Returns:</span>
<span class="sd">        argparse.ArgumentParser: Augmented parser</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="p">[</span><span class="n">parent_parser</span><span class="p">],</span> <span class="n">add_help</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--val-percent&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.val_percent&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Percent of validation data to be randomly split from the training set, if no validation set is provided&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--test-percent&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.test_percent&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Percent of test data to be randomly split from the training set, if no test set is provided&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--bsz&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.batch_size&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Training batch size&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--bsz-eval&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.batch_size_eval&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Evaluation batch size&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--num-workers&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.num_workers&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of workers to be used in the DataLoader&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--no-pin-memory&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.pin_memory&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_false&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Don&#39;t pin data to GPU memory when transferring&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--drop-last&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.drop_last&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Drop last incomplete batch&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--no-shuffle-eval&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;data.shuffle_eval&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_false&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Don&#39;t shuffle val &amp; test sets&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.dm.PLDataModuleFromDatasets.prepare_data" class="doc doc-heading">
<code class="highlight language-python"><span class="n">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Use this to download and prepare data.</p>
<p>.. warning:: DO NOT set state to the model (use <code>setup</code> instead)
    since this is NOT called on every GPU in DDP/TPU</p>
<p>Example::</p>
<pre><code>def prepare_data(self):
    # good
    download_data()
    tokenize()
    etc()

    # bad
    self.split = data_split
    self.some_state = some_other_state()
</code></pre>
<p>In DDP prepare_data can be called in two ways (using Trainer(prepare_data_per_node)):</p>
<ol>
<li>Once per node. This is the default and is only called on LOCAL_RANK=0.</li>
<li>Once in total. Only called on GLOBAL_RANK=0.</li>
</ol>
<p>Example::</p>
<pre><code># DEFAULT
# called once per node on LOCAL_RANK=0 of that node
Trainer(prepare_data_per_node=True)

# call on GLOBAL_RANK=0 (great for shared file systems)
Trainer(prepare_data_per_node=False)
</code></pre>
<p>This is called before requesting the dataloaders:</p>
<p>.. code-block:: python</p>
<pre><code>model.prepare_data()
    if ddp/tpu: init()
model.setup(stage)
model.train_dataloader()
model.val_dataloader()
model.test_dataloader()
</code></pre>

        <details class="quote">
          <summary>Source code in <code>slp/plbind/dm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">prepare_data</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="k">return</span> <span class="kc">None</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.dm.PLDataModuleFromDatasets.test_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Configure test DataLoader</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>DataLoader</code></td>
      <td><p>Pytorch DataLoader for test set</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/dm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Configure test DataLoader</span>

<span class="sd">    Returns:</span>
<span class="sd">        DataLoader: Pytorch DataLoader for test set</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size_eval</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_test</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_test</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">sampler</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler_test</span><span class="p">,</span>
        <span class="n">batch_sampler</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_test</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shuffle_eval</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_test</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler_test</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.dm.PLDataModuleFromDatasets.train_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Configure train DataLoader</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>DataLoader</code></td>
      <td><p>DataLoader: Pytorch DataLoader for train set</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/dm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">train_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">DataLoader</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Configure train DataLoader</span>

<span class="sd">    Returns:</span>
<span class="sd">        DataLoader: Pytorch DataLoader for train set</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_train</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_train</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">sampler</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler_train</span><span class="p">,</span>
        <span class="n">batch_sampler</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_train</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_train</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler_train</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.dm.PLDataModuleFromDatasets.val_dataloader" class="doc doc-heading">
<code class="highlight language-python"><span class="n">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Configure validation DataLoader</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>DataLoader</code></td>
      <td><p>Pytorch DataLoader for validation set</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/dm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">val_dataloader</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Configure validation DataLoader</span>

<span class="sd">    Returns:</span>
<span class="sd">        DataLoader: Pytorch DataLoader for validation set</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">val</span> <span class="o">=</span> <span class="n">DataLoader</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val</span><span class="p">,</span>
        <span class="n">batch_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_size_eval</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_val</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="mi">1</span><span class="p">,</span>
        <span class="n">num_workers</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_workers</span><span class="p">,</span>
        <span class="n">pin_memory</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pin_memory</span><span class="p">,</span>
        <span class="n">drop_last</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">drop_last</span> <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">),</span>
        <span class="n">sampler</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler_val</span><span class="p">,</span>
        <span class="n">batch_sampler</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_val</span><span class="p">,</span>
        <span class="n">shuffle</span><span class="o">=</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">shuffle_eval</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_sampler_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
            <span class="ow">and</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">sampler_val</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">)</span>
        <span class="p">),</span>
        <span class="n">collate_fn</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">collate_fn</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">val</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="slp.plbind.dm.split_data" class="doc doc-heading">
<code class="highlight language-python"><span class="n">split_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">test_size</span><span class="p">,</span> <span class="n">seed</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Train-test split of dataset.</p>
<p>Dataset can be either a torch.utils.data.Dataset or a list</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>dataset</code></td>
        <td><code>Union[Dataset, List]</code></td>
        <td><p>Input dataset</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>test_size</code></td>
        <td><code>float</code></td>
        <td><p>Size of the test set. Defaults to 0.2.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>seed</code></td>
        <td><code>int</code></td>
        <td><p>Optional seed for deterministic run. Defaults to None.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Union[Dataset, List], Union[Dataset, List]</code></td>
      <td><p>(train set, test set)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/dm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">split_data</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">test_size</span><span class="p">,</span> <span class="n">seed</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Train-test split of dataset.</span>

<span class="sd">    Dataset can be either a torch.utils.data.Dataset or a list</span>

<span class="sd">    Args:</span>
<span class="sd">        dataset (Union[Dataset, List]): Input dataset</span>
<span class="sd">        test_size (float): Size of the test set. Defaults to 0.2.</span>
<span class="sd">        seed (int): Optional seed for deterministic run. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[Union[Dataset, List], Union[Dataset, List]: (train set, test set)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">Dataset</span><span class="p">):</span>
        <span class="n">test_len</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">test_size</span> <span class="o">*</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">))</span>
        <span class="n">train_len</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">dataset</span><span class="p">)</span> <span class="o">-</span> <span class="n">test_len</span>

        <span class="n">seed_generator</span> <span class="o">=</span> <span class="kc">None</span>

        <span class="k">if</span> <span class="n">seed</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">seed_generator</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">Generator</span><span class="p">()</span><span class="o">.</span><span class="n">manual_seed</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">random_split</span><span class="p">(</span>
            <span class="n">dataset</span><span class="p">,</span> <span class="p">[</span><span class="n">train_len</span><span class="p">,</span> <span class="n">test_len</span><span class="p">],</span> <span class="n">generator</span><span class="o">=</span><span class="n">seed_generator</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>

        <span class="n">train</span><span class="p">,</span> <span class="n">test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">dataset</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="n">test_size</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">train</span><span class="p">,</span> <span class="n">test</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.plbind.helpers"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.helpers.FixedWandbLogger" class="doc doc-heading">
        <code>FixedWandbLogger</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.helpers.FixedWandbLogger.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">save_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">offline</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="nb">id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">anonymous</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">project</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">log_model</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">experiment</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prefix</span><span class="o">=</span><span class="s1">&#39;&#39;</span><span class="p">,</span> <span class="n">sync_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">checkpoint_dir</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Wandb logger fix to save checkpoints in wandb</p>
<p>Accepts an additional checkpoint_dir argument, pointing to the real checkpoint directory</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>name</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Display name for the run. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>save_dir</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Path where data is saved. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>offline</code></td>
        <td><code>Optional[bool]</code></td>
        <td><p>Run offline (data can be streamed later to wandb servers). Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>id</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Sets the version, mainly used to resume a previous run. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>anonymous</code></td>
        <td><code>Optional[bool]</code></td>
        <td><p>Enables or explicitly disables anonymous logging. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>version</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Sets the version, mainly used to resume a previous run. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>project</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>The name of the project to which this run will belong. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>log_model</code></td>
        <td><code>Optional[bool]</code></td>
        <td><p>Save checkpoints in wandb dir to upload on W&amp;B servers. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>experiment</code></td>
        <td><code>Run</code></td>
        <td><p>WandB experiment object. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>prefix</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>A string to put at the beginning of metric keys. Defaults to "".</p></td>
        <td><code>&#39;&#39;</code></td>
      </tr>
      <tr>
        <td><code>sync_step</code></td>
        <td><code>Optional[bool]</code></td>
        <td><p>Sync Trainer step with wandb step. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>checkpoint_dir</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Real checkpoint dir. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/helpers.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">save_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">offline</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="nb">id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">anonymous</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">version</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">project</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">log_model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">experiment</span><span class="p">:</span> <span class="n">wandb</span><span class="o">.</span><span class="n">sdk</span><span class="o">.</span><span class="n">wandb_run</span><span class="o">.</span><span class="n">Run</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="p">,</span>
    <span class="n">sync_step</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">bool</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">checkpoint_dir</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wandb logger fix to save checkpoints in wandb</span>

<span class="sd">    Accepts an additional checkpoint_dir argument, pointing to the real checkpoint directory</span>

<span class="sd">    Args:</span>
<span class="sd">        name (Optional[str]): Display name for the run. Defaults to None.</span>
<span class="sd">        save_dir (Optional[str]): Path where data is saved. Defaults to None.</span>
<span class="sd">        offline (Optional[bool]): Run offline (data can be streamed later to wandb servers). Defaults to False.</span>
<span class="sd">        id (Optional[str]): Sets the version, mainly used to resume a previous run. Defaults to None.</span>
<span class="sd">        anonymous (Optional[bool]): Enables or explicitly disables anonymous logging. Defaults to False.</span>
<span class="sd">        version (Optional[str]): Sets the version, mainly used to resume a previous run. Defaults to None.</span>
<span class="sd">        project (Optional[str]): The name of the project to which this run will belong. Defaults to None.</span>
<span class="sd">        log_model (Optional[bool]): Save checkpoints in wandb dir to upload on W&amp;B servers. Defaults to False.</span>
<span class="sd">        experiment ([type]): WandB experiment object. Defaults to None.</span>
<span class="sd">        prefix (Optional[str]): A string to put at the beginning of metric keys. Defaults to &quot;&quot;.</span>
<span class="sd">        sync_step (Optional[bool]): Sync Trainer step with wandb step. Defaults to True.</span>
<span class="sd">        checkpoint_dir (Optional[str]): Real checkpoint dir. Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_dir</span> <span class="o">=</span> <span class="n">checkpoint_dir</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FixedWandbLogger</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">name</span><span class="o">=</span><span class="n">name</span><span class="p">,</span>
        <span class="n">save_dir</span><span class="o">=</span><span class="n">save_dir</span><span class="p">,</span>
        <span class="n">offline</span><span class="o">=</span><span class="n">offline</span><span class="p">,</span>
        <span class="nb">id</span><span class="o">=</span><span class="nb">id</span><span class="p">,</span>
        <span class="n">anonymous</span><span class="o">=</span><span class="n">anonymous</span><span class="p">,</span>
        <span class="n">version</span><span class="o">=</span><span class="n">version</span><span class="p">,</span>
        <span class="n">project</span><span class="o">=</span><span class="n">project</span><span class="p">,</span>
        <span class="n">log_model</span><span class="o">=</span><span class="n">log_model</span><span class="p">,</span>
        <span class="n">experiment</span><span class="o">=</span><span class="n">experiment</span><span class="p">,</span>
        <span class="n">prefix</span><span class="o">=</span><span class="n">prefix</span><span class="p">,</span>
        <span class="n">sync_step</span><span class="o">=</span><span class="n">sync_step</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.helpers.FixedWandbLogger.finalize" class="doc doc-heading">
<code class="highlight language-python"><span class="n">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">status</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Determine where checkpoints are saved and upload to wandb servers</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>status</code></td>
        <td><code>str</code></td>
        <td><p>Experiment status</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/helpers.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@rank_zero_only</span>
<span class="k">def</span> <span class="nf">finalize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">status</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Determine where checkpoints are saved and upload to wandb servers</span>

<span class="sd">    Args:</span>
<span class="sd">        status (str): Experiment status</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># offset future training logged on same W&amp;B run</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_experiment</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_step_offset</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_experiment</span><span class="o">.</span><span class="n">step</span>

    <span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_dir</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_checkpoint_dir</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">save_dir</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">checkpoint_dir</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;Invalid checkpoint dir. Checkpoints will not be uploaded to Wandb.&quot;</span>
        <span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;You can manually upload your checkpoints through the CLI interface.&quot;</span>
        <span class="p">)</span>

    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># upload all checkpoints from saving dir</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">_log_model</span><span class="p">:</span>
            <span class="n">wandb</span><span class="o">.</span><span class="n">save</span><span class="p">(</span><span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">checkpoint_dir</span><span class="p">,</span> <span class="s2">&quot;*.ckpt&quot;</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.helpers.FromLogits" class="doc doc-heading">
        <code>FromLogits</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.helpers.FromLogits.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Wrap pytorch lighting metric to accept logits input</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>metric</code></td>
        <td><code>Metric</code></td>
        <td><p>The metric to wrap, e.g. pl.metrics.Accuracy</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/helpers.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metric</span><span class="p">:</span> <span class="n">pl</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrap pytorch lighting metric to accept logits input</span>

<span class="sd">    Args:</span>
<span class="sd">        metric (pl.metrics.Metric): The metric to wrap, e.g. pl.metrics.Accuracy</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">FromLogits</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">compute_on_step</span><span class="o">=</span><span class="n">metric</span><span class="o">.</span><span class="n">compute_on_step</span><span class="p">,</span>
        <span class="n">dist_sync_on_step</span><span class="o">=</span><span class="n">metric</span><span class="o">.</span><span class="n">dist_sync_on_step</span><span class="p">,</span>
        <span class="n">process_group</span><span class="o">=</span><span class="n">metric</span><span class="o">.</span><span class="n">process_group</span><span class="p">,</span>
        <span class="n">dist_sync_fn</span><span class="o">=</span><span class="n">metric</span><span class="o">.</span><span class="n">dist_sync_fn</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">metric</span> <span class="o">=</span> <span class="n">metric</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.helpers.FromLogits.compute" class="doc doc-heading">
<code class="highlight language-python"><span class="n">compute</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Compute metric</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: metric value</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/helpers.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">compute</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Compute metric</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: metric value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">compute</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.helpers.FromLogits.update" class="doc doc-heading">
<code class="highlight language-python"><span class="n">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Update underlying metric</p>
<p>Calculate softmax under the hood and pass probs to the underlying metric</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>preds</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, *, num_classes] Logits</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>target</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, *] Ground truths</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/helpers.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">update</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">preds</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">target</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
    <span class="sd">&quot;&quot;&quot;Update underlying metric</span>

<span class="sd">    Calculate softmax under the hood and pass probs to the underlying metric</span>

<span class="sd">    Args:</span>
<span class="sd">        preds (torch.Tensor): [B, *, num_classes] Logits</span>
<span class="sd">        target (torch.Tensor): [B, *] Ground truths</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">preds</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">metric</span><span class="o">.</span><span class="n">update</span><span class="p">(</span><span class="n">preds</span><span class="p">,</span> <span class="n">target</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.plbind.module"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.module.AutoEncoderPLModule" class="doc doc-heading">
        <code>AutoEncoderPLModule</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.AutoEncoderPLModule.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hparams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Pass arguments through to base class</p>

        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]],</span>
    <span class="n">criterion</span><span class="p">:</span> <span class="n">LossType</span><span class="p">,</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hparams</span><span class="p">:</span> <span class="n">Configuration</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pass arguments through to base class&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AutoEncoderPLModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">criterion</span><span class="p">,</span>
        <span class="n">predictor_cls</span><span class="o">=</span><span class="n">_AutoEncoder</span><span class="p">,</span>
        <span class="n">lr_scheduler</span><span class="o">=</span><span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="n">hparams</span><span class="o">=</span><span class="n">hparams</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
        <span class="n">calculate_perplexity</span><span class="o">=</span><span class="n">calculate_perplexity</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.module.BertPLModule" class="doc doc-heading">
        <code>BertPLModule</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.BertPLModule.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hparams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Pass arguments through to base class</p>

        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]],</span>
    <span class="n">criterion</span><span class="p">:</span> <span class="n">LossType</span><span class="p">,</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hparams</span><span class="p">:</span> <span class="n">Configuration</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pass arguments through to base class&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BertPLModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">criterion</span><span class="p">,</span>
        <span class="n">predictor_cls</span><span class="o">=</span><span class="n">_BertSequenceClassification</span><span class="p">,</span>
        <span class="n">lr_scheduler</span><span class="o">=</span><span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="n">hparams</span><span class="o">=</span><span class="n">hparams</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
        <span class="n">calculate_perplexity</span><span class="o">=</span><span class="n">calculate_perplexity</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.module.MultimodalTransformerClassificationPLModule" class="doc doc-heading">
        <code>MultimodalTransformerClassificationPLModule</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.MultimodalTransformerClassificationPLModule.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hparams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Pass arguments through to base class</p>

        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]],</span>
    <span class="n">criterion</span><span class="p">:</span> <span class="n">LossType</span><span class="p">,</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hparams</span><span class="p">:</span> <span class="n">Configuration</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pass arguments through to base class&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultimodalTransformerClassificationPLModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">criterion</span><span class="p">,</span>
        <span class="n">predictor_cls</span><span class="o">=</span><span class="n">_MultimodalTransformerClassification</span><span class="p">,</span>
        <span class="n">lr_scheduler</span><span class="o">=</span><span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="n">hparams</span><span class="o">=</span><span class="n">hparams</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
        <span class="n">calculate_perplexity</span><span class="o">=</span><span class="n">calculate_perplexity</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.module.PLModule" class="doc doc-heading">
        <code>PLModule</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.PLModule.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hparams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Pass arguments through to base class</p>

        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]],</span>
    <span class="n">criterion</span><span class="p">:</span> <span class="n">LossType</span><span class="p">,</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hparams</span><span class="p">:</span> <span class="n">Configuration</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pass arguments through to base class&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PLModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">criterion</span><span class="p">,</span>
        <span class="n">predictor_cls</span><span class="o">=</span><span class="n">_Classification</span><span class="p">,</span>
        <span class="n">lr_scheduler</span><span class="o">=</span><span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="n">hparams</span><span class="o">=</span><span class="n">hparams</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
        <span class="n">calculate_perplexity</span><span class="o">=</span><span class="n">calculate_perplexity</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.module.RnnPLModule" class="doc doc-heading">
        <code>RnnPLModule</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.RnnPLModule.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hparams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Pass arguments through to base class</p>

        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]],</span>
    <span class="n">criterion</span><span class="p">:</span> <span class="n">LossType</span><span class="p">,</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hparams</span><span class="p">:</span> <span class="n">Configuration</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pass arguments through to base class&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">RnnPLModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">criterion</span><span class="p">,</span>
        <span class="n">predictor_cls</span><span class="o">=</span><span class="n">_RnnClassification</span><span class="p">,</span>
        <span class="n">lr_scheduler</span><span class="o">=</span><span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="n">hparams</span><span class="o">=</span><span class="n">hparams</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
        <span class="n">calculate_perplexity</span><span class="o">=</span><span class="n">calculate_perplexity</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.module.SimplePLModule" class="doc doc-heading">
        <code>SimplePLModule</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hparams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">predictor_cls</span><span class="o">=&lt;</span><span class="k">class</span> <span class="err">&#39;</span><span class="nc">slp</span><span class="o">.</span><span class="n">plbind</span><span class="o">.</span><span class="n">module</span><span class="o">.</span><span class="n">_Classification</span><span class="s1">&#39;&gt;, calculate_perplexity=False)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Wraps a (model, optimizer, criterion, lr_scheduler) tuple in a LightningModule</p>
<p>Handles the boilerplate for metrics calculation and logging and defines the train_step / val_step / test_step
with use of the predictor helper classes (e.g. _Classification, _RnnClassification)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>model</code></td>
        <td><code>Module</code></td>
        <td><p>Module to use for prediction</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>optimizer</code></td>
        <td><code>Union[torch.optim.optimizer.Optimizer, List[torch.optim.optimizer.Optimizer]]</code></td>
        <td><p>Optimizers to use for training</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>criterion</code></td>
        <td><code>Union[torch.nn.modules.module.Module, Callable]</code></td>
        <td><p>Task loss</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lr_scheduler</code></td>
        <td><code>Union[torch.optim.lr_scheduler._LRScheduler, List[torch.optim.lr_scheduler._LRScheduler]]</code></td>
        <td><p>Learning rate scheduler. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>hparams</code></td>
        <td><code>Union[omegaconf.dictconfig.DictConfig, Dict[str, Any], argparse.Namespace]</code></td>
        <td><p>Hyperparameter values. This ensures they are logged with trainer.loggers. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>metrics</code></td>
        <td><code>Optional[Dict[str, pytorch_lightning.metrics.metric.Metric]]</code></td>
        <td><p>Metrics to track. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>predictor_cls</code></td>
        <td><code>[type]</code></td>
        <td><p>Class that defines a parse_batch and a
    get_predictions_and_targets method. Defaults to _Classification.</p></td>
        <td><code>&lt;class &#39;slp.plbind.module._Classification&#39;&gt;</code></td>
      </tr>
      <tr>
        <td><code>calculate_perplexity</code></td>
        <td><code>bool</code></td>
        <td><p>Whether to calculate perplexity.
    Would be cleaner as a metric, but this is more efficient. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]],</span>
    <span class="n">criterion</span><span class="p">:</span> <span class="n">LossType</span><span class="p">,</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hparams</span><span class="p">:</span> <span class="n">Configuration</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">predictor_cls</span><span class="o">=</span><span class="n">_Classification</span><span class="p">,</span>
    <span class="n">calculate_perplexity</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># for LM. Dirty but much more efficient</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wraps a (model, optimizer, criterion, lr_scheduler) tuple in a LightningModule</span>

<span class="sd">    Handles the boilerplate for metrics calculation and logging and defines the train_step / val_step / test_step</span>
<span class="sd">    with use of the predictor helper classes (e.g. _Classification, _RnnClassification)</span>

<span class="sd">    Args:</span>
<span class="sd">        model (nn.Module): Module to use for prediction</span>
<span class="sd">        optimizer (Union[Optimizer, List[Optimizer]]): Optimizers to use for training</span>
<span class="sd">        criterion (LossType): Task loss</span>
<span class="sd">        lr_scheduler (Union[_LRScheduler, List[_LRScheduler]], optional): Learning rate scheduler. Defaults to None.</span>
<span class="sd">        hparams (Configuration, optional): Hyperparameter values. This ensures they are logged with trainer.loggers. Defaults to None.</span>
<span class="sd">        metrics (Optional[Dict[str, pl.metrics.Metric]], optional): Metrics to track. Defaults to None.</span>
<span class="sd">        predictor_cls ([type], optional): Class that defines a parse_batch and a</span>
<span class="sd">                get_predictions_and_targets method. Defaults to _Classification.</span>
<span class="sd">        calculate_perplexity (bool, optional): Whether to calculate perplexity.</span>
<span class="sd">                Would be cleaner as a metric, but this is more efficient. Defaults to False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SimplePLModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">calculate_perplexity</span> <span class="o">=</span> <span class="n">calculate_perplexity</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">model</span> <span class="o">=</span> <span class="n">model</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span> <span class="o">=</span> <span class="n">optimizer</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="o">=</span> <span class="n">lr_scheduler</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span> <span class="o">=</span> <span class="n">criterion</span>

    <span class="k">if</span> <span class="n">metrics</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span><span class="n">metrics</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val_metrics</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">({</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_metrics</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span>
            <span class="p">{</span><span class="n">k</span><span class="p">:</span> <span class="n">v</span><span class="o">.</span><span class="n">clone</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span><span class="n">modules</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val_metrics</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span><span class="n">modules</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_metrics</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleDict</span><span class="p">(</span><span class="n">modules</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span> <span class="o">=</span> <span class="n">predictor_cls</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">hparams</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hparams</span><span class="p">,</span> <span class="n">Namespace</span><span class="p">):</span>
            <span class="n">dict_params</span> <span class="o">=</span> <span class="nb">vars</span><span class="p">(</span><span class="n">hparams</span><span class="p">)</span>
        <span class="k">elif</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">hparams</span><span class="p">,</span> <span class="n">DictConfig</span><span class="p">):</span>
            <span class="n">dict_params</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">hparams</span><span class="p">))</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dict_params</span> <span class="o">=</span> <span class="n">hparams</span>
        <span class="c1"># self.hparams = dict_params</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">save_hyperparameters</span><span class="p">(</span><span class="n">dict_params</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.aggregate_epoch_metrics" class="doc doc-heading">
<code class="highlight language-python"><span class="n">aggregate_epoch_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;Training&#39;</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Aggregate metrics over a whole epoch</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>List[Dict[str, torch.Tensor]]</code></td>
        <td><p>Aggregated outputs from train_step, validation_step or test_step</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>mode</code></td>
        <td><code>str</code></td>
        <td><p>"Training", "Validation" or "Testing". Defaults to "Training".</p></td>
        <td><code>&#39;Training&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">aggregate_epoch_metrics</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;Training&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Aggregate metrics over a whole epoch</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs (List[Dict[str, torch.Tensor]]): Aggregated outputs from train_step, validation_step or test_step</span>
<span class="sd">        mode (str, optional): &quot;Training&quot;, &quot;Validation&quot; or &quot;Testing&quot;. Defaults to &quot;Training&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">fmt</span><span class="p">(</span><span class="n">name</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Format metric name&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">name</span><span class="si">}</span><span class="s2">&quot;</span> <span class="k">if</span> <span class="n">name</span> <span class="o">!=</span> <span class="s2">&quot;loss&quot;</span> <span class="k">else</span> <span class="s2">&quot;train_loss&quot;</span>

    <span class="n">keys</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">keys</span><span class="p">())</span>
    <span class="n">aggregated</span> <span class="o">=</span> <span class="p">{</span><span class="n">fmt</span><span class="p">(</span><span class="n">k</span><span class="p">):</span> <span class="n">torch</span><span class="o">.</span><span class="n">stack</span><span class="p">([</span><span class="n">x</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">outputs</span><span class="p">])</span><span class="o">.</span><span class="n">mean</span><span class="p">()</span> <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="n">keys</span><span class="p">}</span>
    <span class="n">aggregated</span><span class="p">[</span><span class="s2">&quot;epoch&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">+</span> <span class="mi">1</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span><span class="n">aggregated</span><span class="p">,</span> <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prog_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">on_epoch</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">aggregated</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.configure_optimizers" class="doc doc-heading">
<code class="highlight language-python"><span class="n">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Return optimizers and learning rate schedulers</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[List[Optimizer], List[_LRScheduler]]</code></td>
      <td><p>(optimizers, lr_schedulers)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">configure_optimizers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Return optimizers and learning rate schedulers</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[List[Optimizer], List[_LRScheduler]]: (optimizers, lr_schedulers)</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scheduler</span> <span class="o">=</span> <span class="p">{</span>
            <span class="s2">&quot;scheduler&quot;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">lr_scheduler</span><span class="p">,</span>
            <span class="s2">&quot;interval&quot;</span><span class="p">:</span> <span class="s2">&quot;epoch&quot;</span><span class="p">,</span>
            <span class="s2">&quot;monitor&quot;</span><span class="p">:</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
        <span class="p">}</span>

        <span class="k">return</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span><span class="p">],</span> <span class="p">[</span><span class="n">scheduler</span><span class="p">]</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizer</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Call wrapped module forward</p>

        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Call wrapped module forward&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.log_to_console" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_to_console</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;Training&#39;</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Log metrics to console</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>metrics</code></td>
        <td><code>Dict[str, torch.Tensor]</code></td>
        <td><p>Computed metrics</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>mode</code></td>
        <td><code>str</code></td>
        <td><p>"Training", "Validation" or "Testing". Defaults to "Training".</p></td>
        <td><code>&#39;Training&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_to_console</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">metrics</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;Training&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Log metrics to console</span>

<span class="sd">    Args:</span>
<span class="sd">        metrics (Dict[str, torch.Tensor]): Computed metrics</span>
<span class="sd">        mode (str, optional): &quot;Training&quot;, &quot;Validation&quot; or &quot;Testing&quot;. Defaults to &quot;Training&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Epoch </span><span class="si">{}</span><span class="s2"> </span><span class="si">{}</span><span class="s2"> results&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">current_epoch</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">mode</span><span class="p">))</span>
    <span class="n">print_separator</span><span class="p">(</span><span class="n">symbol</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">print_fn</span><span class="o">=</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">value</span> <span class="ow">in</span> <span class="n">metrics</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
        <span class="k">if</span> <span class="n">name</span> <span class="o">==</span> <span class="s2">&quot;epoch&quot;</span><span class="p">:</span>
            <span class="k">continue</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;</span><span class="si">{:&lt;15}</span><span class="s2"> </span><span class="si">{:&lt;15}</span><span class="s2">&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">name</span><span class="p">,</span> <span class="n">value</span><span class="p">))</span>

    <span class="n">print_separator</span><span class="p">(</span><span class="n">symbol</span><span class="o">=</span><span class="s2">&quot;%&quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">print_fn</span><span class="o">=</span><span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.test_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Aggregate metrics of a test epoch</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>List[Dict[str, torch.Tensor]]</code></td>
        <td><p>Aggregated outputs from test_step</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Aggregate metrics of a test epoch</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs (List[Dict[str, torch.Tensor]]): Aggregated outputs from test_step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregate_epoch_metrics</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;Test&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_to_console</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;Test&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.test_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Compute loss for a single test step and log metrics to loggers</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Tuple[torch.Tensor, ...]</code></td>
        <td><p>Input batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>Index of batch</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, torch.Tensor]</code></td>
      <td><p>computed metrics</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">test_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss for a single test step and log metrics to loggers</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (Tuple[torch.Tensor, ...]): Input batch</span>
<span class="sd">        batch_idx (int): Index of batch</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, torch.Tensor]: computed metrics</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_hat</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span><span class="o">.</span><span class="n">get_predictions_and_targets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_metrics</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">test_metrics</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;test&quot;</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.training_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Aggregate metrics of a training epoch</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>List[Dict[str, torch.Tensor]]</code></td>
        <td><p>Aggregated outputs from train_step</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">training_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Aggregate metrics of a training epoch</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs (List[Dict[str, torch.Tensor]]): Aggregated outputs from train_step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregate_epoch_metrics</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;Training&quot;</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_to_console</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;Training&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.training_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Compute loss for a single training step and log metrics to loggers</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Tuple[torch.Tensor, ...]</code></td>
        <td><p>Input batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>Index of batch</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, torch.Tensor]</code></td>
      <td><p>computed metrics</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">training_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss for a single training step and log metrics to loggers</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (Tuple[torch.Tensor, ...]): Input batch</span>
<span class="sd">        batch_idx (int): Index of batch</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, torch.Tensor]: computed metrics</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_hat</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span><span class="o">.</span><span class="n">get_predictions_and_targets</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">model</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_metrics</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">train_metrics</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;train&quot;</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">log_dict</span><span class="p">(</span>
        <span class="n">metrics</span><span class="p">,</span>
        <span class="n">on_step</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">on_epoch</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">logger</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">prog_bar</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">metrics</span><span class="p">[</span><span class="s2">&quot;loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">loss</span>

    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.validation_epoch_end" class="doc doc-heading">
<code class="highlight language-python"><span class="n">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Aggregate metrics of a validation epoch</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>outputs</code></td>
        <td><code>List[Dict[str, torch.Tensor]]</code></td>
        <td><p>Aggregated outputs from validation_step</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_epoch_end</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">outputs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Aggregate metrics of a validation epoch</span>

<span class="sd">    Args:</span>
<span class="sd">        outputs (List[Dict[str, torch.Tensor]]): Aggregated outputs from validation_step</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">outputs</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">aggregate_epoch_metrics</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;Validation&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">torch</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">])</span> <span class="ow">or</span> <span class="n">torch</span><span class="o">.</span><span class="n">isinf</span><span class="p">(</span><span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]):</span>
        <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;val_loss&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1000000</span>

    <span class="n">outputs</span><span class="p">[</span><span class="s2">&quot;best_score&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span>
        <span class="n">outputs</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">early_stopping_callback</span><span class="o">.</span><span class="n">monitor</span><span class="p">]</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">early_stopping_callback</span><span class="o">.</span><span class="n">best_score</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">(),</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">log_to_console</span><span class="p">(</span><span class="n">outputs</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;Validation&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.SimplePLModule.validation_step" class="doc doc-heading">
<code class="highlight language-python"><span class="n">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Compute loss for a single validation step and log metrics to loggers</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>Tuple[torch.Tensor, ...]</code></td>
        <td><p>Input batch</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_idx</code></td>
        <td><code>int</code></td>
        <td><p>Index of batch</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, torch.Tensor]</code></td>
      <td><p>computed metrics</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">validation_step</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">,</span> <span class="n">batch_idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Compute loss for a single validation step and log metrics to loggers</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (Tuple[torch.Tensor, ...]): Input batch</span>
<span class="sd">        batch_idx (int): Index of batch</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, torch.Tensor]: computed metrics</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">y_hat</span><span class="p">,</span> <span class="n">targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predictor</span><span class="o">.</span><span class="n">get_predictions_and_targets</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span>
    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">criterion</span><span class="p">(</span><span class="n">y_hat</span><span class="p">,</span> <span class="n">targets</span><span class="p">)</span>
    <span class="n">metrics</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_compute_metrics</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">val_metrics</span><span class="p">,</span> <span class="n">loss</span><span class="p">,</span> <span class="n">y_hat</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s2">&quot;val&quot;</span>
    <span class="p">)</span>

    <span class="n">metrics</span><span class="p">[</span>
        <span class="s2">&quot;best_score&quot;</span>
    <span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">trainer</span><span class="o">.</span><span class="n">early_stopping_callback</span><span class="o">.</span><span class="n">best_score</span><span class="o">.</span><span class="n">detach</span><span class="p">()</span><span class="o">.</span><span class="n">cpu</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">metrics</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.module.TransformerClassificationPLModule" class="doc doc-heading">
        <code>TransformerClassificationPLModule</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.TransformerClassificationPLModule.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hparams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Pass arguments through to base class</p>

        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]],</span>
    <span class="n">criterion</span><span class="p">:</span> <span class="n">LossType</span><span class="p">,</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hparams</span><span class="p">:</span> <span class="n">Configuration</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pass arguments through to base class&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TransformerClassificationPLModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">criterion</span><span class="p">,</span>
        <span class="n">predictor_cls</span><span class="o">=</span><span class="n">_TransformerClassification</span><span class="p">,</span>
        <span class="n">lr_scheduler</span><span class="o">=</span><span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="n">hparams</span><span class="o">=</span><span class="n">hparams</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
        <span class="n">calculate_perplexity</span><span class="o">=</span><span class="n">calculate_perplexity</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.plbind.module.TransformerPLModule" class="doc doc-heading">
        <code>TransformerPLModule</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.plbind.module.TransformerPLModule.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span> <span class="n">lr_scheduler</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">hparams</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">metrics</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Pass arguments through to base class</p>

        <details class="quote">
          <summary>Source code in <code>slp/plbind/module.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">optimizer</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">Optimizer</span><span class="p">]],</span>
    <span class="n">criterion</span><span class="p">:</span> <span class="n">LossType</span><span class="p">,</span>
    <span class="n">lr_scheduler</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">,</span> <span class="n">List</span><span class="p">[</span><span class="n">_LRScheduler</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">hparams</span><span class="p">:</span> <span class="n">Configuration</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">metrics</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">pl</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">Metric</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">calculate_perplexity</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Pass arguments through to base class&quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TransformerPLModule</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">model</span><span class="p">,</span>
        <span class="n">optimizer</span><span class="p">,</span>
        <span class="n">criterion</span><span class="p">,</span>
        <span class="n">predictor_cls</span><span class="o">=</span><span class="n">_Transformer</span><span class="p">,</span>
        <span class="n">lr_scheduler</span><span class="o">=</span><span class="n">lr_scheduler</span><span class="p">,</span>
        <span class="n">hparams</span><span class="o">=</span><span class="n">hparams</span><span class="p">,</span>
        <span class="n">metrics</span><span class="o">=</span><span class="n">metrics</span><span class="p">,</span>
        <span class="n">calculate_perplexity</span><span class="o">=</span><span class="n">calculate_perplexity</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.plbind.trainer"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-function">



<h2 id="slp.plbind.trainer.add_optimizer_args" class="doc doc-heading">
<code class="highlight language-python"><span class="n">add_optimizer_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Augment parser with optimizer arguments</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>parent_parser</code></td>
        <td><code>ArgumentParser</code></td>
        <td><p>Parser created by the user</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>ArgumentParser</code></td>
      <td><p>argparse.ArgumentParser: Augmented parser</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/trainer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">add_optimizer_args</span><span class="p">(</span>
    <span class="n">parent_parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Augment parser with optimizer arguments</span>

<span class="sd">    Args:</span>
<span class="sd">        parent_parser (argparse.ArgumentParser): Parser created by the user</span>

<span class="sd">    Returns:</span>
<span class="sd">        argparse.ArgumentParser: Augmented parser</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="p">[</span><span class="n">parent_parser</span><span class="p">],</span> <span class="n">add_help</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--optimizer&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;optimizer&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">choices</span><span class="o">=</span><span class="p">[</span>
            <span class="s2">&quot;Adam&quot;</span><span class="p">,</span>
            <span class="s2">&quot;AdamW&quot;</span><span class="p">,</span>
            <span class="s2">&quot;SGD&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Adadelta&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Adagrad&quot;</span><span class="p">,</span>
            <span class="s2">&quot;Adamax&quot;</span><span class="p">,</span>
            <span class="s2">&quot;ASGD&quot;</span><span class="p">,</span>
            <span class="s2">&quot;RMSprop&quot;</span><span class="p">,</span>
        <span class="p">],</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;Adam&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Which optimizer to use&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;optim.lr&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">1e-3</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Learning rate&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--weight-decay&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;optim.weight_decay&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Learning rate&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr-scheduler&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;lr_scheduler&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="c1"># type=str,</span>
        <span class="c1"># choices=[&quot;ReduceLROnPlateau&quot;],</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use learning rate scheduling. Currently only ReduceLROnPlateau is supported out of the box&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr-factor&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;lr_schedule.factor&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Multiplicative factor by which LR is reduced. Used if --lr-scheduler is provided.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr-patience&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;lr_schedule.patience&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of epochs with no improvement after which learning rate will be reduced. Used if --lr-scheduler is provided.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--lr-cooldown&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;lr_schedule.cooldown&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of epochs to wait before resuming normal operation after lr has been reduced. Used if --lr-scheduler is provided.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--min-lr&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;lr_schedule.min_lr&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Minimum lr for LR scheduling. Used if --lr-scheduler is provided.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.plbind.trainer.add_trainer_args" class="doc doc-heading">
<code class="highlight language-python"><span class="n">add_trainer_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Augment parser with trainer arguments</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>parent_parser</code></td>
        <td><code>ArgumentParser</code></td>
        <td><p>Parser created by the user</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>ArgumentParser</code></td>
      <td><p>argparse.ArgumentParser: Augmented parser</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/trainer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">add_trainer_args</span><span class="p">(</span><span class="n">parent_parser</span><span class="p">:</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Augment parser with trainer arguments</span>

<span class="sd">    Args:</span>
<span class="sd">        parent_parser (argparse.ArgumentParser): Parser created by the user</span>

<span class="sd">    Returns:</span>
<span class="sd">        argparse.ArgumentParser: Augmented parser</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="n">parents</span><span class="o">=</span><span class="p">[</span><span class="n">parent_parser</span><span class="p">],</span> <span class="n">add_help</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--seed&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;seed&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Seed for reproducibility&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--config&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;config&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>  <span class="c1"># dir_path,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Path to YAML configuration file&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--experiment-name&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.experiment_name&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;experiment&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Name of the running experiment&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--run-id&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.run_id&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Unique identifier for the current run. If not provided it is inferred from datetime.now()&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--experiment-group&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.experiment_group&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Group of current experiment. Useful when evaluating for different seeds / cross-validation etc.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--experiments-folder&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.experiments_folder&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;experiments&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Top-level folder where experiment results &amp; checkpoints are saved&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--save-top-k&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.save_top_k&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Save checkpoints for top k models&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--patience&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.patience&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of epochs to wait before early stopping&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--wandb-project&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.wandb_project&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Wandb project under which results are saved&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--tags&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.tags&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">nargs</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Tags for current run to make results searchable.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--stochastic_weight_avg&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.stochastic_weight_avg&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Use Stochastic weight averaging.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--gpus&quot;</span><span class="p">,</span> <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.gpus&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span> <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of GPUs to use&quot;</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--val-interval&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.check_val_every_n_epoch&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Run validation every n epochs&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--clip-grad-norm&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.gradient_clip_val&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">float</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Clip gradients with ||grad(w)|| &gt;= args.clip_grad_norm&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--epochs&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.max_epochs&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Maximum number of training epochs&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--num-nodes&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.num_nodes&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Number of nodes to run&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--steps&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.max_steps&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Maximum number of training steps&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--tbtt_steps&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.truncated_bptt_steps&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">int</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Truncated Back-propagation-through-time steps.&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--debug&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;debug&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;If true, we run a full run on a small subset of the input data and overfit 10 training batches&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--offline&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.force_wandb_offline&quot;</span><span class="p">,</span>
        <span class="n">action</span><span class="o">=</span><span class="s2">&quot;store_true&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;If true, forces offline execution of wandb logger&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--early-stop-on&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.early_stop_on&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Metric for early stopping&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span>
        <span class="s2">&quot;--early-stop-mode&quot;</span><span class="p">,</span>
        <span class="n">dest</span><span class="o">=</span><span class="s2">&quot;trainer.early_stop_mode&quot;</span><span class="p">,</span>
        <span class="nb">type</span><span class="o">=</span><span class="nb">str</span><span class="p">,</span>
        <span class="n">choices</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;min&quot;</span><span class="p">,</span> <span class="s2">&quot;max&quot;</span><span class="p">],</span>
        <span class="n">default</span><span class="o">=</span><span class="s2">&quot;min&quot;</span><span class="p">,</span>
        <span class="n">help</span><span class="o">=</span><span class="s2">&quot;Minimize or maximize early stopping metric&quot;</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">parser</span>
</code></pre></div>
        </details>
    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="slp.plbind.trainer.make_trainer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">make_trainer</span><span class="p">(</span><span class="n">experiment_name</span><span class="o">=</span><span class="s1">&#39;experiment&#39;</span><span class="p">,</span> <span class="n">experiment_description</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">run_id</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">experiment_group</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">experiments_folder</span><span class="o">=</span><span class="s1">&#39;experiments&#39;</span><span class="p">,</span> <span class="n">save_top_k</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">wandb_project</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">wandb_user</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">force_wandb_offline</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">tags</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">stochastic_weight_avg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">auto_scale_batch_size</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">check_val_every_n_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">num_nodes</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">truncated_bptt_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fast_dev_run</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">overfit_batches</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">terminate_on_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">profiler</span><span class="o">=</span><span class="s1">&#39;simple&#39;</span><span class="p">,</span> <span class="n">early_stop_on</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">early_stop_mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Configure trainer with preferred defaults</p>
<ul>
<li>Experiment folder and run_id configured (based on datetime.now())</li>
<li>Wandb and CSV loggers run by default</li>
<li>Wandb configured to save code and checkpoints</li>
<li>Wandb configured in online mode except if no internet connection is available</li>
<li>Early stopping on best validation loss is configured by default</li>
<li>Checkpointing on best validation loss is configured by default
*</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>experiment_name</code></td>
        <td><code>str</code></td>
        <td><p>Experiment name. Defaults to "experiment".</p></td>
        <td><code>&#39;experiment&#39;</code></td>
      </tr>
      <tr>
        <td><code>experiment_description</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Detailed description of the experiment. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>run_id</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Unique run_id. Defaults to datetime.now(). Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>experiment_group</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Group experiments over multiple runs. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>experiments_folder</code></td>
        <td><code>str</code></td>
        <td><p>Folder to save outputs. Defaults to "experiments".</p></td>
        <td><code>&#39;experiments&#39;</code></td>
      </tr>
      <tr>
        <td><code>save_top_k</code></td>
        <td><code>int</code></td>
        <td><p>Save top k checkpoints. Defaults to 3.</p></td>
        <td><code>3</code></td>
      </tr>
      <tr>
        <td><code>patience</code></td>
        <td><code>int</code></td>
        <td><p>Patience for early stopping. Defaults to 3.</p></td>
        <td><code>3</code></td>
      </tr>
      <tr>
        <td><code>wandb_project</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Wandb project to save the experiment. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>wandb_user</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Wandb username. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>force_wandb_offline</code></td>
        <td><code>bool</code></td>
        <td><p>Force offline execution of wandb</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>tags</code></td>
        <td><code>Optional[Sequence]</code></td>
        <td><p>Additional tags to attach to the experiment. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>stochastic_weight_avg</code></td>
        <td><code>bool</code></td>
        <td><p>Use stochastic weight averaging. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>auto_scale_batch_size</code></td>
        <td><code>bool</code></td>
        <td><p>Find optimal batch size for the available resources when running
    trainer.tune(). Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>gpus</code></td>
        <td><code>int</code></td>
        <td><p>number of GPUs to use. Defaults to 0.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>check_val_every_n_epoch</code></td>
        <td><code>int</code></td>
        <td><p>Run validation every n epochs. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>gradient_clip_val</code></td>
        <td><code>float</code></td>
        <td><p>Clip gradient norm value. Defaults to 0 (no clipping).</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>precision</code></td>
        <td><code>int</code></td>
        <td><p>Floating point precision. Defaults to 32.</p></td>
        <td><code>32</code></td>
      </tr>
      <tr>
        <td><code>num_nodes</code></td>
        <td><code>int</code></td>
        <td><p>Number of nodes to run on</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>max_epochs</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Maximum number of epochs for training. Defaults to 100.</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>max_steps</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Maximum number of steps for training. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>truncated_bptt_steps</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Truncated back prop breaks performs backprop every k steps of much longer
    sequence. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>fast_dev_run</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Run training on a small number of  batches for debugging. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>overfit_batches</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Try to overfit a small number of batches for debugging. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>terminate_on_nan</code></td>
        <td><code>bool</code></td>
        <td><p>Terminate on NaN gradients. Warning this makes training slow. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>profiler</code></td>
        <td><code>Union[pytorch_lightning.profiler.profilers.BaseProfiler, bool, str]</code></td>
        <td><p>Use profiler to track execution times of each function</p></td>
        <td><code>&#39;simple&#39;</code></td>
      </tr>
      <tr>
        <td><code>early_stop_on</code></td>
        <td><code>str</code></td>
        <td><p>metric for early stopping</p></td>
        <td><code>&#39;val_loss&#39;</code></td>
      </tr>
      <tr>
        <td><code>early_stop_mode</code></td>
        <td><code>str</code></td>
        <td><p>"min" or "max"</p></td>
        <td><code>&#39;min&#39;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Trainer</code></td>
      <td><p>pl.Trainer: Configured trainer</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/trainer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">make_trainer</span><span class="p">(</span>
    <span class="n">experiment_name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;experiment&quot;</span><span class="p">,</span>
    <span class="n">experiment_description</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">run_id</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">experiment_group</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">experiments_folder</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;experiments&quot;</span><span class="p">,</span>
    <span class="n">save_top_k</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">patience</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">wandb_project</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">wandb_user</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">force_wandb_offline</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">tags</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Sequence</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">stochastic_weight_avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">auto_scale_batch_size</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">gpus</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">check_val_every_n_epoch</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">gradient_clip_val</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">precision</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">num_nodes</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">max_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">truncated_bptt_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fast_dev_run</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">overfit_batches</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">terminate_on_nan</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Be careful this makes training very slow for large models</span>
    <span class="n">profiler</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">pl</span><span class="o">.</span><span class="n">profiler</span><span class="o">.</span><span class="n">BaseProfiler</span><span class="p">,</span> <span class="nb">bool</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="s2">&quot;simple&quot;</span><span class="p">,</span>
    <span class="n">early_stop_on</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
    <span class="n">early_stop_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;min&quot;</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Configure trainer with preferred defaults</span>

<span class="sd">    * Experiment folder and run_id configured (based on datetime.now())</span>
<span class="sd">    * Wandb and CSV loggers run by default</span>
<span class="sd">    * Wandb configured to save code and checkpoints</span>
<span class="sd">    * Wandb configured in online mode except if no internet connection is available</span>
<span class="sd">    * Early stopping on best validation loss is configured by default</span>
<span class="sd">    * Checkpointing on best validation loss is configured by default</span>
<span class="sd">    *</span>

<span class="sd">    Args:</span>
<span class="sd">        experiment_name (str, optional): Experiment name. Defaults to &quot;experiment&quot;.</span>
<span class="sd">        experiment_description (Optional[str], optional): Detailed description of the experiment. Defaults to None.</span>
<span class="sd">        run_id (Optional[str], optional): Unique run_id. Defaults to datetime.now(). Defaults to None.</span>
<span class="sd">        experiment_group (Optional[str], optional): Group experiments over multiple runs. Defaults to None.</span>
<span class="sd">        experiments_folder (str, optional): Folder to save outputs. Defaults to &quot;experiments&quot;.</span>
<span class="sd">        save_top_k (int, optional): Save top k checkpoints. Defaults to 3.</span>
<span class="sd">        patience (int, optional): Patience for early stopping. Defaults to 3.</span>
<span class="sd">        wandb_project (Optional[str], optional): Wandb project to save the experiment. Defaults to None.</span>
<span class="sd">        wandb_user (Optional[str], optional): Wandb username. Defaults to None.</span>
<span class="sd">        force_wandb_offline (bool): Force offline execution of wandb</span>
<span class="sd">        tags (Optional[Sequence], optional): Additional tags to attach to the experiment. Defaults to None.</span>
<span class="sd">        stochastic_weight_avg (bool, optional): Use stochastic weight averaging. Defaults to False.</span>
<span class="sd">        auto_scale_batch_size (bool, optional): Find optimal batch size for the available resources when running</span>
<span class="sd">                trainer.tune(). Defaults to False.</span>
<span class="sd">        gpus (int, optional): number of GPUs to use. Defaults to 0.</span>
<span class="sd">        check_val_every_n_epoch (int, optional): Run validation every n epochs. Defaults to 1.</span>
<span class="sd">        gradient_clip_val (float, optional): Clip gradient norm value. Defaults to 0 (no clipping).</span>
<span class="sd">        precision (int, optional): Floating point precision. Defaults to 32.</span>
<span class="sd">        num_nodes (int): Number of nodes to run on</span>
<span class="sd">        max_epochs (Optional[int], optional): Maximum number of epochs for training. Defaults to 100.</span>
<span class="sd">        max_steps (Optional[int], optional): Maximum number of steps for training. Defaults to None.</span>
<span class="sd">        truncated_bptt_steps (Optional[int], optional): Truncated back prop breaks performs backprop every k steps of much longer</span>
<span class="sd">                sequence. Defaults to None.</span>
<span class="sd">        fast_dev_run (Optional[int], optional): Run training on a small number of  batches for debugging. Defaults to None.</span>
<span class="sd">        overfit_batches (Optional[int], optional): Try to overfit a small number of batches for debugging. Defaults to None.</span>
<span class="sd">        terminate_on_nan (bool, optional): Terminate on NaN gradients. Warning this makes training slow. Defaults to False.</span>
<span class="sd">        profiler (Optional[Union[pl.profiler.BaseProfiler, bool, str]]): Use profiler to track execution times of each function</span>
<span class="sd">        early_stop_on (str): metric for early stopping</span>
<span class="sd">        early_stop_mode (str): &quot;min&quot; or &quot;max&quot;</span>

<span class="sd">    Returns:</span>
<span class="sd">        pl.Trainer: Configured trainer</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">overfit_batches</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">overfit_batches</span><span class="o">=</span><span class="n">overfit_batches</span><span class="p">,</span> <span class="n">gpus</span><span class="o">=</span><span class="n">gpus</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">trainer</span>

    <span class="k">if</span> <span class="n">fast_dev_run</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span><span class="n">fast_dev_run</span><span class="o">=</span><span class="n">fast_dev_run</span><span class="p">,</span> <span class="n">gpus</span><span class="o">=</span><span class="n">gpus</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">trainer</span>

    <span class="n">logging_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">experiments_folder</span><span class="p">,</span> <span class="n">experiment_name</span><span class="p">)</span>
    <span class="n">safe_mkdirs</span><span class="p">(</span><span class="n">logging_dir</span><span class="p">)</span>

    <span class="n">run_id</span> <span class="o">=</span> <span class="n">run_id</span> <span class="k">if</span> <span class="n">run_id</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">date_fname</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">run_id</span> <span class="ow">in</span> <span class="n">os</span><span class="o">.</span><span class="n">listdir</span><span class="p">(</span><span class="n">logging_dir</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;The run id you provided </span><span class="si">{run_id}</span><span class="s2"> already exists in </span><span class="si">{logging_dir}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">run_id</span> <span class="o">=</span> <span class="n">date_fname</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Setting run_id=</span><span class="si">{run_id}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">checkpoint_dir</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logging_dir</span><span class="p">,</span> <span class="n">run_id</span><span class="p">,</span> <span class="s2">&quot;checkpoints&quot;</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Logs will be saved in </span><span class="si">{</span><span class="n">logging_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Logs will be saved in </span><span class="si">{</span><span class="n">checkpoint_dir</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">wandb_project</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">wandb_project</span> <span class="o">=</span> <span class="n">experiment_name</span>

    <span class="n">connected</span> <span class="o">=</span> <span class="n">has_internet_connection</span><span class="p">()</span>
    <span class="n">offline_run</span> <span class="o">=</span> <span class="n">force_wandb_offline</span> <span class="ow">or</span> <span class="ow">not</span> <span class="n">connected</span>

    <span class="n">loggers</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">pl</span><span class="o">.</span><span class="n">loggers</span><span class="o">.</span><span class="n">CSVLogger</span><span class="p">(</span><span class="n">logging_dir</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s2">&quot;csv_logs&quot;</span><span class="p">,</span> <span class="n">version</span><span class="o">=</span><span class="n">run_id</span><span class="p">),</span>
        <span class="n">FixedWandbLogger</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="n">name</span><span class="o">=</span><span class="n">experiment_name</span><span class="p">,</span>
            <span class="n">project</span><span class="o">=</span><span class="n">wandb_project</span><span class="p">,</span>
            <span class="n">anonymous</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">save_dir</span><span class="o">=</span><span class="n">logging_dir</span><span class="p">,</span>
            <span class="n">version</span><span class="o">=</span><span class="n">run_id</span><span class="p">,</span>
            <span class="n">save_code</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">checkpoint_dir</span><span class="o">=</span><span class="n">checkpoint_dir</span><span class="p">,</span>
            <span class="n">offline</span><span class="o">=</span><span class="n">offline_run</span><span class="p">,</span>
            <span class="n">log_model</span><span class="o">=</span><span class="ow">not</span> <span class="n">offline_run</span><span class="p">,</span>
            <span class="n">entity</span><span class="o">=</span><span class="n">wandb_user</span><span class="p">,</span>
            <span class="n">group</span><span class="o">=</span><span class="n">experiment_group</span><span class="p">,</span>
            <span class="n">notes</span><span class="o">=</span><span class="n">experiment_description</span><span class="p">,</span>
            <span class="n">tags</span><span class="o">=</span><span class="n">tags</span><span class="p">,</span>
        <span class="p">),</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="n">gpus</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">loggers</span><span class="p">[</span>
            <span class="mi">1</span>
        <span class="p">]</span>  <span class="c1"># https://github.com/PyTorchLightning/pytorch-lightning/issues/6106</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Configured wandb and CSV loggers.&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Wandb configured to run </span><span class="si">{</span><span class="n">experiment_name</span><span class="si">}</span><span class="s2">/</span><span class="si">{</span><span class="n">run_id</span><span class="si">}</span><span class="s2"> in project </span><span class="si">{</span><span class="n">wandb_project</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">connected</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Results will be stored online.&quot;</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Results will be stored offline due to bad internet connection.&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;If you want to upload your results later run</span><span class="se">\n\t</span><span class="s2"> wandb sync </span><span class="si">{</span><span class="n">logging_dir</span><span class="si">}</span><span class="s2">/wandb/run-</span><span class="si">{</span><span class="n">run_id</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">experiment_description</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Experiment verbose description:</span><span class="se">\n</span><span class="si">{</span><span class="n">experiment_description</span><span class="si">}</span><span class="se">\n\n</span><span class="s2">Tags:</span><span class="si">{</span><span class="s1">&#39;n/a&#39;</span> <span class="k">if</span> <span class="n">tags</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">tags</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">EarlyStoppingWithLogs</span><span class="p">(</span>
            <span class="n">monitor</span><span class="o">=</span><span class="n">early_stop_on</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">early_stop_mode</span><span class="p">,</span>
            <span class="n">patience</span><span class="o">=</span><span class="n">patience</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">pl</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">ModelCheckpoint</span><span class="p">(</span>
            <span class="n">dirpath</span><span class="o">=</span><span class="n">checkpoint_dir</span><span class="p">,</span>
            <span class="n">filename</span><span class="o">=</span><span class="s2">&quot;</span><span class="si">{epoch}</span><span class="s2">-</span><span class="si">{val_loss:.2f}</span><span class="s2">&quot;</span><span class="p">,</span>
            <span class="n">monitor</span><span class="o">=</span><span class="n">early_stop_on</span><span class="p">,</span>
            <span class="n">save_top_k</span><span class="o">=</span><span class="n">save_top_k</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">early_stop_mode</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">pl</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateMonitor</span><span class="p">(</span><span class="n">logging_interval</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Configured Early stopping and Model checkpointing to track val_loss&quot;</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
        <span class="n">default_root_dir</span><span class="o">=</span><span class="n">logging_dir</span><span class="p">,</span>
        <span class="n">gpus</span><span class="o">=</span><span class="n">gpus</span><span class="p">,</span>
        <span class="n">max_epochs</span><span class="o">=</span><span class="n">max_epochs</span><span class="p">,</span>
        <span class="n">max_steps</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
        <span class="n">logger</span><span class="o">=</span><span class="n">loggers</span><span class="p">,</span>
        <span class="n">check_val_every_n_epoch</span><span class="o">=</span><span class="n">check_val_every_n_epoch</span><span class="p">,</span>
        <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
        <span class="n">auto_scale_batch_size</span><span class="o">=</span><span class="n">auto_scale_batch_size</span><span class="p">,</span>
        <span class="n">stochastic_weight_avg</span><span class="o">=</span><span class="n">stochastic_weight_avg</span><span class="p">,</span>
        <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
        <span class="n">truncated_bptt_steps</span><span class="o">=</span><span class="n">truncated_bptt_steps</span><span class="p">,</span>
        <span class="n">terminate_on_nan</span><span class="o">=</span><span class="n">terminate_on_nan</span><span class="p">,</span>
        <span class="n">progress_bar_refresh_rate</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">profiler</span><span class="o">=</span><span class="n">profiler</span><span class="p">,</span>
        <span class="n">num_nodes</span><span class="o">=</span><span class="n">num_nodes</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">trainer</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.plbind.trainer.make_trainer_for_ray_tune" class="doc doc-heading">
<code class="highlight language-python"><span class="n">make_trainer_for_ray_tune</span><span class="p">(</span><span class="n">patience</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">stochastic_weight_avg</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">gpus</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">gradient_clip_val</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">precision</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">max_epochs</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">max_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">truncated_bptt_steps</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">terminate_on_nan</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">early_stop_on</span><span class="o">=</span><span class="s1">&#39;val_loss&#39;</span><span class="p">,</span> <span class="n">early_stop_mode</span><span class="o">=</span><span class="s1">&#39;min&#39;</span><span class="p">,</span> <span class="n">metrics_map</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="o">**</span><span class="n">extra_kwargs</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Configure trainer with preferred defaults</p>
<ul>
<li>Early stopping on best validation loss is configured by default</li>
<li>Ray tune callback configured</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>patience</code></td>
        <td><code>int</code></td>
        <td><p>Patience for early stopping. Defaults to 3.</p></td>
        <td><code>3</code></td>
      </tr>
      <tr>
        <td><code>stochastic_weight_avg</code></td>
        <td><code>bool</code></td>
        <td><p>Use stochastic weight averaging. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>gpus</code></td>
        <td><code>int</code></td>
        <td><p>number of GPUs to use. Defaults to 0.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>gradient_clip_val</code></td>
        <td><code>float</code></td>
        <td><p>Clip gradient norm value. Defaults to 0 (no clipping).</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>precision</code></td>
        <td><code>int</code></td>
        <td><p>Floating point precision. Defaults to 32.</p></td>
        <td><code>32</code></td>
      </tr>
      <tr>
        <td><code>max_epochs</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Maximum number of epochs for training. Defaults to 100.</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>max_steps</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Maximum number of steps for training. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>truncated_bptt_steps</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Truncated back prop breaks performs backprop every k steps of much longer
    sequence. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>terminate_on_nan</code></td>
        <td><code>bool</code></td>
        <td><p>Terminate on NaN gradients. Warning this makes training slow. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>early_stop_on</code></td>
        <td><code>str</code></td>
        <td><p>metric for early stopping</p></td>
        <td><code>&#39;val_loss&#39;</code></td>
      </tr>
      <tr>
        <td><code>early_stop_mode</code></td>
        <td><code>str</code></td>
        <td><p>"min" or "max"</p></td>
        <td><code>&#39;min&#39;</code></td>
      </tr>
      <tr>
        <td><code>metrics_map</code></td>
        <td><code>Optional[Dict[str, str]]</code></td>
        <td><p>The mapping from pytorch lightning logged metrics
to ray tune metrics. The --tune-metric argument should be one of the keys of this
mapping</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>extra_kwargs</code></td>
        <td><code>kwargs</code></td>
        <td><p>Ignored. We use it so that we are able to pass the same config
object as in make_trainer</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Trainer</code></td>
      <td><p>pl.Trainer: Configured trainer</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/trainer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">make_trainer_for_ray_tune</span><span class="p">(</span>
    <span class="n">patience</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">stochastic_weight_avg</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">gpus</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">gradient_clip_val</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">precision</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">max_epochs</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">max_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">truncated_bptt_steps</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">terminate_on_nan</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>  <span class="c1"># Be careful this makes training very slow for large models</span>
    <span class="n">early_stop_on</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">,</span>
    <span class="n">early_stop_mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;min&quot;</span><span class="p">,</span>
    <span class="n">metrics_map</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="o">**</span><span class="n">extra_kwargs</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Configure trainer with preferred defaults</span>

<span class="sd">    * Early stopping on best validation loss is configured by default</span>
<span class="sd">    * Ray tune callback configured</span>

<span class="sd">    Args:</span>
<span class="sd">        patience (int, optional): Patience for early stopping. Defaults to 3.</span>
<span class="sd">        stochastic_weight_avg (bool, optional): Use stochastic weight averaging. Defaults to False.</span>
<span class="sd">        gpus (int, optional): number of GPUs to use. Defaults to 0.</span>
<span class="sd">        gradient_clip_val (float, optional): Clip gradient norm value. Defaults to 0 (no clipping).</span>
<span class="sd">        precision (int, optional): Floating point precision. Defaults to 32.</span>
<span class="sd">        max_epochs (Optional[int], optional): Maximum number of epochs for training. Defaults to 100.</span>
<span class="sd">        max_steps (Optional[int], optional): Maximum number of steps for training. Defaults to None.</span>
<span class="sd">        truncated_bptt_steps (Optional[int], optional): Truncated back prop breaks performs backprop every k steps of much longer</span>
<span class="sd">                sequence. Defaults to None.</span>
<span class="sd">        terminate_on_nan (bool, optional): Terminate on NaN gradients. Warning this makes training slow. Defaults to False.</span>
<span class="sd">        early_stop_on (str): metric for early stopping</span>
<span class="sd">        early_stop_mode (str): &quot;min&quot; or &quot;max&quot;</span>
<span class="sd">        metrics_map (Optional[Dict[str, str]]): The mapping from pytorch lightning logged metrics</span>
<span class="sd">            to ray tune metrics. The --tune-metric argument should be one of the keys of this</span>
<span class="sd">            mapping</span>
<span class="sd">        extra_kwargs (kwargs): Ignored. We use it so that we are able to pass the same config</span>
<span class="sd">            object as in make_trainer</span>
<span class="sd">    Returns:</span>
<span class="sd">        pl.Trainer: Configured trainer</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">metrics_map</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Need to pass metrics for TuneReportCallback&quot;</span><span class="p">)</span>

    <span class="n">callbacks</span> <span class="o">=</span> <span class="p">[</span>
        <span class="n">EarlyStoppingWithLogs</span><span class="p">(</span>
            <span class="n">monitor</span><span class="o">=</span><span class="n">early_stop_on</span><span class="p">,</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">early_stop_mode</span><span class="p">,</span>
            <span class="n">patience</span><span class="o">=</span><span class="n">patience</span><span class="p">,</span>
            <span class="n">verbose</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="p">),</span>
        <span class="n">TuneReportCallback</span><span class="p">(</span><span class="n">metrics_map</span><span class="p">,</span> <span class="n">on</span><span class="o">=</span><span class="s2">&quot;validation_end&quot;</span><span class="p">),</span>
        <span class="n">pl</span><span class="o">.</span><span class="n">callbacks</span><span class="o">.</span><span class="n">LearningRateMonitor</span><span class="p">(</span><span class="n">logging_interval</span><span class="o">=</span><span class="s2">&quot;step&quot;</span><span class="p">),</span>
    <span class="p">]</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Configured Early stopping to track val_loss&quot;</span><span class="p">)</span>

    <span class="n">trainer</span> <span class="o">=</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">(</span>
        <span class="n">gpus</span><span class="o">=</span><span class="n">gpus</span><span class="p">,</span>
        <span class="n">max_epochs</span><span class="o">=</span><span class="n">max_epochs</span><span class="p">,</span>
        <span class="n">max_steps</span><span class="o">=</span><span class="n">max_steps</span><span class="p">,</span>
        <span class="n">callbacks</span><span class="o">=</span><span class="n">callbacks</span><span class="p">,</span>
        <span class="n">logger</span><span class="o">=</span><span class="p">[],</span>
        <span class="n">check_val_every_n_epoch</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">gradient_clip_val</span><span class="o">=</span><span class="n">gradient_clip_val</span><span class="p">,</span>
        <span class="n">stochastic_weight_avg</span><span class="o">=</span><span class="n">stochastic_weight_avg</span><span class="p">,</span>
        <span class="n">precision</span><span class="o">=</span><span class="n">precision</span><span class="p">,</span>
        <span class="n">truncated_bptt_steps</span><span class="o">=</span><span class="n">truncated_bptt_steps</span><span class="p">,</span>
        <span class="n">terminate_on_nan</span><span class="o">=</span><span class="n">terminate_on_nan</span><span class="p">,</span>
        <span class="n">progress_bar_refresh_rate</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">num_sanity_val_steps</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
        <span class="n">auto_scale_batch_size</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">trainer</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.plbind.trainer.watch_model" class="doc doc-heading">
<code class="highlight language-python"><span class="n">watch_model</span><span class="p">(</span><span class="n">trainer</span><span class="p">,</span> <span class="n">model</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>If wandb logger is configured track gradient and weight norms</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>trainer</code></td>
        <td><code>Trainer</code></td>
        <td><p>Trainer</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>model</code></td>
        <td><code>Module</code></td>
        <td><p>Module to watch</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/plbind/trainer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">watch_model</span><span class="p">(</span><span class="n">trainer</span><span class="p">:</span> <span class="n">pl</span><span class="o">.</span><span class="n">Trainer</span><span class="p">,</span> <span class="n">model</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;If wandb logger is configured track gradient and weight norms</span>

<span class="sd">    Args:</span>
<span class="sd">        trainer (pl.Trainer): Trainer</span>
<span class="sd">        model (nn.Module): Module to watch</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">trainer</span><span class="o">.</span><span class="n">num_gpus</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
        <span class="k">return</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">log</span> <span class="ow">in</span> <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="p">:</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">log</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Tracking model weights &amp; gradients in wandb.&quot;</span><span class="p">)</span>

                <span class="k">break</span>
            <span class="k">except</span><span class="p">:</span>
                <span class="k">pass</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">trainer</span><span class="o">.</span><span class="n">logger</span><span class="o">.</span><span class="n">experiment</span><span class="o">.</span><span class="n">watch</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">log</span><span class="o">=</span><span class="s2">&quot;all&quot;</span><span class="p">)</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Tracking model weights &amp; gradients in wandb.&quot;</span><span class="p">)</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="k">pass</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.util.log"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-function">



<h2 id="slp.util.log.configure_logging" class="doc doc-heading">
<code class="highlight language-python"><span class="n">configure_logging</span><span class="p">(</span><span class="n">logfile_prefix</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>configure_logging Configure loguru to intercept logging module logs, tqdm.writes and write to a logfile</p>
<p>We use logure for stdout/stderr logging in this project.
This function configures loguru to intercept logs from other modules that use the default python logging module.
It also configures loguru so that it plays well with writes in the tqdm progress bars
If a logfile_prefix is provided, loguru will also write all logs into a logfile with a unique name constructed using
logfile_prefix and datetime.now()</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>logfile_prefix</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Optional prefix to file where logs will be written.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Optional[str]</code></td>
      <td><p>str: The logfile where logs are written</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">configure_logging</span><span class="p">(</span><span class="s2">&quot;logs/my-cool-experiment)</span>
<span class="n">logs</span><span class="o">/</span><span class="n">my</span><span class="o">-</span><span class="n">cool</span><span class="o">-</span><span class="n">experiment</span><span class="o">.</span><span class="mi">20210228</span><span class="o">-</span><span class="mf">211832.</span><span class="n">log</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/util/log.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">configure_logging</span><span class="p">(</span><span class="n">logfile_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;configure_logging Configure loguru to intercept logging module logs, tqdm.writes and write to a logfile</span>

<span class="sd">    We use logure for stdout/stderr logging in this project.</span>
<span class="sd">    This function configures loguru to intercept logs from other modules that use the default python logging module.</span>
<span class="sd">    It also configures loguru so that it plays well with writes in the tqdm progress bars</span>
<span class="sd">    If a logfile_prefix is provided, loguru will also write all logs into a logfile with a unique name constructed using</span>
<span class="sd">    logfile_prefix and datetime.now()</span>

<span class="sd">    Args:</span>
<span class="sd">        logfile_prefix (Optional[str]): Optional prefix to file where logs will be written.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: The logfile where logs are written</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; configure_logging(&quot;logs/my-cool-experiment)</span>
<span class="sd">        logs/my-cool-experiment.20210228-211832.log</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">class</span> <span class="nc">InterceptHandler</span><span class="p">(</span><span class="n">logging</span><span class="o">.</span><span class="n">Handler</span><span class="p">):</span>
        <span class="k">def</span> <span class="nf">emit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">record</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Intercept standard logging logs in loguru. Should test this for distributed pytorch lightning&quot;&quot;&quot;</span>
            <span class="c1"># Get corresponding Loguru level if it exists</span>
            <span class="k">try</span><span class="p">:</span>
                <span class="n">level</span> <span class="o">=</span> <span class="n">logger</span><span class="o">.</span><span class="n">level</span><span class="p">(</span><span class="n">record</span><span class="o">.</span><span class="n">levelname</span><span class="p">)</span><span class="o">.</span><span class="n">name</span>
            <span class="k">except</span> <span class="ne">ValueError</span><span class="p">:</span>
                <span class="n">level</span> <span class="o">=</span> <span class="n">record</span><span class="o">.</span><span class="n">levelno</span>

            <span class="c1"># Find caller from where originated the logged message</span>
            <span class="n">frame</span><span class="p">,</span> <span class="n">depth</span> <span class="o">=</span> <span class="n">logging</span><span class="o">.</span><span class="n">currentframe</span><span class="p">(),</span> <span class="mi">2</span>
            <span class="k">while</span> <span class="n">frame</span><span class="o">.</span><span class="n">f_code</span><span class="o">.</span><span class="n">co_filename</span> <span class="o">==</span> <span class="n">logging</span><span class="o">.</span><span class="vm">__file__</span><span class="p">:</span>
                <span class="n">frame</span> <span class="o">=</span> <span class="n">frame</span><span class="o">.</span><span class="n">f_back</span>
                <span class="n">depth</span> <span class="o">+=</span> <span class="mi">1</span>

            <span class="n">logger</span><span class="o">.</span><span class="n">opt</span><span class="p">(</span><span class="n">depth</span><span class="o">=</span><span class="n">depth</span><span class="p">,</span> <span class="n">exception</span><span class="o">=</span><span class="n">record</span><span class="o">.</span><span class="n">exc_info</span><span class="p">)</span><span class="o">.</span><span class="n">log</span><span class="p">(</span>
                <span class="n">level</span><span class="p">,</span> <span class="n">record</span><span class="o">.</span><span class="n">getMessage</span><span class="p">()</span>
            <span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Intercepting standard logging logs in loguru&quot;</span><span class="p">)</span>

    <span class="c1"># Make loguru play well with tqdm</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">remove</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">tqdm_write</span><span class="p">(</span><span class="n">msg</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Loguru wrapper for tqdm.write&quot;&quot;&quot;</span>
        <span class="k">return</span> <span class="n">tqdm</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">msg</span><span class="p">,</span> <span class="n">end</span><span class="o">=</span><span class="s2">&quot;&quot;</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="n">tqdm_write</span><span class="p">,</span> <span class="n">colorize</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>

    <span class="n">logging</span><span class="o">.</span><span class="n">basicConfig</span><span class="p">(</span><span class="n">handlers</span><span class="o">=</span><span class="p">[</span><span class="n">InterceptHandler</span><span class="p">()],</span> <span class="n">level</span><span class="o">=</span><span class="n">logging</span><span class="o">.</span><span class="n">INFO</span><span class="p">)</span>

    <span class="n">logfile</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="n">logfile_prefix</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logfile</span> <span class="o">=</span> <span class="n">log_to_file</span><span class="p">(</span><span class="n">logfile_prefix</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Log file will be saved in </span><span class="si">{</span><span class="n">logfile</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">logfile</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.log.log_to_file" class="doc doc-heading">
<code class="highlight language-python"><span class="n">log_to_file</span><span class="p">(</span><span class="n">fname_prefix</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>log_to_file Configure loguru to log to a logfile</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>fname_prefix</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Optional prefix to file where logs will be written.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>str</code></td>
      <td><p>str: The logfile where logs are written</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/log.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">log_to_file</span><span class="p">(</span><span class="n">fname_prefix</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;log_to_file Configure loguru to log to a logfile</span>

<span class="sd">    Args:</span>
<span class="sd">        fname_prefix (Optional[str]): Optional prefix to file where logs will be written.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: The logfile where logs are written</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">logfile</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">fname_prefix</span><span class="si">}</span><span class="s2">.</span><span class="si">{</span><span class="n">date_fname</span><span class="p">()</span><span class="si">}</span><span class="s2">.log&quot;</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">add</span><span class="p">(</span>
        <span class="n">logfile</span><span class="p">,</span>
        <span class="n">colorize</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="n">level</span><span class="o">=</span><span class="s2">&quot;DEBUG&quot;</span><span class="p">,</span>
        <span class="n">enqueue</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">logfile</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.util.pytorch"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.util.pytorch.NoOp" class="doc doc-heading">
        <code>NoOp</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.util.pytorch.NoOp.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.util.pytorch.PackSequence" class="doc doc-heading">
        <code>PackSequence</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.util.pytorch.PackSequence.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Wrap sequence packing in nn.Module</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Use batch first representation. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrap sequence packing in nn.Module</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_first (bool, optional): Use batch first representation. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PackSequence</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.util.pytorch.PackSequence.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Pack a padded sequence and sort lengths</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>Padded tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Tensor</code></td>
        <td><p>Original lengths befor padding</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.nn.utils.rnn.PackedSequence, torch.Tensor]</code></td>
      <td><p>Tuple[torch.nn.utils.rnn.PackedSequence, torch.Tensor]: (packed sequence, sorted lengths)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">PackedSequence</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Pack a padded sequence and sort lengths</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): Padded tensor</span>
<span class="sd">        lengths (torch.Tensor): Original lengths befor padding</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.nn.utils.rnn.PackedSequence, torch.Tensor]: (packed sequence, sorted lengths)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">PackedSequence</span> <span class="o">=</span> <span class="n">pack_padded_sequence</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span><span class="p">,</span> <span class="n">enforce_sorted</span><span class="o">=</span><span class="kc">False</span>
    <span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="n">out</span><span class="o">.</span><span class="n">sorted_indices</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">lengths</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.util.pytorch.PadPackedSequence" class="doc doc-heading">
        <code>PadPackedSequence</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.util.pytorch.PadPackedSequence.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Wrap sequence padding in nn.Module</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Use batch first representation. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrap sequence padding in nn.Module</span>

<span class="sd">    Args:</span>
<span class="sd">        batch_first (bool, optional): Use batch first representation. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PadPackedSequence</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span> <span class="k">if</span> <span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">0</span> <span class="k">else</span> <span class="kc">None</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.util.pytorch.PadPackedSequence.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Convert packed sequence to padded sequence</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>PackedSequence</code></td>
        <td><p>Packed sequence</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Tensor</code></td>
        <td><p>Sorted original sequence lengths</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Padded sequence</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">PackedSequence</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Convert packed sequence to padded sequence</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.nn.utils.rnn.PackedSequence): Packed sequence</span>
<span class="sd">        lengths (torch.Tensor): Sorted original sequence lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Padded sequence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">pad_packed_sequence</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span><span class="p">,</span> <span class="n">total_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span>  <span class="c1"># type: ignore</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.from_checkpoint" class="doc doc-heading">
<code class="highlight language-python"><span class="n">from_checkpoint</span><span class="p">(</span><span class="n">checkpoint_file</span><span class="p">,</span> <span class="n">obj</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">dataparallel</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Load model or optimizer from saved state_dict</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>checkpoint_file</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>File containing the state dict</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>obj</code></td>
        <td><code>Union[torch.nn.modules.module.Module, torch.optim.optimizer.Optimizer]</code></td>
        <td><p>Module or optimizer instance to load the checkpoint</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>map_location</code></td>
        <td><code>Union[torch.device, str]</code></td>
        <td><p>Where to load. Defaults to "cpu".</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>dataparallel</code></td>
        <td><code>bool</code></td>
        <td><p>If data parallel remove leading "module." from statedict keys. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch.nn.modules.module.Module, torch.optim.optimizer.Optimizer]</code></td>
      <td><p>types.ModuleOrOptimizer: Loaded module or optimizer</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">from_checkpoint</span><span class="p">(</span>
    <span class="n">checkpoint_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">obj</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">ModuleOrOptimizer</span><span class="p">,</span>
    <span class="n">map_location</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">types</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">dataparallel</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">types</span><span class="o">.</span><span class="n">ModuleOrOptimizer</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Load model or optimizer from saved state_dict</span>

<span class="sd">    Args:</span>
<span class="sd">        checkpoint_file (Optional[str]): File containing the state dict</span>
<span class="sd">        obj (types.ModuleOrOptimizer): Module or optimizer instance to load the checkpoint</span>
<span class="sd">        map_location (Optional[types.Device], optional): Where to load. Defaults to &quot;cpu&quot;.</span>
<span class="sd">        dataparallel (bool, optional): If data parallel remove leading &quot;module.&quot; from statedict keys. Defaults to False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        types.ModuleOrOptimizer: Loaded module or optimizer</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">checkpoint_file</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">obj</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">system</span><span class="o">.</span><span class="n">is_file</span><span class="p">(</span><span class="n">checkpoint_file</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The checkpoint </span><span class="si">{</span><span class="n">checkpoint_file</span><span class="si">}</span><span class="s2"> you are trying to load &quot;</span>
            <span class="s2">&quot;does not exist. Continuing without loading...&quot;</span>
        <span class="p">)</span>

        <span class="k">return</span> <span class="n">obj</span>

    <span class="n">state_dict</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">checkpoint_file</span><span class="p">,</span> <span class="n">map_location</span><span class="o">=</span><span class="n">map_location</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">dataparallel</span><span class="p">:</span>
        <span class="n">state_dict</span> <span class="o">=</span> <span class="p">{</span><span class="n">k</span><span class="o">.</span><span class="n">replace</span><span class="p">(</span><span class="s2">&quot;module.&quot;</span><span class="p">,</span> <span class="s2">&quot;&quot;</span><span class="p">):</span> <span class="n">v</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="n">state_dict</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>
    <span class="n">obj</span><span class="o">.</span><span class="n">load_state_dict</span><span class="p">(</span><span class="n">state_dict</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">obj</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.mktensor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">mktensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">copy_tensor</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Convert a list or numpy array to torch tensor. If a torch tensor
    is passed it is cast to  dtype, device and the requires_grad flag is
    set. This can copy data or make the operation in place.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>data</code></td>
        <td><code>Union[numpy.ndarray, torch.Tensor, List[~T]]</code></td>
        <td><p>(list, np.ndarray, torch.Tensor): Data to be converted to
torch tensor.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dtype</code></td>
        <td><code>dtype</code></td>
        <td><p>(torch.dtype): The type of the tensor elements
(Default value = torch.float)</p></td>
        <td><code>torch.float32</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>Union[torch.device, str]</code></td>
        <td><p>(torch.device, str): Device where the tensor should be
(Default value = 'cpu')</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>requires_grad</code></td>
        <td><code>bool</code></td>
        <td><p>(bool): Trainable tensor or not? (Default value = False)</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>copy_tensor</code></td>
        <td><code>bool</code></td>
        <td><p>(bool): If false creates the tensor inplace else makes a copy
(Default value = True)</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>(torch.Tensor): A tensor of appropriate dtype, device and
    requires_grad containing data</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">mktensor</span><span class="p">(</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">NdTensor</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">Device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">copy_tensor</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Convert a list or numpy array to torch tensor. If a torch tensor</span>
<span class="sd">        is passed it is cast to  dtype, device and the requires_grad flag is</span>
<span class="sd">        set. This can copy data or make the operation in place.</span>

<span class="sd">    Args:</span>
<span class="sd">        data: (list, np.ndarray, torch.Tensor): Data to be converted to</span>
<span class="sd">            torch tensor.</span>
<span class="sd">        dtype: (torch.dtype): The type of the tensor elements</span>
<span class="sd">            (Default value = torch.float)</span>
<span class="sd">        device: (torch.device, str): Device where the tensor should be</span>
<span class="sd">            (Default value = &#39;cpu&#39;)</span>
<span class="sd">        requires_grad: (bool): Trainable tensor or not? (Default value = False)</span>
<span class="sd">        copy_tensor: (bool): If false creates the tensor inplace else makes a copy</span>
<span class="sd">            (Default value = True)</span>

<span class="sd">    Returns:</span>
<span class="sd">        (torch.Tensor): A tensor of appropriate dtype, device and</span>
<span class="sd">            requires_grad containing data</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tensor_factory</span> <span class="o">=</span> <span class="n">t</span> <span class="k">if</span> <span class="n">copy_tensor</span> <span class="k">else</span> <span class="n">t_</span>

    <span class="k">return</span> <span class="n">tensor_factory</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.moore_penrose_pinv" class="doc doc-heading">
<code class="highlight language-python"><span class="n">moore_penrose_pinv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">6</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Calculate approximate Moore-Penrose pseudoinverse, via iterative method</p>
<ul>
<li>Method is described in (Razavi et al 2014) https://www.hindawi.com/journals/aaa/2014/563787/</li>
<li>Implementation modified from lucidrains https://github.com/lucidrains/nystrom-attention/blob/main/nystrom_attention/nystrom_attention.py#L13</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>torch.Tensor</code></td>
        <td><p>(*, M, M) The square tensors to inverse.
Dimension * can be any number of additional dimensions, e.g. (batch_size, num_heads, M, M)</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_iter</code></td>
        <td><code>int</code></td>
        <td><p>Number of iterations to run for approximation (6 is good enough usually)</p></td>
        <td><code>6</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(torch.Tensor)</code></td>
      <td><p>(B, H, N, N) The approximate Moore-Penrose pseudoinverse of mat</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">moore_penrose_pinv</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="mi">6</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate approximate Moore-Penrose pseudoinverse, via iterative method</span>

<span class="sd">    * Method is described in (Razavi et al 2014) https://www.hindawi.com/journals/aaa/2014/563787/</span>
<span class="sd">    * Implementation modified from lucidrains https://github.com/lucidrains/nystrom-attention/blob/main/nystrom_attention/nystrom_attention.py#L13</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): (*, M, M) The square tensors to inverse.</span>
<span class="sd">            Dimension * can be any number of additional dimensions, e.g. (batch_size, num_heads, M, M)</span>
<span class="sd">        num_iter (int): Number of iterations to run for approximation (6 is good enough usually)</span>
<span class="sd">    Returns:</span>
<span class="sd">        (torch.Tensor): (B, H, N, N) The approximate Moore-Penrose pseudoinverse of mat</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">abs_x</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">abs</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">col</span> <span class="o">=</span> <span class="n">abs_x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">row</span> <span class="o">=</span> <span class="n">abs_x</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dim</span><span class="o">=-</span><span class="mi">2</span><span class="p">)</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>
    <span class="n">z</span> <span class="o">=</span> <span class="n">z</span> <span class="o">/</span> <span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">col</span><span class="p">)</span> <span class="o">*</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">row</span><span class="p">))</span>

    <span class="n">I</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">eye</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="n">x</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_iter</span><span class="p">):</span>
        <span class="n">xz</span> <span class="o">=</span> <span class="n">x</span> <span class="o">@</span> <span class="n">z</span>
        <span class="n">z</span> <span class="o">=</span> <span class="mf">0.25</span> <span class="o">*</span> <span class="n">z</span> <span class="o">@</span> <span class="p">(</span><span class="mi">13</span> <span class="o">*</span> <span class="n">I</span> <span class="o">-</span> <span class="p">(</span><span class="n">xz</span> <span class="o">@</span> <span class="p">(</span><span class="mi">15</span> <span class="o">*</span> <span class="n">I</span> <span class="o">-</span> <span class="p">(</span><span class="n">xz</span> <span class="o">@</span> <span class="p">(</span><span class="mi">7</span> <span class="o">*</span> <span class="n">I</span> <span class="o">-</span> <span class="n">xz</span><span class="p">)))))</span>

    <span class="k">return</span> <span class="n">z</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.pad_mask" class="doc doc-heading">
<code class="highlight language-python"><span class="n">pad_mask</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Generate mask for padded tokens</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Tensor</code></td>
        <td><p>Original sequence lengths before padding</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>Union[torch.Tensor, int]</code></td>
        <td><p>Maximum sequence length. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: padding mask</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">pad_mask</span><span class="p">(</span>
    <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">max_length</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Generate mask for padded tokens</span>

<span class="sd">    Args:</span>
<span class="sd">        lengths (torch.Tensor): Original sequence lengths before padding</span>
<span class="sd">        max_length (Optional[Union[torch.Tensor, int]], optional): Maximum sequence length. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: padding mask</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">max_length</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">max_length</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">max_length</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">lengths</span><span class="p">)</span><span class="o">.</span><span class="n">item</span><span class="p">())</span>
    <span class="n">max_length</span> <span class="o">=</span> <span class="n">cast</span><span class="p">(</span><span class="nb">int</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
    <span class="n">idx</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">lengths</span><span class="o">.</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">mask</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="p">(</span><span class="n">idx</span> <span class="o">&lt;</span> <span class="n">lengths</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">mask</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.pad_sequence" class="doc doc-heading">
<code class="highlight language-python"><span class="n">pad_sequence</span><span class="p">(</span><span class="n">sequences</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">padding_value</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Pad a list of variable length Tensors with <code>padding_value</code></p>
<p><code>pad_sequence</code> stacks a list of Tensors along a new dimension,
and pads them to equal length. For example, if the input is list of
sequences with size <code>L x *</code> and if batch_first is False, and <code>T x B x *</code>
otherwise.</p>
<p><code>B</code> is batch size. It is equal to the number of elements in <code>sequences</code>.
<code>T</code> is length of the longest sequence.
<code>L</code> is length of the sequence.
<code>*</code> is any number of trailing dimensions, including none.</p>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">torch.nn.utils.rnn</span> <span class="kn">import</span> <span class="n">pad_sequence</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">a</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">25</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">b</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">22</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">c</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">15</span><span class="p">,</span> <span class="mi">300</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">pad_sequence</span><span class="p">([</span><span class="n">a</span><span class="p">,</span> <span class="n">b</span><span class="p">,</span> <span class="n">c</span><span class="p">])</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">25</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">300</span><span class="p">])</span>
</code></pre></div>
      <p>!!! note
    This function returns a Tensor of size <code>T x B x *</code> or <code>B x T x *</code>
    where <code>T</code> is the length of the longest sequence. This function assumes
    trailing dimensions and type of all the Tensors in sequences are same.</p>
<pre><code>Note:
This implementation is modified from torch.nn.utils.rnn.pad_sequence, to accept a
max_length argument for fixed length padding
</code></pre>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>sequences</code></td>
        <td><code>List[torch.Tensor]</code></td>
        <td><p>list of variable length sequences.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>output will be in <code>B x T x *</code> if True, or in
<code>T x B x *</code> otherwise</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>padding_value</code></td>
        <td><code>Union[float, int]</code></td>
        <td><p>value for padded elements. Default: 0.</p></td>
        <td><code>0.0</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>If max length is &gt; 0 then this function will pad to a fixed maximum
length. If any sequence is longer than max_length, it will be trimmed.</p></td>
        <td><code>-1</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor of size ``T x B x *`` if </code></td>
      <td><p>attr:<code>batch_first</code> is <code>False</code>.
Tensor of size <code>B x T x *</code> otherwise</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">pad_sequence</span><span class="p">(</span>
    <span class="n">sequences</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">padding_value</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="nb">float</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Pad a list of variable length Tensors with ``padding_value``</span>

<span class="sd">    ``pad_sequence`` stacks a list of Tensors along a new dimension,</span>
<span class="sd">    and pads them to equal length. For example, if the input is list of</span>
<span class="sd">    sequences with size ``L x *`` and if batch_first is False, and ``T x B x *``</span>
<span class="sd">    otherwise.</span>

<span class="sd">    `B` is batch size. It is equal to the number of elements in ``sequences``.</span>
<span class="sd">    `T` is length of the longest sequence.</span>
<span class="sd">    `L` is length of the sequence.</span>
<span class="sd">    `*` is any number of trailing dimensions, including none.</span>

<span class="sd">    Example:</span>
<span class="sd">        &gt;&gt;&gt; from torch.nn.utils.rnn import pad_sequence</span>
<span class="sd">        &gt;&gt;&gt; a = torch.ones(25, 300)</span>
<span class="sd">        &gt;&gt;&gt; b = torch.ones(22, 300)</span>
<span class="sd">        &gt;&gt;&gt; c = torch.ones(15, 300)</span>
<span class="sd">        &gt;&gt;&gt; pad_sequence([a, b, c]).size()</span>
<span class="sd">        torch.Size([25, 3, 300])</span>

<span class="sd">    Note:</span>
<span class="sd">        This function returns a Tensor of size ``T x B x *`` or ``B x T x *``</span>
<span class="sd">        where `T` is the length of the longest sequence. This function assumes</span>
<span class="sd">        trailing dimensions and type of all the Tensors in sequences are same.</span>

<span class="sd">        Note:</span>
<span class="sd">        This implementation is modified from torch.nn.utils.rnn.pad_sequence, to accept a</span>
<span class="sd">        max_length argument for fixed length padding</span>

<span class="sd">    Args:</span>
<span class="sd">        sequences (list[Tensor]): list of variable length sequences.</span>
<span class="sd">        batch_first (bool, optional): output will be in ``B x T x *`` if True, or in</span>
<span class="sd">            ``T x B x *`` otherwise</span>
<span class="sd">        padding_value (float, optional): value for padded elements. Default: 0.</span>
<span class="sd">        max_length (int): If max length is &gt; 0 then this function will pad to a fixed maximum</span>
<span class="sd">            length. If any sequence is longer than max_length, it will be trimmed.</span>
<span class="sd">    Returns:</span>
<span class="sd">        Tensor of size ``T x B x *`` if :attr:`batch_first` is ``False``.</span>
<span class="sd">        Tensor of size ``B x T x *`` otherwise</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="c1"># assuming trailing dimensions and type of all the Tensors</span>
    <span class="c1"># in sequences are same and fetching those from sequences[0]</span>
    <span class="n">max_size</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">trailing_dims</span> <span class="o">=</span> <span class="n">max_size</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="k">if</span> <span class="n">max_length</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">max_len</span> <span class="o">=</span> <span class="nb">max</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">sequences</span><span class="p">])</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">max_len</span> <span class="o">=</span> <span class="n">max_length</span>
    <span class="k">if</span> <span class="n">batch_first</span><span class="p">:</span>
        <span class="n">out_dims</span> <span class="o">=</span> <span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">),</span> <span class="n">max_len</span><span class="p">)</span> <span class="o">+</span> <span class="n">trailing_dims</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">out_dims</span> <span class="o">=</span> <span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">sequences</span><span class="p">))</span> <span class="o">+</span> <span class="n">trailing_dims</span>

    <span class="n">out_tensor</span> <span class="o">=</span> <span class="n">sequences</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">new_full</span><span class="p">(</span><span class="n">out_dims</span><span class="p">,</span> <span class="n">padding_value</span><span class="p">)</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">tensor</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">sequences</span><span class="p">):</span>
        <span class="n">length</span> <span class="o">=</span> <span class="n">tensor</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
        <span class="c1"># use index notation to prevent duplicate references to the tensor</span>
        <span class="k">if</span> <span class="n">batch_first</span><span class="p">:</span>
            <span class="n">out_tensor</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">max_len</span><span class="p">),</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span>
                <span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">max_len</span><span class="p">),</span> <span class="o">...</span>
            <span class="p">]</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">out_tensor</span><span class="p">[:</span> <span class="nb">min</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">max_len</span><span class="p">),</span> <span class="n">i</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="o">=</span> <span class="n">tensor</span><span class="p">[</span>
                <span class="p">:</span> <span class="nb">min</span><span class="p">(</span><span class="n">length</span><span class="p">,</span> <span class="n">max_len</span><span class="p">),</span> <span class="o">...</span>
            <span class="p">]</span>

    <span class="k">return</span> <span class="n">out_tensor</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.repeat_layer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">repeat_layer</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">times</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Clone a layer multiple times</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>l</code></td>
        <td><code>Module</code></td>
        <td><p>nn.Module to stack</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>times</code></td>
        <td><code>int</code></td>
        <td><p>Times to clone</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[torch.nn.modules.module.Module]</code></td>
      <td><p>List[nn.Module]: List of identical clones of input layer</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">repeat_layer</span><span class="p">(</span><span class="n">l</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span> <span class="n">times</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Clone a layer multiple times</span>

<span class="sd">    Args:</span>
<span class="sd">        l (nn.Module): nn.Module to stack</span>
<span class="sd">        times (int): Times to clone</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[nn.Module]: List of identical clones of input layer</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="p">[</span><span class="n">l</span><span class="p">]</span> <span class="o">+</span> <span class="p">[</span><span class="n">copy</span><span class="o">.</span><span class="n">deepcopy</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">times</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.rotate_tensor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">rotate_tensor</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Roate tensor by n positions to the right</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>l</code></td>
        <td><code>Tensor</code></td>
        <td><p>input tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n</code></td>
        <td><code>int</code></td>
        <td><p>positions to rotate. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: rotated tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">rotate_tensor</span><span class="p">(</span><span class="n">l</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Roate tensor by n positions to the right</span>

<span class="sd">    Args:</span>
<span class="sd">        l (torch.Tensor): input tensor</span>
<span class="sd">        n (int, optional): positions to rotate. Defaults to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: rotated tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">((</span><span class="n">l</span><span class="p">[</span><span class="n">n</span><span class="p">:],</span> <span class="n">l</span><span class="p">[:</span><span class="n">n</span><span class="p">]))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.shift_tensor" class="doc doc-heading">
<code class="highlight language-python"><span class="n">shift_tensor</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Shift tensor by n positions</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>l</code></td>
        <td><code>Tensor</code></td>
        <td><p>input tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n</code></td>
        <td><code>int</code></td>
        <td><p>positions to shift. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: shifted tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">shift_tensor</span><span class="p">(</span><span class="n">l</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Shift tensor by n positions</span>

<span class="sd">    Args:</span>
<span class="sd">        l (torch.Tensor): input tensor</span>
<span class="sd">        n (int, optional): positions to shift. Defaults to 1.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: shifted tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">rotate_tensor</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="n">n</span><span class="p">)</span>
    <span class="n">out</span><span class="p">[</span><span class="o">-</span><span class="n">n</span><span class="p">:]</span> <span class="o">=</span> <span class="mi">0</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.sort_sequences" class="doc doc-heading">
<code class="highlight language-python"><span class="n">sort_sequences</span><span class="p">(</span><span class="n">inputs</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Sort sequences according to lengths (descending)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>inputs</code></td>
        <td><code>Tensor</code></td>
        <td><p>input sequences, size [B, T, D]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Tensor</code></td>
        <td><p>length of each sequence, size [B]</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor, Callable[[torch.Tensor], torch.Tensor]]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor, Callable[[torch.Tensor], torch.tensor]]:
    (sorted inputs, sorted lengths, function to revert inputs and lengths to unsorted state)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">sort_sequences</span><span class="p">(</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Sort sequences according to lengths (descending)</span>

<span class="sd">    Args:</span>
<span class="sd">        inputs (torch.Tensor): input sequences, size [B, T, D]</span>
<span class="sd">        lengths (torch.Tensor): length of each sequence, size [B]</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor, Callable[[torch.Tensor], torch.tensor]]:</span>
<span class="sd">            (sorted inputs, sorted lengths, function to revert inputs and lengths to unsorted state)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">lengths_sorted</span><span class="p">,</span> <span class="n">sorted_idx</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">descending</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">unsorted_idx</span> <span class="o">=</span> <span class="n">sorted_idx</span><span class="o">.</span><span class="n">sort</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">unsort</span><span class="p">(</span><span class="n">tt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Restore original unsorted sequence&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">tt</span><span class="p">[</span><span class="n">unsorted_idx</span><span class="p">]</span>

    <span class="k">return</span> <span class="n">inputs</span><span class="p">[</span><span class="n">sorted_idx</span><span class="p">],</span> <span class="n">lengths_sorted</span><span class="p">,</span> <span class="n">unsort</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.subsequent_mask" class="doc doc-heading">
<code class="highlight language-python"><span class="n">subsequent_mask</span><span class="p">(</span><span class="n">max_length</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Generate subsequent (lower triangular) mask for transformer autoregressive tasks</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Maximum sequence length</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: The subsequent mask</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">subsequent_mask</span><span class="p">(</span><span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Generate subsequent (lower triangular) mask for transformer autoregressive tasks</span>

<span class="sd">    Args:</span>
<span class="sd">        max_length (int): Maximum sequence length</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The subsequent mask</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">max_length</span><span class="p">,</span> <span class="n">max_length</span><span class="p">)</span>
    <span class="c1"># Ignore typecheck because pytorch types are incomplete</span>

    <span class="k">return</span> <span class="n">mask</span><span class="o">.</span><span class="n">triu</span><span class="p">()</span><span class="o">.</span><span class="n">t</span><span class="p">()</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.t" class="doc doc-heading">
<code class="highlight language-python"><span class="n">t</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Convert a list or numpy array to torch tensor. If a torch tensor
is passed it is cast to  dtype, device and the requires_grad flag is
set. This always copies data.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>data</code></td>
        <td><code>Union[numpy.ndarray, torch.Tensor, List[~T]]</code></td>
        <td><p>(list, np.ndarray, torch.Tensor): Data to be converted to
torch tensor.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dtype</code></td>
        <td><code>dtype</code></td>
        <td><p>(torch.dtype): The type of the tensor elements
(Default value = torch.float)</p></td>
        <td><code>torch.float32</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>Union[torch.device, str]</code></td>
        <td><p>(torch.device, str): Device where the tensor should be
(Default value = 'cpu')</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>requires_grad</code></td>
        <td><code>bool</code></td>
        <td><p>(bool): Trainable tensor or not? (Default value = False)</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>(torch.Tensor): A tensor of appropriate dtype, device and
    requires_grad containing data</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">t</span><span class="p">(</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">NdTensor</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">Device</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Convert a list or numpy array to torch tensor. If a torch tensor</span>
<span class="sd">    is passed it is cast to  dtype, device and the requires_grad flag is</span>
<span class="sd">    set. This always copies data.</span>

<span class="sd">    Args:</span>
<span class="sd">        data: (list, np.ndarray, torch.Tensor): Data to be converted to</span>
<span class="sd">            torch tensor.</span>
<span class="sd">        dtype: (torch.dtype): The type of the tensor elements</span>
<span class="sd">            (Default value = torch.float)</span>
<span class="sd">        device: (torch.device, str): Device where the tensor should be</span>
<span class="sd">            (Default value = &#39;cpu&#39;)</span>
<span class="sd">        requires_grad: (bool): Trainable tensor or not? (Default value = False)</span>

<span class="sd">    Returns:</span>
<span class="sd">        (torch.Tensor): A tensor of appropriate dtype, device and</span>
<span class="sd">            requires_grad containing data</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">tt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tt</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.t_" class="doc doc-heading">
<code class="highlight language-python"><span class="n">t_</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Convert a list or numpy array to torch tensor. If a torch tensor
is passed it is cast to  dtype, device and the requires_grad flag is
set IN PLACE.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>data</code></td>
        <td><code>Union[numpy.ndarray, torch.Tensor, List[~T]]</code></td>
        <td><p>(list, np.ndarray, torch.Tensor): Data to be converted to
torch tensor.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dtype</code></td>
        <td><code>dtype</code></td>
        <td><p>(torch.dtype): The type of the tensor elements
(Default value = torch.float)</p></td>
        <td><code>torch.float32</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>Union[torch.device, str]</code></td>
        <td><p>(torch.device, str): Device where the tensor should be
(Default value = 'cpu')</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>requires_grad</code></td>
        <td><code>bool</code></td>
        <td><p>bool): Trainable tensor or not? (Default value = False)</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>(torch.Tensor): A tensor of appropriate dtype, device and
    requires_grad containing data</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">t_</span><span class="p">(</span>
    <span class="n">data</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">NdTensor</span><span class="p">,</span>
    <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
    <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">types</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
    <span class="n">requires_grad</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Convert a list or numpy array to torch tensor. If a torch tensor</span>
<span class="sd">    is passed it is cast to  dtype, device and the requires_grad flag is</span>
<span class="sd">    set IN PLACE.</span>

<span class="sd">    Args:</span>
<span class="sd">        data: (list, np.ndarray, torch.Tensor): Data to be converted to</span>
<span class="sd">            torch tensor.</span>
<span class="sd">        dtype: (torch.dtype): The type of the tensor elements</span>
<span class="sd">            (Default value = torch.float)</span>
<span class="sd">        device: (torch.device, str): Device where the tensor should be</span>
<span class="sd">            (Default value = &#39;cpu&#39;)</span>
<span class="sd">        requires_grad: bool): Trainable tensor or not? (Default value = False)</span>

<span class="sd">    Returns:</span>
<span class="sd">        (torch.Tensor): A tensor of appropriate dtype, device and</span>
<span class="sd">            requires_grad containing data</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="nb">str</span><span class="p">):</span>
        <span class="n">device</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">device</span><span class="p">(</span><span class="n">device</span><span class="p">)</span>

    <span class="n">tt</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">as_tensor</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">dtype</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="n">device</span><span class="p">)</span><span class="o">.</span><span class="n">requires_grad_</span><span class="p">(</span><span class="n">requires_grad</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">tt</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.pytorch.to_device" class="doc doc-heading">
<code class="highlight language-python"><span class="n">to_device</span><span class="p">(</span><span class="n">tt</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Send a tensor to a device</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>tt</code></td>
        <td><code>Tensor</code></td>
        <td><p>input tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>Union[torch.device, str]</code></td>
        <td><p>Output device. Defaults to "cpu".</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>non_blocking</code></td>
        <td><code>bool</code></td>
        <td><p>Use blocking or non-blocking memory transfer. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Tensor in the desired device</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/pytorch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">to_device</span><span class="p">(</span>
    <span class="n">tt</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">types</span><span class="o">.</span><span class="n">Device</span><span class="p">]</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">non_blocking</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Send a tensor to a device</span>

<span class="sd">    Args:</span>
<span class="sd">        tt (torch.Tensor): input tensor</span>
<span class="sd">        device (Optional[types.Device], optional): Output device. Defaults to &quot;cpu&quot;.</span>
<span class="sd">        non_blocking (bool, optional): Use blocking or non-blocking memory transfer. Defaults to False.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Tensor in the desired device</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">tt</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">device</span><span class="p">,</span> <span class="n">non_blocking</span><span class="o">=</span><span class="n">non_blocking</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.util.system"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.date_fname" class="doc doc-heading">
<code class="highlight language-python"><span class="n">date_fname</span><span class="p">()</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>date_fname Generate a filename based on datetime.now().</p>
<p>If multiple calls are made within the same second, the filename will not be unique.
We could add miliseconds etc. in the fname but that would hinder readability.
For practical purposes e.g. unique logs between different experiments this should be enough.
Either way if we need a truly unique descriptor, there is the uuid module.</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>str</code></td>
      <td><p>str: A filename, e.g. 20210228-211832</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">date_fname</span><span class="p">()</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;date_fname Generate a filename based on datetime.now().</span>

<span class="sd">    If multiple calls are made within the same second, the filename will not be unique.</span>
<span class="sd">    We could add miliseconds etc. in the fname but that would hinder readability.</span>
<span class="sd">    For practical purposes e.g. unique logs between different experiments this should be enough.</span>
<span class="sd">    Either way if we need a truly unique descriptor, there is the uuid module.</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: A filename, e.g. 20210228-211832</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">datetime</span><span class="o">.</span><span class="n">now</span><span class="p">()</span><span class="o">.</span><span class="n">strftime</span><span class="p">(</span><span class="s2">&quot;%Y%m</span><span class="si">%d</span><span class="s2">-%H%M%S&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.download_url" class="doc doc-heading">
<code class="highlight language-python"><span class="n">download_url</span><span class="p">(</span><span class="n">url</span><span class="p">,</span> <span class="n">dest_path</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>download_url Download a file to a destination path given a URL</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>url</code></td>
        <td><code>str</code></td>
        <td><p>A url pointing to the file we want to download</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dest_path</code></td>
        <td><code>str</code></td>
        <td><p>The destination path to write the file</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>str</code></td>
      <td><p>(str): The filename where the downloaded file is written</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">download_url</span><span class="p">(</span><span class="n">url</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">dest_path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;download_url Download a file to a destination path given a URL</span>

<span class="sd">    Args:</span>
<span class="sd">        url (str): A url pointing to the file we want to download</span>
<span class="sd">        dest_path (str): The destination path to write the file</span>

<span class="sd">    Returns:</span>
<span class="sd">        (str): The filename where the downloaded file is written</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">name</span> <span class="o">=</span> <span class="n">url</span><span class="o">.</span><span class="n">rsplit</span><span class="p">(</span><span class="s2">&quot;/&quot;</span><span class="p">)[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">dest</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">dest_path</span><span class="p">,</span> <span class="n">name</span><span class="p">)</span>
    <span class="n">safe_mkdirs</span><span class="p">(</span><span class="n">dest_path</span><span class="p">)</span>
    <span class="n">response</span> <span class="o">=</span> <span class="n">urllib</span><span class="o">.</span><span class="n">request</span><span class="o">.</span><span class="n">urlopen</span><span class="p">(</span><span class="n">url</span><span class="p">)</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">dest</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fd</span><span class="p">:</span>
        <span class="n">shutil</span><span class="o">.</span><span class="n">copyfileobj</span><span class="p">(</span><span class="n">response</span><span class="p">,</span> <span class="n">fd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">dest</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.has_internet_connection" class="doc doc-heading">
<code class="highlight language-python"><span class="n">has_internet_connection</span><span class="p">(</span><span class="n">timeout</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>has_internet_connection Check if you are connected to the internet</p>
<p>Check if internet connection exists by pinging Google DNS server</p>
<p>Host: 8.8.8.8 (google-public-dns-a.google.com)
OpenPort: 53/tcp
Service: domain (DNS/TCP)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>timeout</code></td>
        <td><code>int</code></td>
        <td><p>Seconds to wait before giving up</p></td>
        <td><code>3</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>bool</code></td>
      <td><p>bool: True if connection is established, False if we are not connected to the internet</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">has_internet_connection</span><span class="p">(</span><span class="n">timeout</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;has_internet_connection Check if you are connected to the internet</span>

<span class="sd">    Check if internet connection exists by pinging Google DNS server</span>

<span class="sd">    Host: 8.8.8.8 (google-public-dns-a.google.com)</span>
<span class="sd">    OpenPort: 53/tcp</span>
<span class="sd">    Service: domain (DNS/TCP)</span>

<span class="sd">    Args:</span>
<span class="sd">        timeout (int): Seconds to wait before giving up</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if connection is established, False if we are not connected to the internet</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">host</span><span class="p">,</span> <span class="n">port</span> <span class="o">=</span> <span class="s2">&quot;8.8.8.8&quot;</span><span class="p">,</span> <span class="mi">53</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">socket</span><span class="o">.</span><span class="n">setdefaulttimeout</span><span class="p">(</span><span class="n">timeout</span><span class="p">)</span>
        <span class="n">socket</span><span class="o">.</span><span class="n">socket</span><span class="p">(</span><span class="n">socket</span><span class="o">.</span><span class="n">AF_INET</span><span class="p">,</span> <span class="n">socket</span><span class="o">.</span><span class="n">SOCK_STREAM</span><span class="p">)</span><span class="o">.</span><span class="n">connect</span><span class="p">((</span><span class="n">host</span><span class="p">,</span> <span class="n">port</span><span class="p">))</span>
        <span class="k">return</span> <span class="kc">True</span>
    <span class="k">except</span> <span class="n">socket</span><span class="o">.</span><span class="n">error</span> <span class="k">as</span> <span class="n">ex</span><span class="p">:</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">ex</span><span class="p">)</span>
        <span class="k">return</span> <span class="kc">False</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.is_file" class="doc doc-heading">
<code class="highlight language-python"><span class="n">is_file</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>is_file Check if the provided string is valid file in the system path</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>inp</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>A potential file or None</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[validators.utils.ValidationFailure, bool]</code></td>
      <td><p>types.ValidationResult: True if a valid file is provided, False if the string is not a url</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">is_file</span><span class="p">(</span><span class="s2">&quot;/bin/bash&quot;</span><span class="p">)</span>
<span class="kc">True</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">is_file</span><span class="p">(</span><span class="s2">&quot;/supercalifragilisticexpialidocious&quot;</span><span class="p">)</span>  <span class="c1"># This does not exist. I hope...</span>
<span class="kc">False</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">is_file</span><span class="p">(</span><span class="n">inp</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">types</span><span class="o">.</span><span class="n">ValidationResult</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;is_file Check if the provided string is valid file in the system path</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (Optional[str]): A potential file or None</span>

<span class="sd">    Returns:</span>
<span class="sd">        types.ValidationResult: True if a valid file is provided, False if the string is not a url</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; is_file(&quot;/bin/bash&quot;)</span>
<span class="sd">        True</span>
<span class="sd">        &gt;&gt;&gt; is_file(&quot;/supercalifragilisticexpialidocious&quot;)  # This does not exist. I hope...</span>
<span class="sd">        False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inp</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isfile</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.is_subpath" class="doc doc-heading">
<code class="highlight language-python"><span class="n">is_subpath</span><span class="p">(</span><span class="n">child</span><span class="p">,</span> <span class="n">parent</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>is_subpath Check if child path is a subpath of parent</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>child</code></td>
        <td><code>str</code></td>
        <td><p>Child path</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>parent</code></td>
        <td><code>str</code></td>
        <td><p>parent path</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>bool</code></td>
      <td><p>bool: True if child is a subpath of parent, false if not</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">is_subpath</span><span class="p">(</span><span class="s2">&quot;/usr/bin/Xorg&quot;</span><span class="p">,</span> <span class="s2">&quot;/usr&quot;</span><span class="p">)</span>
<span class="kc">True</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">is_subpath</span><span class="p">(</span><span class="n">child</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">parent</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;is_subpath Check if child path is a subpath of parent</span>

<span class="sd">    Args:</span>
<span class="sd">        child (str): Child path</span>
<span class="sd">        parent (str): parent path</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: True if child is a subpath of parent, false if not</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; is_subpath(&quot;/usr/bin/Xorg&quot;, &quot;/usr&quot;)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">parent</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">parent</span><span class="p">)</span>
    <span class="n">child</span> <span class="o">=</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">abspath</span><span class="p">(</span><span class="n">child</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cast</span><span class="p">(</span>
        <span class="nb">bool</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">commonpath</span><span class="p">([</span><span class="n">parent</span><span class="p">])</span> <span class="o">==</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">commonpath</span><span class="p">([</span><span class="n">parent</span><span class="p">,</span> <span class="n">child</span><span class="p">])</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.is_url" class="doc doc-heading">
<code class="highlight language-python"><span class="n">is_url</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>is_url Check if the provided string is a URL</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>inp</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>A potential link or None</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[validators.utils.ValidationFailure, bool]</code></td>
      <td><p>types.ValidationResult: True if a valid url is provided, False if the string is not a url</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">is_url</span><span class="p">(</span><span class="s2">&quot;Hello World&quot;</span><span class="p">)</span>
<span class="n">ValidationFailure</span><span class="p">(</span><span class="n">func</span><span class="o">=</span><span class="n">url</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;value&#39;</span><span class="p">:</span> <span class="s1">&#39;Hello World&#39;</span><span class="p">,</span> <span class="s1">&#39;public&#39;</span><span class="p">:</span> <span class="kc">False</span><span class="p">})</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">is_url</span><span class="p">(</span><span class="s2">&quot;http://google.com&quot;</span><span class="p">)</span>
<span class="kc">True</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">is_url</span><span class="p">(</span><span class="n">inp</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">types</span><span class="o">.</span><span class="n">ValidationResult</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;is_url Check if the provided string is a URL</span>

<span class="sd">    Args:</span>
<span class="sd">        inp (Optional[str]): A potential link or None</span>

<span class="sd">    Returns:</span>
<span class="sd">        types.ValidationResult: True if a valid url is provided, False if the string is not a url</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; is_url(&quot;Hello World&quot;)</span>
<span class="sd">        ValidationFailure(func=url, args={&#39;value&#39;: &#39;Hello World&#39;, &#39;public&#39;: False})</span>
<span class="sd">        &gt;&gt;&gt; is_url(&quot;http://google.com&quot;)</span>
<span class="sd">        True</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">inp</span><span class="p">:</span>
        <span class="k">return</span> <span class="kc">False</span>
    <span class="k">return</span> <span class="n">validators</span><span class="o">.</span><span class="n">url</span><span class="p">(</span><span class="n">inp</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.json_dump" class="doc doc-heading">
<code class="highlight language-python"><span class="n">json_dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>json_dump Save dict to a json file</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>data</code></td>
        <td><code>Dict[~K, ~V]</code></td>
        <td><p>Dict to save</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>fname</code></td>
        <td><code>str</code></td>
        <td><p>Output json file</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">json_dump</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">GenericDict</span><span class="p">,</span> <span class="n">fname</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;json_dump Save dict to a json file</span>

<span class="sd">    Args:</span>
<span class="sd">        data (types.GenericDict): Dict to save</span>
<span class="sd">        fname (str): Output json file</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fd</span><span class="p">:</span>
        <span class="n">json</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">fd</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.json_load" class="doc doc-heading">
<code class="highlight language-python"><span class="n">json_load</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>json_load Load dict from a json file</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>fname</code></td>
        <td><code>str</code></td>
        <td><p>Json file to load</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[~K, ~V]</code></td>
      <td><p>types.GenericDict: Dict of loaded data</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">json_load</span><span class="p">(</span><span class="n">fname</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">types</span><span class="o">.</span><span class="n">GenericDict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;json_load Load dict from a json file</span>

<span class="sd">    Args:</span>
<span class="sd">        fname (str): Json file to load</span>

<span class="sd">    Returns:</span>
<span class="sd">        types.GenericDict: Dict of loaded data</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fd</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">json</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">types</span><span class="o">.</span><span class="n">GenericDict</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.pickle_dump" class="doc doc-heading">
<code class="highlight language-python"><span class="n">pickle_dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>pickle_dump Save data to pickle file</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>data</code></td>
        <td><code>Any</code></td>
        <td><p>Data to save</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>fname</code></td>
        <td><code>str</code></td>
        <td><p>Output pickle file</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">pickle_dump</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">Any</span><span class="p">,</span> <span class="n">fname</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;pickle_dump Save data to pickle file</span>

<span class="sd">    Args:</span>
<span class="sd">        data (Any): Data to save</span>
<span class="sd">        fname (str): Output pickle file</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">&quot;wb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fd</span><span class="p">:</span>
        <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">fd</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.pickle_load" class="doc doc-heading">
<code class="highlight language-python"><span class="n">pickle_load</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>pickle_load Load data from pickle file</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>fname</code></td>
        <td><code>str</code></td>
        <td><p>file name of pickle file</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Any</code></td>
      <td><p>Any: Loaded data</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">pickle_load</span><span class="p">(</span><span class="n">fname</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;pickle_load Load data from pickle file</span>

<span class="sd">    Args:</span>
<span class="sd">        fname (str): file name of pickle file</span>

<span class="sd">    Returns:</span>
<span class="sd">        Any: Loaded data</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">&quot;rb&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fd</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">data</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.print_separator" class="doc doc-heading">
<code class="highlight language-python"><span class="n">print_separator</span><span class="p">(</span><span class="n">symbol</span><span class="o">=</span><span class="s1">&#39;*&#39;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">print_fn</span><span class="o">=&lt;</span><span class="n">built</span><span class="o">-</span><span class="ow">in</span> <span class="n">function</span> <span class="nb">print</span><span class="o">&gt;</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>print_separator Print a repeated symbol as a separator</p>
<hr />

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>symbol</code></td>
        <td><code>str</code></td>
        <td><p>Symbol to print</p></td>
        <td><code>&#39;*&#39;</code></td>
      </tr>
      <tr>
        <td><code>n</code></td>
        <td><code>int</code></td>
        <td><p>Number of times to print the symbol</p></td>
        <td><code>10</code></td>
      </tr>
      <tr>
        <td><code>print_fn</code></td>
        <td><code>Callable[[str], NoneType]</code></td>
        <td><p>Print function to use, e.g. print or logger.info</p></td>
        <td><code>&lt;built-in function print&gt;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">print_separator</span><span class="p">(</span><span class="n">symbol</span><span class="o">=</span><span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="o">--</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">print_separator</span><span class="p">(</span>
    <span class="n">symbol</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;*&quot;</span><span class="p">,</span> <span class="n">n</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">10</span><span class="p">,</span> <span class="n">print_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="nb">str</span><span class="p">],</span> <span class="kc">None</span><span class="p">]</span> <span class="o">=</span> <span class="nb">print</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;print_separator Print a repeated symbol as a separator</span>

<span class="sd">    *********************************************************</span>

<span class="sd">    Args:</span>
<span class="sd">        symbol (str): Symbol to print</span>
<span class="sd">        n (int): Number of times to print the symbol</span>
<span class="sd">        print_fn (Callable[[str], None]): Print function to use, e.g. print or logger.info</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; print_separator(symbol=&quot;-&quot;, n=2)</span>
<span class="sd">        --</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">print_fn</span><span class="p">(</span><span class="n">symbol</span> <span class="o">*</span> <span class="n">n</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.read_wav" class="doc doc-heading">
<code class="highlight language-python"><span class="n">read_wav</span><span class="p">(</span><span class="n">wav_sample</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>read_wav Reads a wav clip into a string and returns the hex string.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>wav_sample</code></td>
        <td><code>str</code></td>
        <td><p>Path to wav file</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>str</code></td>
      <td><p>A hex string with the audio information.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">read_wav</span><span class="p">(</span><span class="n">wav_sample</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">str</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;read_wav Reads a wav clip into a string and returns the hex string.</span>

<span class="sd">    Args:</span>
<span class="sd">        wav_sample (str): Path to wav file</span>

<span class="sd">    Returns:</span>
<span class="sd">        A hex string with the audio information.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">wav_sample</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">wav_fd</span><span class="p">:</span>
        <span class="n">clip</span> <span class="o">=</span> <span class="n">wav_fd</span><span class="o">.</span><span class="n">read</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">clip</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.run_cmd" class="doc doc-heading">
<code class="highlight language-python"><span class="n">run_cmd</span><span class="p">(</span><span class="n">command</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>run_cmd Run given shell command</p>
<pre><code>!!! args
    command (str): Shell command to run

!!! returns
    (int, str): Status code, stdout of shell command

!!! examples
    &gt;&gt;&gt; run_cmd("ls /")
    (0, 'bin
</code></pre>
<p>boot
dev
etc
home
init
lib
lib32
lib64
libx32
lost+found
media
mnt
opt
proc
root
run
sbin
snap
srv
sys
tmp
usr
var
')</p>

        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">run_cmd</span><span class="p">(</span><span class="n">command</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;run_cmd Run given shell command</span>

<span class="sd">    Args:</span>
<span class="sd">        command (str): Shell command to run</span>

<span class="sd">    Returns:</span>
<span class="sd">        (int, str): Status code, stdout of shell command</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; run_cmd(&quot;ls /&quot;)</span>
<span class="sd">        (0, &#39;bin\nboot\ndev\netc\nhome\ninit\nlib\nlib32\nlib64\nlibx32\nlost+found\nmedia\nmnt\nopt\nproc\nroot\nrun\nsbin\nsnap\nsrv\nsys\ntmp\nusr\nvar\n&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">command</span> <span class="o">=</span> <span class="sa">f</span><span class="s1">&#39;</span><span class="si">{</span><span class="n">os</span><span class="o">.</span><span class="n">getenv</span><span class="p">(</span><span class="s2">&quot;SHELL&quot;</span><span class="p">)</span><span class="si">}</span><span class="s1"> -c &quot;</span><span class="si">{</span><span class="n">command</span><span class="si">}</span><span class="s1">&quot;&#39;</span>
    <span class="n">pipe</span> <span class="o">=</span> <span class="n">subprocess</span><span class="o">.</span><span class="n">Popen</span><span class="p">(</span>
        <span class="n">command</span><span class="p">,</span> <span class="n">shell</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">stdout</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">PIPE</span><span class="p">,</span> <span class="n">stderr</span><span class="o">=</span><span class="n">subprocess</span><span class="o">.</span><span class="n">STDOUT</span>
    <span class="p">)</span>

    <span class="n">stdout</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span>
    <span class="k">if</span> <span class="n">pipe</span><span class="o">.</span><span class="n">stdout</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">stdout</span> <span class="o">=</span> <span class="s2">&quot;&quot;</span><span class="o">.</span><span class="n">join</span><span class="p">(</span>
            <span class="p">[</span><span class="n">line</span><span class="o">.</span><span class="n">decode</span><span class="p">(</span><span class="s2">&quot;utf-8&quot;</span><span class="p">)</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="nb">iter</span><span class="p">(</span><span class="n">pipe</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">readline</span><span class="p">,</span> <span class="sa">b</span><span class="s2">&quot;&quot;</span><span class="p">)]</span>
        <span class="p">)</span>
        <span class="n">pipe</span><span class="o">.</span><span class="n">stdout</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
    <span class="n">returncode</span> <span class="o">=</span> <span class="n">pipe</span><span class="o">.</span><span class="n">wait</span><span class="p">()</span>
    <span class="k">return</span> <span class="n">returncode</span><span class="p">,</span> <span class="n">stdout</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.run_cmd_silent" class="doc doc-heading">
<code class="highlight language-python"><span class="n">run_cmd_silent</span><span class="p">(</span><span class="n">command</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>run_cmd_silent Run command without printing to console</p>
<pre><code>!!! args
    command (str): Shell command to run

!!! returns
    (int, str): Status code, stdout of shell command

!!! examples
    &gt;&gt;&gt; run_cmd("ls /")
    (0, 'bin
</code></pre>
<p>boot
dev
etc
home
init
lib
lib32
lib64
libx32
lost+found
media
mnt
opt
proc
root
run
sbin
snap
srv
sys
tmp
usr
var
')</p>

        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">run_cmd_silent</span><span class="p">(</span><span class="n">command</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;run_cmd_silent Run command without printing to console</span>

<span class="sd">    Args:</span>
<span class="sd">        command (str): Shell command to run</span>

<span class="sd">    Returns:</span>
<span class="sd">        (int, str): Status code, stdout of shell command</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; run_cmd(&quot;ls /&quot;)</span>
<span class="sd">        (0, &#39;bin\nboot\ndev\netc\nhome\ninit\nlib\nlib32\nlib64\nlibx32\nlost+found\nmedia\nmnt\nopt\nproc\nroot\nrun\nsbin\nsnap\nsrv\nsys\ntmp\nusr\nvar\n&#39;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">Tuple</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">suppress_print</span><span class="p">(</span><span class="n">run_cmd</span><span class="p">)(</span><span class="n">command</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.safe_mkdirs" class="doc doc-heading">
<code class="highlight language-python"><span class="n">safe_mkdirs</span><span class="p">(</span><span class="n">path</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Makes recursively all the directories in input path</p>
<p>Utility function similar to mkdir -p. Makes directories recursively, if given path does not exist</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>path</code></td>
        <td><code>str</code></td>
        <td><p>Path to mkdir -p</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">safe_mkdirs</span><span class="p">(</span><span class="s2">&quot;super/cali/fragi/listic/expi/ali/docious&quot;</span><span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">safe_mkdirs</span><span class="p">(</span><span class="n">path</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Makes recursively all the directories in input path</span>

<span class="sd">    Utility function similar to mkdir -p. Makes directories recursively, if given path does not exist</span>

<span class="sd">    Args:</span>
<span class="sd">        path (str): Path to mkdir -p</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; safe_mkdirs(&quot;super/cali/fragi/listic/expi/ali/docious&quot;)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">os</span><span class="o">.</span><span class="n">makedirs</span><span class="p">(</span><span class="n">path</span><span class="p">)</span>
        <span class="k">except</span> <span class="ne">Exception</span> <span class="k">as</span> <span class="n">e</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="n">e</span><span class="p">)</span>
            <span class="k">raise</span> <span class="ne">IOError</span><span class="p">((</span><span class="sa">f</span><span class="s2">&quot;Failed to create recursive directories: </span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.suppress_print" class="doc doc-heading">
<code class="highlight language-python"><span class="n">suppress_print</span><span class="p">(</span><span class="n">func</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>suppress_print Decorator to supress stdout of decorated function</p>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="nd">@slp</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">system</span><span class="o">.</span><span class="n">timethis</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">very_verbose_function</span><span class="p">(</span><span class="o">...</span><span class="p">):</span> <span class="o">...</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">suppress_print</span><span class="p">(</span><span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;suppress_print Decorator to supress stdout of decorated function</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; @slp.util.system.timethis</span>
<span class="sd">        &gt;&gt;&gt; def very_verbose_function(...): ...</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">func_wrapper</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">T</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Inner function for decorator closure&quot;&quot;&quot;</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s2">&quot;/dev/null&quot;</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span><span class="p">:</span>
            <span class="n">ret</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="n">sys</span><span class="o">.</span><span class="n">stdout</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">__stdout__</span>
        <span class="k">return</span> <span class="n">ret</span>

    <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">Callable</span><span class="p">,</span> <span class="n">func_wrapper</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.timethis" class="doc doc-heading">
<code class="highlight language-python"><span class="n">timethis</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Decorator to measure the time it takes for a function to complete</p>

<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="nd">@slp</span><span class="o">.</span><span class="n">util</span><span class="o">.</span><span class="n">system</span><span class="o">.</span><span class="n">timethis</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">time_consuming_function</span><span class="p">(</span><span class="o">...</span><span class="p">):</span> <span class="o">...</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">timethis</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Decorator to measure the time it takes for a function to complete</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; @slp.util.system.timethis</span>
<span class="sd">        &gt;&gt;&gt; def time_consuming_function(...): ...</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">def</span> <span class="nf">timethis_inner</span><span class="p">(</span><span class="n">func</span><span class="p">:</span> <span class="n">Callable</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Callable</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Inner function for decorator closure&quot;&quot;&quot;</span>

        <span class="nd">@functools</span><span class="o">.</span><span class="n">wraps</span><span class="p">(</span><span class="n">func</span><span class="p">)</span>
        <span class="k">def</span> <span class="nf">timed</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">T</span><span class="p">):</span>
            <span class="sd">&quot;&quot;&quot;Inner function for decorator closure&quot;&quot;&quot;</span>

            <span class="n">ts</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">result</span> <span class="o">=</span> <span class="n">func</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="n">te</span> <span class="o">=</span> <span class="n">time</span><span class="o">.</span><span class="n">time</span><span class="p">()</span>
            <span class="n">elapsed</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">te</span> <span class="o">-</span> <span class="n">ts</span><span class="si">}</span><span class="s2">&quot;</span>
            <span class="k">if</span> <span class="n">method</span><span class="p">:</span>

                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;BENCHMARK: </span><span class="si">{cls}</span><span class="s2">.</span><span class="si">{f}</span><span class="s2">(*</span><span class="si">{a}</span><span class="s2">, **</span><span class="si">{kw}</span><span class="s2">) took: </span><span class="si">{t}</span><span class="s2"> sec&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">f</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="bp">cls</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">a</span><span class="o">=</span><span class="n">args</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">kw</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">elapsed</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
                    <span class="s2">&quot;BENCHMARK: </span><span class="si">{f}</span><span class="s2">(*</span><span class="si">{a}</span><span class="s2">, **</span><span class="si">{kw}</span><span class="s2">) took: </span><span class="si">{t}</span><span class="s2"> sec&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
                        <span class="n">f</span><span class="o">=</span><span class="n">func</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="n">a</span><span class="o">=</span><span class="n">args</span><span class="p">,</span> <span class="n">kw</span><span class="o">=</span><span class="n">kwargs</span><span class="p">,</span> <span class="n">t</span><span class="o">=</span><span class="n">elapsed</span>
                    <span class="p">)</span>
                <span class="p">)</span>
            <span class="k">return</span> <span class="n">result</span>

        <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">Callable</span><span class="p">,</span> <span class="n">timed</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">timethis_inner</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.write_wav" class="doc doc-heading">
<code class="highlight language-python"><span class="n">write_wav</span><span class="p">(</span><span class="n">byte_str</span><span class="p">,</span> <span class="n">wav_file</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>write_wav Write a hex string into a wav file</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>byte_str</code></td>
        <td><code>str</code></td>
        <td><p>The hex string containing the audio data</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>wav_file</code></td>
        <td><code>str</code></td>
        <td><p>The output wav file</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">write_wav</span><span class="p">(</span><span class="n">byte_str</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">wav_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;write_wav Write a hex string into a wav file</span>

<span class="sd">    Args:</span>
<span class="sd">        byte_str (str): The hex string containing the audio data</span>
<span class="sd">        wav_file (str): The output wav file</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">wav_file</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fd</span><span class="p">:</span>
        <span class="n">fd</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">byte_str</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.yaml_dump" class="doc doc-heading">
<code class="highlight language-python"><span class="n">yaml_dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">fname</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>yaml_dump Save dict to a yaml file</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>data</code></td>
        <td><code>Dict[~K, ~V]</code></td>
        <td><p>Dict to save</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>fname</code></td>
        <td><code>str</code></td>
        <td><p>Output json file</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">yaml_dump</span><span class="p">(</span><span class="n">data</span><span class="p">:</span> <span class="n">types</span><span class="o">.</span><span class="n">GenericDict</span><span class="p">,</span> <span class="n">fname</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;yaml_dump Save dict to a yaml file</span>

<span class="sd">    Args:</span>
<span class="sd">        data (types.GenericDict): Dict to save</span>
<span class="sd">        fname (str): Output json file</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">&quot;w&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fd</span><span class="p">:</span>
        <span class="n">yaml</span><span class="o">.</span><span class="n">dump</span><span class="p">(</span><span class="n">data</span><span class="p">,</span> <span class="n">fd</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.util.system.yaml_load" class="doc doc-heading">
<code class="highlight language-python"><span class="n">yaml_load</span><span class="p">(</span><span class="n">fname</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>yaml_load Load dict from a yaml file</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>fname</code></td>
        <td><code>str</code></td>
        <td><p>Json file to load</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[~K, ~V]</code></td>
      <td><p>types.GenericDict: Dict of loaded data</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/util/system.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">yaml_load</span><span class="p">(</span><span class="n">fname</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">types</span><span class="o">.</span><span class="n">GenericDict</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;yaml_load Load dict from a yaml file</span>

<span class="sd">    Args:</span>
<span class="sd">        fname (str): Json file to load</span>

<span class="sd">    Returns:</span>
<span class="sd">        types.GenericDict: Dict of loaded data</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fname</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">fd</span><span class="p">:</span>
        <span class="n">data</span> <span class="o">=</span> <span class="n">yaml</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">fd</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">cast</span><span class="p">(</span><span class="n">types</span><span class="o">.</span><span class="n">GenericDict</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.util.types"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">




















  <div class="doc doc-object doc-function">



<h2 id="slp.util.types.dir_path" class="doc doc-heading">
<code class="highlight language-python"><span class="n">dir_path</span><span class="p">(</span><span class="n">path</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>dir_path Type to use when parsing a path in argparse arguments</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>path</code></td>
        <td><code>str</code></td>
        <td><p>User provided path</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Exceptions:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>argparse.ArgumentTypeError</code></td>
        <td><p>Path does not exists, so argparse fails</p></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>str</code></td>
      <td><p>User provided path</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="kn">from</span> <span class="nn">slp.util.types</span> <span class="kn">import</span> <span class="n">dir_path</span>
<span class="o">&gt;&gt;&gt;</span> <span class="kn">import</span> <span class="nn">argparse</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">parser</span> <span class="o">=</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentParser</span><span class="p">(</span><span class="s2">&quot;My cool model&quot;</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">parser</span><span class="o">.</span><span class="n">add_argument</span><span class="p">(</span><span class="s2">&quot;--config&quot;</span><span class="p">,</span> <span class="nb">type</span><span class="o">=</span><span class="n">dir_path</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">parser</span><span class="o">.</span><span class="n">parse_args</span><span class="p">(</span><span class="n">args</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;--config&quot;</span><span class="p">,</span> <span class="s2">&quot;my_random_config_that_does_not_exist.yaml&quot;</span><span class="p">])</span>
<span class="n">Traceback</span> <span class="p">(</span><span class="n">most</span> <span class="n">recent</span> <span class="n">call</span> <span class="n">last</span><span class="p">):</span>
<span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentTypeError</span><span class="p">:</span> <span class="n">User</span> <span class="n">provided</span> <span class="n">path</span> <span class="s1">&#39;my_random_config_that_does_not_exist.yaml&#39;</span> <span class="n">does</span> <span class="ow">not</span> <span class="n">exist</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/util/types.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">dir_path</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;dir_path Type to use when parsing a path in argparse arguments</span>


<span class="sd">    Args:</span>
<span class="sd">        path (str): User provided path</span>

<span class="sd">    Raises:</span>
<span class="sd">        argparse.ArgumentTypeError: Path does not exists, so argparse fails</span>

<span class="sd">    Returns:</span>
<span class="sd">        str: User provided path</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; from slp.util.types import dir_path</span>
<span class="sd">        &gt;&gt;&gt; import argparse</span>
<span class="sd">        &gt;&gt;&gt; parser = argparse.ArgumentParser(&quot;My cool model&quot;)</span>
<span class="sd">        &gt;&gt;&gt; parser.add_argument(&quot;--config&quot;, type=dir_path)</span>
<span class="sd">        &gt;&gt;&gt; parser.parse_args(args=[&quot;--config&quot;, &quot;my_random_config_that_does_not_exist.yaml&quot;])</span>
<span class="sd">        Traceback (most recent call last):</span>
<span class="sd">        argparse.ArgumentTypeError: User provided path &#39;my_random_config_that_does_not_exist.yaml&#39; does not exist</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">isdir</span><span class="p">(</span><span class="n">path</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">path</span>

    <span class="k">raise</span> <span class="n">argparse</span><span class="o">.</span><span class="n">ArgumentTypeError</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;User provided path &#39;</span><span class="si">{</span><span class="n">path</span><span class="si">}</span><span class="s2">&#39; does not exist&quot;</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="../utils/" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              SLP utility functions
            </div>
          </div>
        </a>
      
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../assets/javascripts/workers/search.fb4a9340.min.js", "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.5cf3e710.min.js"></script>
      
        <script src="../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>