
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.0.5">
    
    
      
        <title>Getting started - slp Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.77f3fd56.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.7fa14f5b.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#getting-started" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="slp Documentation" class="md-header__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            slp Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Getting started
            
          </span>
        </div>
      </div>
    </div>
    <div class="md-header__options">
      
    </div>
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="slp Documentation" class="md-nav__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    slp Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        slp
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Getting started
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Getting started
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#setup" class="md-nav__link">
    Setup
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-module" class="md-nav__link">
    Data Module
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#defining-the-model" class="md-nav__link">
    Defining the model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-and-debugging" class="md-nav__link">
    Training and Debugging
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix-command-line-arguments" class="md-nav__link">
    Appendix. Command Line arguments
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../tuning/" class="md-nav__link">
        Hyperparameter tuning
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../config/" class="md-nav__link">
        Configuration
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../data-utils/" class="md-nav__link">
        Data manipulation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../plbind/" class="md-nav__link">
        Pytorch Lightning Bindings
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../modules/" class="md-nav__link">
        Generic Modules
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../multimodal/" class="md-nav__link">
        Multimodal Modules
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        SLP utility functions
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../apiref/" class="md-nav__link">
        API reference
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#setup" class="md-nav__link">
    Setup
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#data-module" class="md-nav__link">
    Data Module
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#defining-the-model" class="md-nav__link">
    Defining the model
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#training-and-debugging" class="md-nav__link">
    Training and Debugging
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix-command-line-arguments" class="md-nav__link">
    Appendix. Command Line arguments
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="getting-started">Getting started</h1>
<p>For a quick hands-on we are going to go through creating a MNIST classifier step by step (<code>examples/mnist.py</code>).</p>
<p>First we can see the model definition of a simple CNN classifier:</p>
<pre><code class="language-python">class Net(nn.Module):
    def __init__(self, intermediate_hidden=50):
        super(Net, self).__init__()
        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)
        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)
        self.conv2_drop = nn.Dropout2d()
        self.fc1 = nn.Linear(320, intermediate_hidden)
        self.fc2 = nn.Linear(intermediate_hidden, 10)

    def forward(self, x):
        x = F.relu(F.max_pool2d(self.conv1(x), 2))
        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))
        x = x.view(-1, 320)
        x = F.relu(self.fc1(x))
        x = F.dropout(x, training=self.training)
        x = self.fc2(x)
        return x
</code></pre>
<p>The <code>get_data</code> function downloads MNIST and performs the preprocessing:</p>
<pre><code class="language-python">def get_data():
    data_transform = Compose([ToTensor(), Normalize((0.1307,), (0.3081,))])
    train = MNIST(download=True, root=&quot;.&quot;, transform=data_transform, train=True)

    val = MNIST(download=False, root=&quot;.&quot;, transform=data_transform, train=False)
    return train, val
</code></pre>
<p>Finally the <code>get_parser</code> function creates a CLI parser for the model arguments. Note the destination variable <code>model.intermediate_hidden</code> (important for later):</p>
<pre><code class="language-python">def get_parser():
    parser = ArgumentParser(&quot;MNIST classification example&quot;)
    parser.add_argument(
        &quot;--hidden&quot;,
        dest=&quot;model.intermediate_hidden&quot;,
        type=int,
        default=12,
        help=&quot;Intermediate hidden layers for linear module&quot;,
    )
    return parser
</code></pre>
<h2 id="setup">Setup</h2>
<p>You would create these functions whether you use slp or not. The interesting part is in the main function.</p>
<p>Here we first need to perform some setup for logging, configuration parsing and seeding. Let's go through this step-by-step:</p>
<p>First we create the CLI args parser. <code>make_cli_parser</code> takes the parser we defined in get_parser() and extends it with generic arguments for the data, optimizers, learning rate scheduling, training and experiment tracking.</p>
<p>Run <code>python mnist.py --help</code> for a full list of arguments, or go down in the Appendix. 
The arguments have a detailed description and most of them default to <code>None</code> so they will not be used.</p>
<pre><code class="language-python">parser = get_parser()
parser = make_cli_parser(parser, PLDataModuleFromDatasets)
</code></pre>
<p>Next we parse the configuration. Here we need to provide the parser and (optionally) a YAML configuration file <code>python mnist.py --config my-config.yaml</code>.</p>
<p>The configuration file should have the following format:</p>
<pre><code class="language-yaml">model:
  intermediate_hidden: 100
optimizer: Adam
optim:
  lr: 1e-3
lr_scheduler: true  # ReduceLROnPlateau
lr_schedule:
  factor: 2
data:
  batch_size: 128
  batch_size_eval: 256
</code></pre>
<p>Note that this format, closely follows the <code>dest</code> values we configure in the command line args (e.g. <code>OPTIM.LR</code>, <code>LR_SCHEDULE.FACTOR</code>), namely the dots. form a hierarchy.</p>
<p>This way we can use <code>parse_config</code> to merge the values in the configuration file and the CLI args.</p>
<p>The precedence is as follows:</p>
<pre><code>default CLI args &lt; config file values &lt; user provided CLI args
</code></pre>
<p>So if we call the script with <code>--lr 1e-4</code> this value will overwrite the value in the configuration file.
If a value is not specified in the configuration file, the default value we specified in argparse will be set.
If a value is not specified in any of these places, sane defaults will be used.</p>
<pre><code class="language-python">config = parse_config(parser, parser.parse_args().config)

if config.trainer.experiment_name == &quot;experiment&quot;:
    config.trainer.experiment_name = &quot;mnist-classification&quot;
</code></pre>
<p>Next, we configure logging. This call configures <code>loguru</code> to intercept all logs and print the both to stdout and a log file.
The log file name will depend on the experiment name we provided and <code>datetime.now()</code>, to avoid overwriting previous runs (e.g. <code>mnist-classification.20210302-134714.log</code>).</p>
<pre><code class="language-python">configure_logging(f&quot;logs/{config.trainer.experiment_name}&quot;)
</code></pre>
<p>Finally, we make the run deterministic (<code>--seed</code>)</p>
<pre><code class="language-python">if config.seed is not None:
    logger.info(&quot;Seeding everything with seed={seed}&quot;)
    pl.utilities.seed.seed_everything(seed=config.seed)
</code></pre>
<h2 id="data-module">Data Module</h2>
<p>Here we download the train and test datasets and define the <code>LightningDataModule</code> that will be used in this experiment.</p>
<p>The <code>LightningDataModule</code> is the preferred way to consume datasets in pytorch lightning, and <code>PLDataModuleFromDatasets</code> abstracts the boilerplate of constructing and configuring the <code>DataLoaders</code>, splitting data etc.</p>
<p><strong>Note</strong>: <code>PLDataModuleFromDatasets</code> expects three <code>torch.utils.data.Datasets</code> as input, train, val and test. Val and test are optional. If any of the validation or tests sets are not provided, <code>PLDataModuleFromDatasets</code> will create a split using 20% of the train set by default (see <code>--val-percent</code>, <code>--test-percent</code>).</p>
<pre><code class="language-python">train, test = get_data()

ldm = PLDataModuleFromDatasets(train, test=test, seed=config.seed, **config.data)  # Note we pass **config.data, because config is in a hierarchy.
</code></pre>
<h2 id="defining-the-model">Defining the model</h2>
<p>Next we define the model, optimizer, criterion and learning rate scheduler (pretty standard).</p>
<pre><code class="language-python">model = Net(**config.model)

optimizer = getattr(optim, config.optimizer)(model.parameters(), **config.optim)
criterion = nn.CrossEntropyLoss()

lr_scheduler = None
if config.lr_scheduler:
    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
        optimizer, **config.lr_schedule
    )
</code></pre>
<p>And the <code>LightningModule</code> that will be used for training. This module takes care of defining the train and validation steps, computing and logging metrics etc.</p>
<p>Note that pytorch lightning by default expects softmaxed outputs in the predefined metrics, so we wrap the metric with <code>FromLogits</code> to use crossentropy loss.</p>
<pre><code class="language-python">lm = PLModule(
    model,
    optimizer,
    criterion,
    lr_scheduler=lr_scheduler,
    metrics={&quot;acc&quot;: FromLogits(pl.metrics.classification.Accuracy())},
    hparams=config,  # We pass this so that configuration will be logged in wandb
)
</code></pre>
<h2 id="training-and-debugging">Training and Debugging</h2>
<p>Finally, we have the option to run a full training and testing phase of the model, or run a quick debug execution.</p>
<p>If we need to debug, we can pass <code>--debug</code> flag, and the trainer will run a full training, validation run on 5 batches.
It will also try to overfit the model on 5 batches to verify that gradients flow.</p>
<pre><code class="language-python"># Run debugging session or fit &amp; test the model ############
if config.debug:
    logger.info(&quot;Running in debug mode: Fast run on 5 batches&quot;)
    trainer = make_trainer(fast_dev_run=5)
    trainer.fit(lm, datamodule=ldm)

    logger.info(&quot;Running in debug mode: Overfitting 5 batches&quot;)
    trainer = make_trainer(overfit_batches=5)
    trainer.fit(lm, datamodule=ldm)
</code></pre>
<p>If we run in normal mode, we fit on train / val sets and evaluate the best model on the test set. The best model is selected as the model with the smallest validation loss.</p>
<p>Training will run with early stopping, the best 3 checkpoints will be saved, all the jazz.</p>
<p>Note <code>watch_model</code> tells wandb to track weight norms and gradients for further inspection.</p>
<pre><code class="language-python">else:
    trainer = make_trainer(**config.trainer)
    watch_model(trainer, model)

    trainer.fit(lm, datamodule=ldm)

    trainer.test(ckpt_path=&quot;best&quot;, test_dataloaders=ldm.test_dataloader())

    logger.info(&quot;Run finished. Uploading files to wandb...&quot;)
</code></pre>
<p>Sure, most of this goodness comes from the awesome team in pytorch lightning. But what we do here is we abstract the boilerplate and the large learning curve, without making sacrifices in the features.</p>
<p>For example, you can play spot the differences between <code>examples/mnist.py</code>, which performs digit classification and <code>examples/smt_bert.py</code> which finetunes BERT for sentiment classification on SST-2.</p>
<p>The classes we use change, but the way they are called, the structure and features remains the same.</p>
<pre><code class="language-python"># smt_bert.py
...
if __name__ == &quot;__main__&quot;:
    parser = get_parser()
    parser = make_cli_parser(parser, PLDataModuleFromCorpus)

    args = parser.parse_args()
    config_file = args.config

    config = parse_config(parser, config_file)
    # Set these by default.
    config.hugging_face_model = config.data.tokenizer
    config.data.add_special_tokens = True
    config.data.lower = &quot;uncased&quot; in config.hugging_face_model

    if config.trainer.experiment_name == &quot;experiment&quot;:
        config.trainer.experiment_name = &quot;finetune-bert-smt&quot;

    configure_logging(f&quot;logs/{config.trainer.experiment_name}&quot;)

    if config.seed is not None:
        logger.info(&quot;Seeding everything with seed={seed}&quot;)
        pl.utilities.seed.seed_everything(seed=config.seed)

    (
        raw_train,
        labels_train,
        raw_dev,
        labels_dev,
        raw_test,
        labels_test,
        num_labels,
    ) = get_data(config)

    ldm = PLDataModuleFromCorpus(
        raw_train,
        labels_train,
        val=raw_dev,
        val_labels=labels_dev,
        test=raw_test,
        test_labels=labels_test,
        collate_fn=collate_fn,
        **config.data,
    )

    model = BertForSequenceClassification.from_pretrained(
        config.hugging_face_model, num_labels=num_labels
    )

    logger.info(model)

    # Leave this hardcoded for now.
    optimizer = AdamW([p for p in model.parameters() if p.requires_grad], lr=1e-5)
    criterion = nn.CrossEntropyLoss()

    lm = BertPLModule(
        model,
        optimizer,
        criterion,
        metrics={&quot;acc&quot;: FromLogits(pl.metrics.classification.Accuracy())},
    )

    trainer = make_trainer(**config.trainer)
    watch_model(trainer, model)

    trainer.fit(lm, datamodule=ldm)

    trainer.test(ckpt_path=&quot;best&quot;, test_dataloaders=ldm.test_dataloader())

</code></pre>
<h2 id="appendix-command-line-arguments">Appendix. Command Line arguments</h2>
<pre><code>usage: mnist.py [-h] [--hidden MODEL.INTERMEDIATE_HIDDEN]
                [--optimizer {Adam,AdamW,SGD,Adadelta,Adagrad,Adamax,ASGD,RMSprop}] [--lr OPTIM.LR]
                [--weight-decay OPTIM.WEIGHT_DECAY] [--lr-scheduler]
                [--lr-factor LR_SCHEDULE.FACTOR] [--lr-patience LR_SCHEDULE.PATIENCE]
                [--lr-cooldown LR_SCHEDULE.COOLDOWN] [--min-lr LR_SCHEDULE.MIN_LR] [--seed SEED]
                [--config CONFIG] [--experiment-name TRAINER.EXPERIMENT_NAME]
                [--run-id TRAINER.RUN_ID] [--experiment-group TRAINER.EXPERIMENT_GROUP]
                [--experiments-folder TRAINER.EXPERIMENTS_FOLDER] [--save-top-k TRAINER.SAVE_TOP_K]
                [--patience TRAINER.PATIENCE] [--wandb-project TRAINER.WANDB_PROJECT]
                [--tags [TRAINER.TAGS [TRAINER.TAGS ...]]] [--stochastic_weight_avg]
                [--gpus TRAINER.GPUS] [--val-interval TRAINER.CHECK_VAL_EVERY_N_EPOCH]
                [--clip-grad-norm TRAINER.GRADIENT_CLIP_VAL] [--epochs TRAINER.MAX_EPOCHS]
                [--steps TRAINER.MAX_STEPS] [--tbtt_steps TRAINER.TRUNCATED_BPTT_STEPS] [--debug]
                [--val-percent DATA.VAL_PERCENT] [--test-percent DATA.TEST_PERCENT]
                [--bsz DATA.BATCH_SIZE] [--bsz-eval DATA.BATCH_SIZE_EVAL]
                [--num-workers DATA.NUM_WORKERS] [--pin-memory] [--drop-last] [--shuffle-eval]

optional arguments:
  -h, --help            show this help message and exit
  --hidden MODEL.INTERMEDIATE_HIDDEN
                        Intermediate hidden layers for linear module
  --optimizer {Adam,AdamW,SGD,Adadelta,Adagrad,Adamax,ASGD,RMSprop}
                        Which optimizer to use
  --lr OPTIM.LR         Learning rate
  --weight-decay OPTIM.WEIGHT_DECAY
                        Learning rate
  --lr-scheduler        Use learning rate scheduling. Currently only ReduceLROnPlateau is supported
                        out of the box
  --lr-factor LR_SCHEDULE.FACTOR
                        Multiplicative factor by which LR is reduced. Used if --lr-scheduler is
                        provided.
  --lr-patience LR_SCHEDULE.PATIENCE
                        Number of epochs with no improvement after which learning rate will be
                        reduced. Used if --lr-scheduler is provided.
  --lr-cooldown LR_SCHEDULE.COOLDOWN
                        Number of epochs to wait before resuming normal operation after lr has been
                        reduced. Used if --lr-scheduler is provided.
  --min-lr LR_SCHEDULE.MIN_LR
                        Minimum lr for LR scheduling. Used if --lr-scheduler is provided.
  --seed SEED           Seed for reproducibility
  --config CONFIG       Path to YAML configuration file
  --experiment-name TRAINER.EXPERIMENT_NAME
                        Name of the running experiment
  --run-id TRAINER.RUN_ID
                        Unique identifier for the current run. If not provided it is inferred from
                        datetime.now()
  --experiment-group TRAINER.EXPERIMENT_GROUP
                        Group of current experiment. Useful when evaluating for different seeds /
                        cross-validation etc.
  --experiments-folder TRAINER.EXPERIMENTS_FOLDER
                        Top-level folder where experiment results &amp; checkpoints are saved
  --save-top-k TRAINER.SAVE_TOP_K
                        Save checkpoints for top k models
  --patience TRAINER.PATIENCE
                        Number of epochs to wait before early stopping
  --wandb-project TRAINER.WANDB_PROJECT
                        Wandb project under which results are saved
  --tags [TRAINER.TAGS [TRAINER.TAGS ...]]
                        Tags for current run to make results searchable.
  --stochastic_weight_avg
                        Use Stochastic weight averaging.
  --gpus TRAINER.GPUS   Number of GPUs to use
  --val-interval TRAINER.CHECK_VAL_EVERY_N_EPOCH
                        Run validation every n epochs
  --clip-grad-norm TRAINER.GRADIENT_CLIP_VAL
                        Clip gradients with ||grad(w)|| &gt;= args.clip_grad_norm
  --epochs TRAINER.MAX_EPOCHS
                        Maximum number of training epochs
  --steps TRAINER.MAX_STEPS
                        Maximum number of training steps
  --tbtt_steps TRAINER.TRUNCATED_BPTT_STEPS
                        Truncated Back-propagation-through-time steps.
  --debug               If true, we run a full run on a small subset of the input data and overfit
                        10 training batches
  --val-percent DATA.VAL_PERCENT
                        Percent of validation data to be randomly split from the training set, if
                        no validation set is provided
  --test-percent DATA.TEST_PERCENT
                        Percent of test data to be randomly split from the training set, if no test
                        set is provided
  --bsz DATA.BATCH_SIZE
                        Training batch size
  --bsz-eval DATA.BATCH_SIZE_EVAL
                        Evaluation batch size
  --num-workers DATA.NUM_WORKERS
                        Number of workers to be used in the DataLoader
  --pin-memory          Pin data to GPU memory for faster data loading
  --drop-last           Drop last incomplete batch
  --shuffle-eval        Shuffle val &amp; test sets
</code></pre>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href=".." class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              slp
            </div>
          </div>
        </a>
      
      
        <a href="../tuning/" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Hyperparameter tuning
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../assets/javascripts/workers/search.fb4a9340.min.js", "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.5cf3e710.min.js"></script>
      
        <script src="../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>