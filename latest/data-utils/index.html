
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.0.5">
    
    
      
        <title>Data manipulation - slp Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.77f3fd56.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.7fa14f5b.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#data-manipulation" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="slp Documentation" class="md-header__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            slp Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Data manipulation
            
          </span>
        </div>
      </div>
    </div>
    <div class="md-header__options">
      
    </div>
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="slp Documentation" class="md-nav__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    slp Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        slp
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../get-started/" class="md-nav__link">
        Getting started
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../tuning/" class="md-nav__link">
        Hyperparameter tuning
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../config/" class="md-nav__link">
        Configuration
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Data manipulation
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Data manipulation
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators" class="md-nav__link">
    slp.data.collators
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator" class="md-nav__link">
    MultimodalSequenceClassificationCollator
  </a>
  
    <nav class="md-nav" aria-label="MultimodalSequenceClassificationCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator" class="md-nav__link">
    Seq2SeqCollator
  </a>
  
    <nav class="md-nav" aria-label="Seq2SeqCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator" class="md-nav__link">
    SequenceClassificationCollator
  </a>
  
    <nav class="md-nav" aria-label="SequenceClassificationCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus" class="md-nav__link">
    slp.data.corpus
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader" class="md-nav__link">
    EmbeddingsLoader
  </a>
  
    <nav class="md-nav" aria-label="EmbeddingsLoader">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.augment_embeddings" class="md-nav__link">
    augment_embeddings()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.in_accepted_vocab" class="md-nav__link">
    in_accepted_vocab()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.load" class="md-nav__link">
    load()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus" class="md-nav__link">
    HfCorpus
  </a>
  
    <nav class="md-nav" aria-label="HfCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus" class="md-nav__link">
    TokenizedCorpus
  </a>
  
    <nav class="md-nav" aria-label="TokenizedCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus" class="md-nav__link">
    WordCorpus
  </a>
  
    <nav class="md-nav" aria-label="WordCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.create_vocab" class="md-nav__link">
    create_vocab()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets" class="md-nav__link">
    slp.data.datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset" class="md-nav__link">
    CorpusDataset
  </a>
  
    <nav class="md-nav" aria-label="CorpusDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.map" class="md-nav__link">
    map()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset" class="md-nav__link">
    CorpusLMDataset
  </a>
  
    <nav class="md-nav" aria-label="CorpusLMDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.map" class="md-nav__link">
    map()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms" class="md-nav__link">
    slp.data.transforms
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer" class="md-nav__link">
    HuggingFaceTokenizer
  </a>
  
    <nav class="md-nav" aria-label="HuggingFaceTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.detokenize" class="md-nav__link">
    detokenize()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken" class="md-nav__link">
    ReplaceUnknownToken
  </a>
  
    <nav class="md-nav" aria-label="ReplaceUnknownToken">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer" class="md-nav__link">
    SentencepieceTokenizer
  </a>
  
    <nav class="md-nav" aria-label="SentencepieceTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer" class="md-nav__link">
    SpacyTokenizer
  </a>
  
    <nav class="md-nav" aria-label="SpacyTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.get_nlp" class="md-nav__link">
    get_nlp()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor" class="md-nav__link">
    ToTensor
  </a>
  
    <nav class="md-nav" aria-label="ToTensor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds" class="md-nav__link">
    ToTokenIds
  </a>
  
    <nav class="md-nav" aria-label="ToTokenIds">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../plbind/" class="md-nav__link">
        Pytorch Lightning Bindings
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../modules/" class="md-nav__link">
        Generic Modules
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../multimodal/" class="md-nav__link">
        Multimodal Modules
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        SLP utility functions
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../apiref/" class="md-nav__link">
        API reference
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators" class="md-nav__link">
    slp.data.collators
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator" class="md-nav__link">
    MultimodalSequenceClassificationCollator
  </a>
  
    <nav class="md-nav" aria-label="MultimodalSequenceClassificationCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.MultimodalSequenceClassificationCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator" class="md-nav__link">
    Seq2SeqCollator
  </a>
  
    <nav class="md-nav" aria-label="Seq2SeqCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.Seq2SeqCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator" class="md-nav__link">
    SequenceClassificationCollator
  </a>
  
    <nav class="md-nav" aria-label="SequenceClassificationCollator">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.collators.SequenceClassificationCollator.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus" class="md-nav__link">
    slp.data.corpus
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader" class="md-nav__link">
    EmbeddingsLoader
  </a>
  
    <nav class="md-nav" aria-label="EmbeddingsLoader">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.augment_embeddings" class="md-nav__link">
    augment_embeddings()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.in_accepted_vocab" class="md-nav__link">
    in_accepted_vocab()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.EmbeddingsLoader.load" class="md-nav__link">
    load()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus" class="md-nav__link">
    HfCorpus
  </a>
  
    <nav class="md-nav" aria-label="HfCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.HfCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus" class="md-nav__link">
    TokenizedCorpus
  </a>
  
    <nav class="md-nav" aria-label="TokenizedCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.TokenizedCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus" class="md-nav__link">
    WordCorpus
  </a>
  
    <nav class="md-nav" aria-label="WordCorpus">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.embeddings" class="md-nav__link">
    embeddings
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.frequencies" class="md-nav__link">
    frequencies
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.idx2word" class="md-nav__link">
    idx2word
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.indices" class="md-nav__link">
    indices
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.raw" class="md-nav__link">
    raw
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.tokenized" class="md-nav__link">
    tokenized
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.vocab" class="md-nav__link">
    vocab
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.vocab_size" class="md-nav__link">
    vocab_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.word2idx" class="md-nav__link">
    word2idx
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.corpus.WordCorpus.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.corpus.create_vocab" class="md-nav__link">
    create_vocab()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets" class="md-nav__link">
    slp.data.datasets
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset" class="md-nav__link">
    CorpusDataset
  </a>
  
    <nav class="md-nav" aria-label="CorpusDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusDataset.map" class="md-nav__link">
    map()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset" class="md-nav__link">
    CorpusLMDataset
  </a>
  
    <nav class="md-nav" aria-label="CorpusLMDataset">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__getitem__" class="md-nav__link">
    __getitem__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.__len__" class="md-nav__link">
    __len__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.datasets.CorpusLMDataset.map" class="md-nav__link">
    map()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms" class="md-nav__link">
    slp.data.transforms
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer" class="md-nav__link">
    HuggingFaceTokenizer
  </a>
  
    <nav class="md-nav" aria-label="HuggingFaceTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.HuggingFaceTokenizer.detokenize" class="md-nav__link">
    detokenize()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken" class="md-nav__link">
    ReplaceUnknownToken
  </a>
  
    <nav class="md-nav" aria-label="ReplaceUnknownToken">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ReplaceUnknownToken.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer" class="md-nav__link">
    SentencepieceTokenizer
  </a>
  
    <nav class="md-nav" aria-label="SentencepieceTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SentencepieceTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer" class="md-nav__link">
    SpacyTokenizer
  </a>
  
    <nav class="md-nav" aria-label="SpacyTokenizer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.SpacyTokenizer.get_nlp" class="md-nav__link">
    get_nlp()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor" class="md-nav__link">
    ToTensor
  </a>
  
    <nav class="md-nav" aria-label="ToTensor">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTensor.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds" class="md-nav__link">
    ToTokenIds
  </a>
  
    <nav class="md-nav" aria-label="ToTokenIds">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds.__call__" class="md-nav__link">
    __call__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.data.transforms.ToTokenIds.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="data-manipulation">Data manipulation</h1>
<p>We provide out of the box support for easy preprocessing of NLP corpora and helpers to work with datasets in the Pytorch way.</p>


  <div class="doc doc-object doc-module">

<a id="slp.data.collators"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.data.collators.MultimodalSequenceClassificationCollator" class="doc doc-heading">
        <code>MultimodalSequenceClassificationCollator</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.MultimodalSequenceClassificationCollator.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call collate function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>List[Dict[str, torch.Tensor]]</code></td>
        <td><p>Batch of samples.
It expects a list of dictionaries from modalities to torch tensors</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Dict[str, torch.Tensor], torch.Tensor, Dict[str, torch.Tensor]]</code></td>
      <td><p>Tuple[Dict[str, torch.Tensor], torch.Tensor, Dict[str, torch.Tensor]]: tuple of
    (dict batched modality tensors, labels, dict of modality sequence lengths)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Call collate function</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (List[Dict[str, torch.Tensor]]): Batch of samples.</span>
<span class="sd">            It expects a list of dictionaries from modalities to torch tensors</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[Dict[str, torch.Tensor], torch.Tensor, Dict[str, torch.Tensor]]: tuple of</span>
<span class="sd">            (dict batched modality tensors, labels, dict of modality sequence lengths)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span> <span class="o">=</span> <span class="p">{}</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="p">{}</span>

    <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modalities</span><span class="p">:</span>
        <span class="n">seq</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">extract_sequence</span><span class="p">(</span><span class="n">batch</span><span class="p">,</span> <span class="n">m</span><span class="p">)</span>
        <span class="n">lengths</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">seq</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="n">lengths</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">lengths</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">)</span>

        <span class="n">inputs</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
            <span class="n">seq</span><span class="p">,</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
            <span class="n">padding_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span><span class="p">,</span>
            <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
        <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">targets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Label</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">label_key</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>

    <span class="c1"># Pad and convert to tensor</span>
    <span class="n">ttargets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">mktensor</span><span class="p">(</span>
        <span class="n">targets</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">label_dtype</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">ttargets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">lengths</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.MultimodalSequenceClassificationCollator.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">modalities</span><span class="o">=</span><span class="p">{</span><span class="s1">&#39;text&#39;</span><span class="p">,</span> <span class="s1">&#39;audio&#39;</span><span class="p">,</span> <span class="s1">&#39;visual&#39;</span><span class="p">},</span> <span class="n">label_key</span><span class="o">=</span><span class="s1">&#39;label&#39;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">label_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float32</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Collate function for sequence classification tasks</p>
<ul>
<li>Perform padding</li>
<li>Calculate sequence lengths</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>pad_indx</code></td>
        <td><code>int</code></td>
        <td><p>Pad token index. Defaults to 0.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>modalities</code></td>
        <td><code>Set</code></td>
        <td><p>Which modalities are included in the batch dict</p></td>
        <td><code>{&#39;text&#39;, &#39;audio&#39;, &#39;visual&#39;}</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Pad sequences to a fixed maximum length</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>label_key</code></td>
        <td><code>str</code></td>
        <td><p>String to access the label in the batch dict</p></td>
        <td><code>&#39;label&#39;</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>device of returned tensors. Leave this as "cpu".
The LightningModule will handle the Conversion.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">MultimodalSequenceClassificationCollator</span><span class="p">())</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span>
    <span class="n">modalities</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;visual&quot;</span><span class="p">,</span> <span class="s2">&quot;text&quot;</span><span class="p">,</span> <span class="s2">&quot;audio&quot;</span><span class="p">},</span>
    <span class="n">label_key</span><span class="o">=</span><span class="s2">&quot;label&quot;</span><span class="p">,</span>
    <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">label_dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">,</span>
    <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Collate function for sequence classification tasks</span>

<span class="sd">    * Perform padding</span>
<span class="sd">    * Calculate sequence lengths</span>

<span class="sd">    Args:</span>
<span class="sd">        pad_indx (int): Pad token index. Defaults to 0.</span>
<span class="sd">        modalities (Set): Which modalities are included in the batch dict</span>
<span class="sd">        max_length (int): Pad sequences to a fixed maximum length</span>
<span class="sd">        label_key (str): String to access the label in the batch dict</span>
<span class="sd">        device (str): device of returned tensors. Leave this as &quot;cpu&quot;.</span>
<span class="sd">            The LightningModule will handle the Conversion.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = torch.utils.DataLoader(my_dataset, collate_fn=MultimodalSequenceClassificationCollator())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span> <span class="o">=</span> <span class="n">pad_indx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_key</span> <span class="o">=</span> <span class="n">label_key</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">modalities</span> <span class="o">=</span> <span class="n">modalities</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_dtype</span> <span class="o">=</span> <span class="n">label_dtype</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.collators.Seq2SeqCollator" class="doc doc-heading">
        <code>Seq2SeqCollator</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.Seq2SeqCollator.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call collate function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>List[Tuple[torch.Tensor, torch.Tensor]]</code></td>
        <td><p>Batch of samples.
It expects a list of tuples (source, target)
Each source and target are a sequences of features or ids.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: Returns tuple of batched tensors
    (inputs, labels, lengths_inputs, lengths_targets)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Call collate function</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (List[Tuple[torch.Tensor, torch.Tensor]]): Batch of samples.</span>
<span class="sd">            It expects a list of tuples (source, target)</span>
<span class="sd">            Each source and target are a sequences of features or ids.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]: Returns tuple of batched tensors</span>
<span class="sd">            (inputs, labels, lengths_inputs, lengths_targets)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">targets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">lengths_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>
    <span class="n">lengths_targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">targets</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">lengths_inputs</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">lengths_inputs</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">)</span>
        <span class="n">lengths_targets</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">lengths_targets</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">)</span>

    <span class="n">inputs_padded</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">targets_padded</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
        <span class="n">targets</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inputs_padded</span><span class="p">,</span> <span class="n">targets_padded</span><span class="p">,</span> <span class="n">lengths_inputs</span><span class="p">,</span> <span class="n">lengths_targets</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.Seq2SeqCollator.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Collate function for seq2seq tasks</p>
<ul>
<li>Perform padding</li>
<li>Calculate sequence lengths</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>pad_indx</code></td>
        <td><code>int</code></td>
        <td><p>Pad token index. Defaults to 0.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Pad sequences to a fixed maximum length</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>device of returned tensors. Leave this as "cpu".
The LightningModule will handle the Conversion.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">Seq2SeqClassificationCollator</span><span class="p">())</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Collate function for seq2seq tasks</span>

<span class="sd">    * Perform padding</span>
<span class="sd">    * Calculate sequence lengths</span>

<span class="sd">    Args:</span>
<span class="sd">        pad_indx (int): Pad token index. Defaults to 0.</span>
<span class="sd">        max_length (int): Pad sequences to a fixed maximum length</span>
<span class="sd">        device (str): device of returned tensors. Leave this as &quot;cpu&quot;.</span>
<span class="sd">            The LightningModule will handle the Conversion.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = torch.utils.DataLoader(my_dataset, collate_fn=Seq2SeqClassificationCollator())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span> <span class="o">=</span> <span class="n">pad_indx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.collators.SequenceClassificationCollator" class="doc doc-heading">
        <code>SequenceClassificationCollator</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.SequenceClassificationCollator.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call collate function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>batch</code></td>
        <td><code>List[Tuple[torch.Tensor, Union[numpy.ndarray, torch.Tensor, List[~T], int]]]</code></td>
        <td><p>Batch of samples.
It expects a list of tuples (inputs, label).</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Returns tuple of batched tensors (inputs, labels, lengths)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">batch</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Label</span><span class="p">]]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Call collate function</span>

<span class="sd">    Args:</span>
<span class="sd">        batch (List[Tuple[torch.Tensor, slp.util.types.Label]]): Batch of samples.</span>
<span class="sd">            It expects a list of tuples (inputs, label).</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: Returns tuple of batched tensors (inputs, labels, lengths)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">inputs</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="n">targets</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Label</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">b</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span> <span class="k">for</span> <span class="n">b</span> <span class="ow">in</span> <span class="n">batch</span><span class="p">]</span>
    <span class="c1">#  targets: List[torch.tensor] = map(list, zip(*batch))</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">tensor</span><span class="p">([</span><span class="n">s</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span> <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">inputs</span><span class="p">],</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="n">lengths</span><span class="p">,</span> <span class="nb">min</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="nb">max</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">)</span>
    <span class="c1"># Pad and convert to tensor</span>
    <span class="n">inputs_padded</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">pad_sequence</span><span class="p">(</span>
        <span class="n">inputs</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">padding_value</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">,</span>
    <span class="p">)</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">ttargets</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">mktensor</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">inputs_padded</span><span class="p">,</span> <span class="n">ttargets</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">),</span> <span class="n">lengths</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.collators.SequenceClassificationCollator.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Collate function for sequence classification tasks</p>
<ul>
<li>Perform padding</li>
<li>Calculate sequence lengths</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>pad_indx</code></td>
        <td><code>int</code></td>
        <td><p>Pad token index. Defaults to 0.</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Pad sequences to a fixed maximum length</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>device of returned tensors. Leave this as "cpu".
The LightningModule will handle the Conversion.</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">dataloader</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">DataLoader</span><span class="p">(</span><span class="n">my_dataset</span><span class="p">,</span> <span class="n">collate_fn</span><span class="o">=</span><span class="n">SequenceClassificationCollator</span><span class="p">())</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/data/collators.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pad_indx</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s2">&quot;cpu&quot;</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Collate function for sequence classification tasks</span>

<span class="sd">    * Perform padding</span>
<span class="sd">    * Calculate sequence lengths</span>

<span class="sd">    Args:</span>
<span class="sd">        pad_indx (int): Pad token index. Defaults to 0.</span>
<span class="sd">        max_length (int): Pad sequences to a fixed maximum length</span>
<span class="sd">        device (str): device of returned tensors. Leave this as &quot;cpu&quot;.</span>
<span class="sd">            The LightningModule will handle the Conversion.</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; dataloader = torch.utils.DataLoader(my_dataset, collate_fn=SequenceClassificationCollator())</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pad_indx</span> <span class="o">=</span> <span class="n">pad_indx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.data.corpus"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.data.corpus.EmbeddingsLoader" class="doc doc-heading">
        <code>EmbeddingsLoader</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.EmbeddingsLoader.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embeddings_file</span><span class="p">,</span> <span class="n">dim</span><span class="p">,</span> <span class="n">vocab</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">extra_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Load word embeddings in text format</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>embeddings_file</code></td>
        <td><code>str</code></td>
        <td><p>File where embeddings are stored (e.g. glove.6B.50d.txt)</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dim</code></td>
        <td><code>int</code></td>
        <td><p>Dimensionality of embeddings</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>vocab</code></td>
        <td><code>Optional[Dict[str, int]]</code></td>
        <td><p>Load only embeddings in vocab. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>extra_tokens</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Create random embeddings for these special tokens.
Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">embeddings_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">vocab</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">extra_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="kc">None</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Load word embeddings in text format</span>

<span class="sd">    Args:</span>
<span class="sd">        embeddings_file (str): File where embeddings are stored (e.g. glove.6B.50d.txt)</span>
<span class="sd">        dim (int): Dimensionality of embeddings</span>
<span class="sd">        vocab (Optional[Dict[str, int]]): Load only embeddings in vocab. Defaults to None.</span>
<span class="sd">        extra_tokens (Optional[slp.config.nlp.SPECIAL_TOKENS]): Create random embeddings for these special tokens.</span>
<span class="sd">            Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span> <span class="o">=</span> <span class="n">embeddings_file</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="o">=</span> <span class="n">vocab</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">cache_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_cache_name</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dim_</span> <span class="o">=</span> <span class="n">dim</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">extra_tokens</span> <span class="o">=</span> <span class="n">extra_tokens</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.EmbeddingsLoader.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>String representation of class</p>

        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;String representation of class&quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="si">}</span><span class="s2">(</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="si">}</span><span class="s2">, </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_</span><span class="si">}</span><span class="s2">)&quot;</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.EmbeddingsLoader.augment_embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">augment_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">token</span><span class="p">,</span> <span class="n">emb</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Create a random embedding for a special token and append it to the embeddings array</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>word2idx</code></td>
        <td><code>Dict[str, int]</code></td>
        <td><p>Current word2idx map</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>idx2word</code></td>
        <td><code>Dict[int, str]</code></td>
        <td><p>Current idx2word map</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>List[numpy.ndarray]</code></td>
        <td><p>Embeddings array as list of embeddings</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>token</code></td>
        <td><code>str</code></td>
        <td><p>The special token (e.g. [PAD])</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>emb</code></td>
        <td><code>Optional[numpy.ndarray]</code></td>
        <td><p>Optional value for the embedding to be appended.
Defaults to None, where a random embedding is created.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Dict[str, int], Dict[int, str], List[numpy.ndarray]]</code></td>
      <td><p>Tuple[Dict[str, int], Dict[int, str], List[np.ndarray]]: (word2idx, idx2word, embeddings) tuple</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">augment_embeddings</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">word2idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">idx2word</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span>
    <span class="n">embeddings</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">],</span>
    <span class="n">token</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">emb</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Create a random embedding for a special token and append it to the embeddings array</span>

<span class="sd">    Args:</span>
<span class="sd">        word2idx (Dict[str, int]): Current word2idx map</span>
<span class="sd">        idx2word (Dict[int, str]): Current idx2word map</span>
<span class="sd">        embeddings (List[np.ndarray]): Embeddings array as list of embeddings</span>
<span class="sd">        token (str): The special token (e.g. [PAD])</span>
<span class="sd">        emb (Optional[np.ndarray]): Optional value for the embedding to be appended.</span>
<span class="sd">            Defaults to None, where a random embedding is created.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[Dict[str, int], Dict[int, str], List[np.ndarray]]: (word2idx, idx2word, embeddings) tuple</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">word2idx</span><span class="p">[</span><span class="n">token</span><span class="p">]</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>
    <span class="n">idx2word</span><span class="p">[</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)]</span> <span class="o">=</span> <span class="n">token</span>

    <span class="k">if</span> <span class="n">emb</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">emb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">uniform</span><span class="p">(</span><span class="n">low</span><span class="o">=-</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">high</span><span class="o">=</span><span class="mf">0.05</span><span class="p">,</span> <span class="n">size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_</span><span class="p">)</span>
    <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">emb</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.EmbeddingsLoader.in_accepted_vocab" class="doc doc-heading">
<code class="highlight language-python"><span class="n">in_accepted_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Check if word exists in given vocabulary</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>word</code></td>
        <td><code>str</code></td>
        <td><p>word from embeddings file</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>bool</code></td>
      <td><p>bool: Word exists</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">in_accepted_vocab</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">bool</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Check if word exists in given vocabulary</span>

<span class="sd">    Args:</span>
<span class="sd">        word (str): word from embeddings file</span>

<span class="sd">    Returns:</span>
<span class="sd">        bool: Word exists</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="kc">True</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">word</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.EmbeddingsLoader.load" class="doc doc-heading">
<code class="highlight language-python"><span class="n">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Read the word vectors from a text file</p>
<ul>
<li>Read embeddings</li>
<li>Filter with given vocabulary</li>
<li>Augment with special tokens</li>
</ul>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[Dict[str, int], Dict[int, str], numpy.ndarray]</code></td>
      <td><p>types.Embeddings: (word2idx, idx2word, embeddings) tuple</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@system</span><span class="o">.</span><span class="n">timethis</span><span class="p">(</span><span class="n">method</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="k">def</span> <span class="nf">load</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">types</span><span class="o">.</span><span class="n">Embeddings</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Read the word vectors from a text file</span>

<span class="sd">    * Read embeddings</span>
<span class="sd">    * Filter with given vocabulary</span>
<span class="sd">    * Augment with special tokens</span>

<span class="sd">    Returns:</span>
<span class="sd">        types.Embeddings: (word2idx, idx2word, embeddings) tuple</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># in order to avoid this time consuming operation, cache the results</span>
    <span class="k">try</span><span class="p">:</span>
        <span class="n">cache</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_load_cache</span><span class="p">()</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Loaded word embeddings from cache.&quot;</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">cache</span>
    <span class="k">except</span> <span class="ne">OSError</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Didn&#39;t find embeddings cache file </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span><span class="s2">&quot;Loading embeddings from file.&quot;</span><span class="p">)</span>

    <span class="c1"># create the necessary dictionaries and the word embeddings matrix</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">os</span><span class="o">.</span><span class="n">path</span><span class="o">.</span><span class="n">exists</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="p">):</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">critical</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="si">}</span><span class="s2"> not found!&quot;</span><span class="p">)</span>
        <span class="k">raise</span> <span class="ne">OSError</span><span class="p">(</span><span class="n">errno</span><span class="o">.</span><span class="n">ENOENT</span><span class="p">,</span> <span class="n">os</span><span class="o">.</span><span class="n">strerror</span><span class="p">(</span><span class="n">errno</span><span class="o">.</span><span class="n">ENOENT</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Indexing file </span><span class="si">{</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="si">}</span><span class="s2"> ...&quot;</span><span class="p">)</span>

    <span class="c1"># create the 2D array, which will be used for initializing</span>
    <span class="c1"># the Embedding layer of a NN.</span>
    <span class="c1"># We reserve the first row (idx=0), as the word embedding,</span>
    <span class="c1"># which will be used for zero padding (word with id = 0).</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_tokens</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">augment_embeddings</span><span class="p">(</span>
            <span class="p">{},</span>
            <span class="p">{},</span>
            <span class="p">[],</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">extra_tokens</span><span class="o">.</span><span class="n">PAD</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
            <span class="n">emb</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_</span><span class="p">),</span>
        <span class="p">)</span>

        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_tokens</span><span class="p">:</span>  <span class="c1"># type: ignore</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">debug</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Adding token </span><span class="si">{</span><span class="n">token</span><span class="o">.</span><span class="n">value</span><span class="si">}</span><span class="s2"> to embeddings matrix&quot;</span><span class="p">)</span>

            <span class="k">if</span> <span class="n">token</span> <span class="o">==</span> <span class="bp">self</span><span class="o">.</span><span class="n">extra_tokens</span><span class="o">.</span><span class="n">PAD</span><span class="p">:</span>
                <span class="k">continue</span>
            <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">augment_embeddings</span><span class="p">(</span>
                <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span><span class="p">,</span> <span class="n">token</span><span class="o">.</span><span class="n">value</span>
            <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">augment_embeddings</span><span class="p">(</span>
            <span class="p">{},</span> <span class="p">{},</span> <span class="p">[],</span> <span class="s2">&quot;[PAD]&quot;</span><span class="p">,</span> <span class="n">emb</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dim_</span><span class="p">)</span>
        <span class="p">)</span>
    <span class="c1"># read file, line by line</span>
    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">num_lines</span> <span class="o">=</span> <span class="nb">sum</span><span class="p">(</span><span class="mi">1</span> <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">f</span><span class="p">)</span>

    <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">embeddings_file</span><span class="p">,</span> <span class="s2">&quot;r&quot;</span><span class="p">)</span> <span class="k">as</span> <span class="n">f</span><span class="p">:</span>
        <span class="n">index</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
            <span class="n">f</span><span class="p">,</span> <span class="n">total</span><span class="o">=</span><span class="n">num_lines</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Loading word embeddings...&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">):</span>
            <span class="c1"># skip the first row if it is a header</span>

            <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">dim_</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="n">values</span> <span class="o">=</span> <span class="n">line</span><span class="o">.</span><span class="n">rstrip</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="n">word</span> <span class="o">=</span> <span class="n">values</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

            <span class="k">if</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">word2idx</span><span class="p">:</span>
                <span class="k">continue</span>

            <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_accepted_vocab</span><span class="p">(</span><span class="n">word</span><span class="p">):</span>
                <span class="k">continue</span>

            <span class="n">vector</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">values</span><span class="p">[</span><span class="mi">1</span><span class="p">:],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
            <span class="n">idx2word</span><span class="p">[</span><span class="n">index</span><span class="p">]</span> <span class="o">=</span> <span class="n">word</span>
            <span class="n">word2idx</span><span class="p">[</span><span class="n">word</span><span class="p">]</span> <span class="o">=</span> <span class="n">index</span>
            <span class="n">embeddings</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">vector</span><span class="p">)</span>
            <span class="n">index</span> <span class="o">+=</span> <span class="mi">1</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Loaded </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span><span class="si">}</span><span class="s2"> word vectors.&quot;</span><span class="p">)</span>
    <span class="n">embeddings_out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="s2">&quot;float32&quot;</span><span class="p">)</span>

    <span class="c1"># write the data to a cache file</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_dump_cache</span><span class="p">((</span><span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings_out</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings_out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.corpus.HfCorpus" class="doc doc-heading">
        <code>HfCorpus</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">embeddings</span><span class="p">:</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Unused. Defined for compatibility</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.frequencies" class="doc doc-heading">
<code class="highlight language-python"><span class="n">frequencies</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve wordpieces occurence counts</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: wordpieces occurence counts</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.idx2word" class="doc doc-heading">
<code class="highlight language-python"><span class="n">idx2word</span><span class="p">:</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Unused. Defined for compatibility</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.indices" class="doc doc-heading">
<code class="highlight language-python"><span class="n">indices</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve corpus as token indices</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[List[int]]</code></td>
      <td><p>List[List[int]]: Token indices for corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.raw" class="doc doc-heading">
<code class="highlight language-python"><span class="n">raw</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve raw corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>List[str]: Raw Corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.tokenized" class="doc doc-heading">
<code class="highlight language-python"><span class="n">tokenized</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve tokenized corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[List[str]]</code></td>
      <td><p>List[List[str]]: tokenized corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.vocab" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve set of words in vocabulary</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Set[str]</code></td>
      <td><p>Set[str]: set of words in vocabulary</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.vocab_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve vocabulary size</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Vocabulary size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.HfCorpus.word2idx" class="doc doc-heading">
<code class="highlight language-python"><span class="n">word2idx</span><span class="p">:</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Unused. Defined for compatibility</p>
    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.HfCorpus.__getitem__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Get ith element in corpus as token indices</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>idx</code></td>
        <td><code>List[int]</code></td>
        <td><p>index in corpus</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of token indices for sentence</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Get ith element in corpus as token indices</span>

<span class="sd">    Args:</span>
<span class="sd">        idx (List[int]): index in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of token indices for sentence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&lt;=</span> <span class="mi">0</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">][:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.HfCorpus.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">tokenizer_model</span><span class="o">=</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Process a corpus using hugging face tokenizers</p>
<p>Select one of hugging face tokenizers and process corpus</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>List[str]</code></td>
        <td><p>List of sentences</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lower</code></td>
        <td><code>bool</code></td>
        <td><p>Convert strings to lower case. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>tokenizer_model</code></td>
        <td><code>str</code></td>
        <td><p>Hugging face model to use. Defaults to "bert-base-uncased".</p></td>
        <td><code>&#39;bert-base-uncased&#39;</code></td>
      </tr>
      <tr>
        <td><code>add_special_tokens</code></td>
        <td><code>bool</code></td>
        <td><p>Add special tokens in sentence during tokenization. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>special_tokens</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens to include in the vocabulary.
 Defaults to slp.config.nlp.SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Crop sequences above this length. Defaults to -1 where sequences are left unaltered.</p></td>
        <td><code>-1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">corpus</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">lower</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">tokenizer_model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span>
    <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Process a corpus using hugging face tokenizers</span>

<span class="sd">    Select one of hugging face tokenizers and process corpus</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (List[str]): List of sentences</span>
<span class="sd">        lower (bool): Convert strings to lower case. Defaults to True.</span>
<span class="sd">        tokenizer_model (str): Hugging face model to use. Defaults to &quot;bert-base-uncased&quot;.</span>
<span class="sd">        add_special_tokens (bool): Add special tokens in sentence during tokenization. Defaults to True.</span>
<span class="sd">        special_tokens (Optional[SPECIAL_TOKENS]): Special tokens to include in the vocabulary.</span>
<span class="sd">             Defaults to slp.config.nlp.SPECIAL_TOKENS.</span>
<span class="sd">        max_length (int): Crop sequences above this length. Defaults to -1 where sequences are left unaltered.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">corpus_</span> <span class="o">=</span> <span class="n">corpus</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
        <span class="sa">f</span><span class="s2">&quot;Tokenizing corpus using hugging face tokenizer from </span><span class="si">{</span><span class="n">tokenizer_model</span><span class="si">}</span><span class="s2">&quot;</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">HuggingFaceTokenizer</span><span class="p">(</span>
        <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="n">tokenizer_model</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="n">add_special_tokens</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">corpus_</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Converting tokens to indices...&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span>
        <span class="p">)</span>
    <span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">detokenize</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">,</span>
            <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Mapping indices to tokens...&quot;</span><span class="p">,</span>
            <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span> <span class="o">=</span> <span class="n">create_vocab</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.HfCorpus.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Number of samples in corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Corpus length</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Number of samples in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Corpus length</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.corpus.TokenizedCorpus" class="doc doc-heading">
        <code>TokenizedCorpus</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">embeddings</span><span class="p">:</span> <span class="kc">None</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Unused. Kept for compatibility</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.frequencies" class="doc doc-heading">
<code class="highlight language-python"><span class="n">frequencies</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve wordpieces occurence counts</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: wordpieces occurence counts</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.idx2word" class="doc doc-heading">
<code class="highlight language-python"><span class="n">idx2word</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve idx2word mapping</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[int, str]</code></td>
      <td><p>Dict[str, int]: idx2word mapping</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.indices" class="doc doc-heading">
<code class="highlight language-python"><span class="n">indices</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve corpus as token indices</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[List[int], List[List[int]]]</code></td>
      <td><p>List[List[int]]: Token indices for corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.raw" class="doc doc-heading">
<code class="highlight language-python"><span class="n">raw</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve raw corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[List[str], List[List[str]]]</code></td>
      <td><p>List[str]: Raw Corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.tokenized" class="doc doc-heading">
<code class="highlight language-python"><span class="n">tokenized</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve tokenized corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[List[str], List[List[str]]]</code></td>
      <td><p>List[List[str]]: Tokenized corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.vocab" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve set of words in vocabulary</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Set[str]</code></td>
      <td><p>Set[str]: set of words in vocabulary</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.vocab_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve vocabulary size</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Vocabulary size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.TokenizedCorpus.word2idx" class="doc doc-heading">
<code class="highlight language-python"><span class="n">word2idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve word2idx mapping</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: word2idx mapping</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.TokenizedCorpus.__getitem__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Get ith element in corpus as token indices</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>idx</code></td>
        <td><code>List[int]</code></td>
        <td><p>index in corpus</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of token indices for sentence</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Get ith element in corpus as token indices</span>

<span class="sd">    Args:</span>
<span class="sd">        idx (List[int]): index in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of token indices for sentence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&lt;=</span> <span class="mi">0</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">][:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.TokenizedCorpus.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">word2idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Wrap a corpus that's already tokenized</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>Union[List[str], List[List[str]]]</code></td>
        <td><p>List of tokens or List of lists of tokens</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>word2idx</code></td>
        <td><code>Dict[str, int]</code></td>
        <td><p>Token to index mapping. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>special_tokens</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special Tokens. Defaults to SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">corpus</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]],</span>
    <span class="n">word2idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wrap a corpus that&#39;s already tokenized</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (Union[List[str], List[List[str]]]): List of tokens or List of lists of tokens</span>
<span class="sd">        word2idx (Dict[str, int], optional): Token to index mapping. Defaults to None.</span>
<span class="sd">        special_tokens (Optional[SPECIAL_TOKENS], optional): Special Tokens. Defaults to SPECIAL_TOKENS.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">corpus_</span> <span class="o">=</span> <span class="n">corpus</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span> <span class="o">=</span> <span class="n">corpus</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span> <span class="o">=</span> <span class="n">create_vocab</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">word2idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Converting tokens to ids using word2idx.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span> <span class="o">=</span> <span class="n">word2idx</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="s2">&quot;No word2idx provided. Will convert tokens to ids using an iterative counter.&quot;</span>
        <span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="o">.</span><span class="n">keys</span><span class="p">(),</span> <span class="n">itertools</span><span class="o">.</span><span class="n">count</span><span class="p">()))</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">idx2word_</span> <span class="o">=</span> <span class="p">{</span><span class="n">v</span><span class="p">:</span> <span class="n">k</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span><span class="o">.</span><span class="n">items</span><span class="p">()}</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">to_token_ids</span> <span class="o">=</span> <span class="n">ToTokenIds</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span><span class="p">,</span>
        <span class="n">specials</span><span class="o">=</span><span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_token_ids</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">,</span>
                <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Converting tokens to token ids...&quot;</span><span class="p">,</span>
                <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">to_token_ids</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.TokenizedCorpus.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Number of samples in corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Corpus length</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Number of samples in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Corpus length</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.corpus.WordCorpus" class="doc doc-heading">
        <code>WordCorpus</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">embeddings</span><span class="p">:</span> <span class="n">ndarray</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve embeddings array</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>ndarray</code></td>
      <td><p>np.ndarray: Array of pretrained word embeddings</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.frequencies" class="doc doc-heading">
<code class="highlight language-python"><span class="n">frequencies</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve word occurence counts</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: word occurence counts</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.idx2word" class="doc doc-heading">
<code class="highlight language-python"><span class="n">idx2word</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve idx2word mapping</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[int, str]</code></td>
      <td><p>Dict[str, int]: idx2word mapping</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.indices" class="doc doc-heading">
<code class="highlight language-python"><span class="n">indices</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve corpus as token indices</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[List[int]]</code></td>
      <td><p>List[List[int]]: Token indices for corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.raw" class="doc doc-heading">
<code class="highlight language-python"><span class="n">raw</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve raw corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>List[str]: Raw Corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.tokenized" class="doc doc-heading">
<code class="highlight language-python"><span class="n">tokenized</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve tokenized corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[List[str]]</code></td>
      <td><p>List[List[str]]: Tokenized corpus</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.vocab" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab</span><span class="p">:</span> <span class="n">Set</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve set of words in vocabulary</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Set[str]</code></td>
      <td><p>Set[str]: set of words in vocabulary</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.vocab_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve vocabulary size for corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: vocabulary size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h3 id="slp.data.corpus.WordCorpus.word2idx" class="doc doc-heading">
<code class="highlight language-python"><span class="n">word2idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Retrieve word2idx mapping</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: word2idx mapping</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.WordCorpus.__getitem__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Get ith element in corpus as token indices</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>idx</code></td>
        <td><code>List[int]</code></td>
        <td><p>index in corpus</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of token indices for sentence</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Get ith element in corpus as token indices</span>

<span class="sd">    Args:</span>
<span class="sd">        idx (List[int]): index in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of token indices for sentence</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">&lt;=</span> <span class="mi">0</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">[</span><span class="n">idx</span><span class="p">][:</span> <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span><span class="p">]</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.WordCorpus.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">limit_vocab_size</span><span class="o">=</span><span class="mi">30000</span><span class="p">,</span> <span class="n">word2idx</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">idx2word</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings_file</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings_dim</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">append_eos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;en_core_web_md&#39;</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Load corpus embeddings, tokenize in words using spacy and convert to ids</p>
<p>This class handles the handling of a raw corpus. It handles:</p>
<ul>
<li>Tokenization into words (spacy)</li>
<li>Loading of pretrained word embedding</li>
<li>Calculation of word frequencies / corpus statistics</li>
<li>Conversion to token ids</li>
</ul>
<p>You can pass either:</p>
<ul>
<li>Pass an embeddings file to load pretrained embeddings and create the word2idx mapping</li>
<li>Pass already loaded embeddings array and word2idx. This is useful for the dev / test splits
  where we want to pass the train split embeddings / word2idx.</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>List[str]</code></td>
        <td><p>Corpus as a list of sentences</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>limit_vocab_size</code></td>
        <td><code>int</code></td>
        <td><p>Upper bound for number of most frequent tokens to keep. Defaults to 30000.</p></td>
        <td><code>30000</code></td>
      </tr>
      <tr>
        <td><code>word2idx</code></td>
        <td><code>Optional[Dict[str, int]]</code></td>
        <td><p>Mapping of word to indices. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>idx2word</code></td>
        <td><code>Optional[Dict[int, str]]</code></td>
        <td><p>Mapping of indices to words. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>Optional[numpy.ndarray]</code></td>
        <td><p>Embeddings array. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings_file</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Embeddings file to read. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings_dim</code></td>
        <td><code>int</code></td>
        <td><p>Dimension of embeddings. Defaults to 300.</p></td>
        <td><code>300</code></td>
      </tr>
      <tr>
        <td><code>lower</code></td>
        <td><code>bool</code></td>
        <td><p>Convert strings to lower case. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>special_tokens</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens to include in the vocabulary.
 Defaults to slp.config.nlp.SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
      <tr>
        <td><code>prepend_bos</code></td>
        <td><code>bool</code></td>
        <td><p>Prepend Beginning of Sequence token for seq2seq tasks. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>append_eos</code></td>
        <td><code>bool</code></td>
        <td><p>Append End of Sequence token for seq2seq tasks. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>lang</code></td>
        <td><code>str</code></td>
        <td><p>Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to "en_core_web_md".</p></td>
        <td><code>&#39;en_core_web_md&#39;</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Crop sequences above this length. Defaults to -1 where sequences are left unaltered.</p></td>
        <td><code>-1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">corpus</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span>
    <span class="n">limit_vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">30000</span><span class="p">,</span>
    <span class="n">word2idx</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">idx2word</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">int</span><span class="p">,</span> <span class="nb">str</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings_file</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span>
    <span class="n">lower</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="n">prepend_bos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">append_eos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;en_core_web_md&quot;</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Load corpus embeddings, tokenize in words using spacy and convert to ids</span>

<span class="sd">    This class handles the handling of a raw corpus. It handles:</span>

<span class="sd">    * Tokenization into words (spacy)</span>
<span class="sd">    * Loading of pretrained word embedding</span>
<span class="sd">    * Calculation of word frequencies / corpus statistics</span>
<span class="sd">    * Conversion to token ids</span>

<span class="sd">    You can pass either:</span>

<span class="sd">    * Pass an embeddings file to load pretrained embeddings and create the word2idx mapping</span>
<span class="sd">    * Pass already loaded embeddings array and word2idx. This is useful for the dev / test splits</span>
<span class="sd">      where we want to pass the train split embeddings / word2idx.</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (List[List[str]]): Corpus as a list of sentences</span>
<span class="sd">        limit_vocab_size (int): Upper bound for number of most frequent tokens to keep. Defaults to 30000.</span>
<span class="sd">        word2idx (Optional[Dict[str, int]]): Mapping of word to indices. Defaults to None.</span>
<span class="sd">        idx2word (Optional[Dict[int, str]]): Mapping of indices to words. Defaults to None.</span>
<span class="sd">        embeddings (Optional[np.ndarray]): Embeddings array. Defaults to None.</span>
<span class="sd">        embeddings_file (Optional[str]): Embeddings file to read. Defaults to None.</span>
<span class="sd">        embeddings_dim (int): Dimension of embeddings. Defaults to 300.</span>
<span class="sd">        lower (bool): Convert strings to lower case. Defaults to True.</span>
<span class="sd">        special_tokens (Optional[SPECIAL_TOKENS]): Special tokens to include in the vocabulary.</span>
<span class="sd">             Defaults to slp.config.nlp.SPECIAL_TOKENS.</span>
<span class="sd">        prepend_bos (bool): Prepend Beginning of Sequence token for seq2seq tasks. Defaults to False.</span>
<span class="sd">        append_eos (bool): Append End of Sequence token for seq2seq tasks. Defaults to False.</span>
<span class="sd">        lang (str): Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to &quot;en_core_web_md&quot;.</span>
<span class="sd">        max_length (int): Crop sequences above this length. Defaults to -1 where sequences are left unaltered.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># FIXME: Extract super class to avoid repetition</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">corpus_</span> <span class="o">=</span> <span class="n">corpus</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">max_length</span> <span class="o">=</span> <span class="n">max_length</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">SpacyTokenizer</span><span class="p">(</span>
        <span class="n">lower</span><span class="o">=</span><span class="n">lower</span><span class="p">,</span>
        <span class="n">prepend_bos</span><span class="o">=</span><span class="n">prepend_bos</span><span class="p">,</span>
        <span class="n">append_eos</span><span class="o">=</span><span class="n">append_eos</span><span class="p">,</span>
        <span class="n">specials</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
        <span class="n">lang</span><span class="o">=</span><span class="n">lang</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Tokenizing corpus using spacy </span><span class="si">{</span><span class="n">lang</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
        <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus_</span><span class="p">,</span> <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Tokenizing corpus...&quot;</span><span class="p">,</span> <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span> <span class="o">=</span> <span class="n">create_vocab</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">,</span>
        <span class="n">vocab_size</span><span class="o">=</span><span class="n">limit_vocab_size</span> <span class="k">if</span> <span class="n">word2idx</span> <span class="ow">is</span> <span class="kc">None</span> <span class="k">else</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
        <span class="n">special_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">idx2word_</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
    <span class="c1"># self.corpus_indices_ = self.tokenized_corpus_</span>

    <span class="k">if</span> <span class="n">word2idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Word2idx was already provided. Going to used it.&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">embeddings_file</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="ow">and</span> <span class="n">word2idx</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Going to load </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="p">)</span><span class="si">}</span><span class="s2"> embeddings from </span><span class="si">{</span><span class="n">embeddings_file</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>
        <span class="n">loader</span> <span class="o">=</span> <span class="n">EmbeddingsLoader</span><span class="p">(</span>
            <span class="n">embeddings_file</span><span class="p">,</span>
            <span class="n">embeddings_dim</span><span class="p">,</span>
            <span class="n">vocab</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="p">,</span>
            <span class="n">extra_tokens</span><span class="o">=</span><span class="n">special_tokens</span><span class="p">,</span>
        <span class="p">)</span>
        <span class="n">word2idx</span><span class="p">,</span> <span class="n">idx2word</span><span class="p">,</span> <span class="n">embeddings</span> <span class="o">=</span> <span class="n">loader</span><span class="o">.</span><span class="n">load</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">embeddings_</span> <span class="o">=</span> <span class="n">embeddings</span>

    <span class="k">if</span> <span class="n">idx2word</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">idx2word_</span> <span class="o">=</span> <span class="n">idx2word</span>

    <span class="k">if</span> <span class="n">word2idx</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span> <span class="o">=</span> <span class="n">word2idx</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Converting tokens to ids using word2idx.&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">to_token_ids</span> <span class="o">=</span> <span class="n">ToTokenIds</span><span class="p">(</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span><span class="p">,</span>
            <span class="n">specials</span><span class="o">=</span><span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span> <span class="o">=</span> <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">to_token_ids</span><span class="p">(</span><span class="n">s</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">s</span> <span class="ow">in</span> <span class="n">tqdm</span><span class="p">(</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">tokenized_corpus_</span><span class="p">,</span>
                <span class="n">desc</span><span class="o">=</span><span class="s2">&quot;Converting tokens to token ids...&quot;</span><span class="p">,</span>
                <span class="n">leave</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="p">)</span>
        <span class="p">]</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Filtering corpus vocabulary.&quot;</span><span class="p">)</span>

        <span class="n">updated_vocab</span> <span class="o">=</span> <span class="p">{}</span>

        <span class="k">for</span> <span class="n">k</span><span class="p">,</span> <span class="n">v</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="o">.</span><span class="n">items</span><span class="p">():</span>
            <span class="k">if</span> <span class="n">k</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx_</span><span class="p">:</span>
                <span class="n">updated_vocab</span><span class="p">[</span><span class="n">k</span><span class="p">]</span> <span class="o">=</span> <span class="n">v</span>

        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;Out of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="p">)</span><span class="si">}</span><span class="s2"> tokens </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span><span class="p">)</span> <span class="o">-</span> <span class="nb">len</span><span class="p">(</span><span class="n">updated_vocab</span><span class="p">)</span><span class="si">}</span><span class="s2"> were not found in the pretrained embeddings.&quot;</span>
        <span class="p">)</span>

        <span class="bp">self</span><span class="o">.</span><span class="n">vocab_</span> <span class="o">=</span> <span class="n">updated_vocab</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.corpus.WordCorpus.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Number of samples in corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Corpus length</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="nb">int</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Number of samples in corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Corpus length</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus_indices_</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="slp.data.corpus.create_vocab" class="doc doc-heading">
<code class="highlight language-python"><span class="n">create_vocab</span><span class="p">(</span><span class="n">corpus</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Create the vocabulary based on tokenized input corpus</p>
<ul>
<li>Injects special tokens in the vocabulary</li>
<li>Calculates the occurence count for each token</li>
<li>Limits vocabulary to vocab_size most common tokens</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>Union[List[str], List[List[str]]]</code></td>
        <td><p>The tokenized corpus as a list of sentences or a list of tokenized sentences</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>vocab_size</code></td>
        <td><code>int</code></td>
        <td><p>[description]. Limit vocabulary to vocab_size most common tokens.
Defaults to -1 which keeps all tokens.</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>special_tokens</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens to include in the vocabulary. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, int]</code></td>
      <td><p>Dict[str, int]: Dictionary of all accepted tokens and their corresponding occurence counts</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="n">create_vocab</span><span class="p">([</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;galaxy&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;away&quot;</span><span class="p">])</span>
<span class="p">{</span><span class="s1">&#39;far&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;away&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;galaxy&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">create_vocab</span><span class="p">([</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;galaxy&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;away&quot;</span><span class="p">],</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;far&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
<span class="o">&gt;&gt;&gt;</span> <span class="n">create_vocab</span><span class="p">([</span><span class="s2">&quot;in&quot;</span><span class="p">,</span> <span class="s2">&quot;a&quot;</span><span class="p">,</span> <span class="s2">&quot;galaxy&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;far&quot;</span><span class="p">,</span> <span class="s2">&quot;away&quot;</span><span class="p">],</span> <span class="n">vocab_size</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">special_tokens</span><span class="o">=</span><span class="n">slp</span><span class="o">.</span><span class="n">config</span><span class="o">.</span><span class="n">nlp</span><span class="o">.</span><span class="n">SPECIAL_TOKENS</span><span class="p">)</span>
<span class="p">{</span><span class="s1">&#39;[PAD]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[MASK]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[UNK]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[BOS]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[EOS]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[CLS]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;[SEP]&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;far&#39;</span><span class="p">:</span> <span class="mi">2</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">,</span> <span class="s1">&#39;in&#39;</span><span class="p">:</span> <span class="mi">1</span><span class="p">}</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/data/corpus.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">create_vocab</span><span class="p">(</span>
    <span class="n">corpus</span><span class="p">:</span> <span class="n">Union</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">],</span> <span class="n">List</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]]],</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">special_tokens</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Create the vocabulary based on tokenized input corpus</span>

<span class="sd">    * Injects special tokens in the vocabulary</span>
<span class="sd">    * Calculates the occurence count for each token</span>
<span class="sd">    * Limits vocabulary to vocab_size most common tokens</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (Union[List[str], List[List[str]]]): The tokenized corpus as a list of sentences or a list of tokenized sentences</span>
<span class="sd">        vocab_size (int): [description]. Limit vocabulary to vocab_size most common tokens.</span>
<span class="sd">            Defaults to -1 which keeps all tokens.</span>
<span class="sd">        special_tokens Optional[SPECIAL_TOKENS]: Special tokens to include in the vocabulary. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, int]: Dictionary of all accepted tokens and their corresponding occurence counts</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; create_vocab([&quot;in&quot;, &quot;a&quot;, &quot;galaxy&quot;, &quot;far&quot;, &quot;far&quot;, &quot;away&quot;])</span>
<span class="sd">        {&#39;far&#39;: 2, &#39;away&#39;: 1, &#39;galaxy&#39;: 1, &#39;a&#39;: 1, &#39;in&#39;: 1}</span>
<span class="sd">        &gt;&gt;&gt; create_vocab([&quot;in&quot;, &quot;a&quot;, &quot;galaxy&quot;, &quot;far&quot;, &quot;far&quot;, &quot;away&quot;], vocab_size=3)</span>
<span class="sd">        {&#39;far&#39;: 2, &#39;a&#39;: 1, &#39;in&#39;: 1}</span>
<span class="sd">        &gt;&gt;&gt; create_vocab([&quot;in&quot;, &quot;a&quot;, &quot;galaxy&quot;, &quot;far&quot;, &quot;far&quot;, &quot;away&quot;], vocab_size=3, special_tokens=slp.config.nlp.SPECIAL_TOKENS)</span>
<span class="sd">        {&#39;[PAD]&#39;: 0, &#39;[MASK]&#39;: 0, &#39;[UNK]&#39;: 0, &#39;[BOS]&#39;: 0, &#39;[EOS]&#39;: 0, &#39;[CLS]&#39;: 0, &#39;[SEP]&#39;: 0, &#39;far&#39;: 2, &#39;a&#39;: 1, &#39;in&#39;: 1}</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="n">corpus</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">corpus</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">itertools</span><span class="o">.</span><span class="n">chain</span><span class="o">.</span><span class="n">from_iterable</span><span class="p">(</span><span class="n">corpus</span><span class="p">))</span>
    <span class="n">freq</span> <span class="o">=</span> <span class="n">Counter</span><span class="p">(</span><span class="n">corpus</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">special_tokens</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">extra_tokens</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">extra_tokens</span> <span class="o">=</span> <span class="n">special_tokens</span><span class="o">.</span><span class="n">to_list</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">vocab_size</span> <span class="o">&lt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">freq</span><span class="p">)</span>
    <span class="n">take</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="n">vocab_size</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">freq</span><span class="p">))</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Keeping </span><span class="si">{</span><span class="n">vocab_size</span><span class="si">}</span><span class="s2"> most common tokens out of </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">freq</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">take0</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">Any</span><span class="p">:</span>
        <span class="sd">&quot;&quot;&quot;Take first tuple element&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="n">x</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>

    <span class="n">common_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="n">take0</span><span class="p">,</span> <span class="n">freq</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="n">take</span><span class="p">)))</span>
    <span class="n">common_words</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">set</span><span class="p">(</span><span class="n">common_words</span><span class="p">)</span> <span class="o">-</span> <span class="nb">set</span><span class="p">(</span><span class="n">extra_tokens</span><span class="p">))</span>
    <span class="n">words</span> <span class="o">=</span> <span class="n">extra_tokens</span> <span class="o">+</span> <span class="n">common_words</span>

    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">words</span><span class="p">)</span> <span class="o">&gt;</span> <span class="n">vocab_size</span><span class="p">:</span>
        <span class="n">words</span> <span class="o">=</span> <span class="n">words</span><span class="p">[:</span> <span class="n">vocab_size</span> <span class="o">+</span> <span class="nb">len</span><span class="p">(</span><span class="n">extra_tokens</span><span class="p">)]</span>

    <span class="k">def</span> <span class="nf">token_freq</span><span class="p">(</span><span class="n">t</span><span class="p">):</span>
        <span class="sd">&quot;&quot;&quot;Token frequeny&quot;&quot;&quot;</span>

        <span class="k">return</span> <span class="mi">0</span> <span class="k">if</span> <span class="n">t</span> <span class="ow">in</span> <span class="n">extra_tokens</span> <span class="k">else</span> <span class="n">freq</span><span class="p">[</span><span class="n">t</span><span class="p">]</span>

    <span class="n">vocab</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="nb">zip</span><span class="p">(</span><span class="n">words</span><span class="p">,</span> <span class="nb">map</span><span class="p">(</span><span class="n">token_freq</span><span class="p">,</span> <span class="n">words</span><span class="p">)))</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Vocabulary created with </span><span class="si">{</span><span class="nb">len</span><span class="p">(</span><span class="n">vocab</span><span class="p">)</span><span class="si">}</span><span class="s2"> tokens.&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;The 10 most common tokens are:</span><span class="se">\n</span><span class="si">{</span><span class="n">freq</span><span class="o">.</span><span class="n">most_common</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">vocab</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.data.datasets"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.data.datasets.CorpusDataset" class="doc doc-heading">
        <code>CorpusDataset</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusDataset.__getitem__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Get a source and target token from the corpus</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>idx</code></td>
        <td><code>int</code></td>
        <td><p>Token position</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>(processed sentence, label)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a source and target token from the corpus</span>

<span class="sd">    Args:</span>
<span class="sd">        idx (int): Token position</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: (processed sentence, label)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">text</span><span class="p">,</span> <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">corpus</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span><span class="o">.</span><span class="n">transform</span><span class="p">([</span><span class="n">target</span><span class="p">])[</span><span class="mi">0</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="p">:</span>
        <span class="n">text</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">text</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">text</span><span class="p">,</span> <span class="n">target</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusDataset.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">labels</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Labeled corpus dataset</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>WordCorpus, HfCorpus etc..</code></td>
        <td><p>Input corpus</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>labels</code></td>
        <td><code>List[Any]</code></td>
        <td><p>Labels for examples</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">,</span> <span class="n">labels</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Labeled corpus dataset</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (WordCorpus, HfCorpus etc..): Input corpus</span>
<span class="sd">        labels (List[Any]): Labels for examples</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">corpus</span> <span class="o">=</span> <span class="n">corpus</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">labels</span> <span class="o">=</span> <span class="n">labels</span>
    <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span> <span class="o">==</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus</span><span class="p">),</span> <span class="s2">&quot;Incompatible labels and corpus&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">if</span> <span class="nb">isinstance</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">str</span><span class="p">):</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">label_encoder</span> <span class="o">=</span> <span class="n">LabelEncoder</span><span class="p">()</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">labels</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusDataset.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Length of corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>Corpus Length</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Length of corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Corpus Length</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">corpus</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusDataset.map" class="doc doc-heading">
<code class="highlight language-python"><span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Append a transform to self.transforms, in order to be applied to the data</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>t</code></td>
        <td><code>Callable[[str], Any]</code></td>
        <td><p>Transform of input token</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>CorpusDataset</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Append a transform to self.transforms, in order to be applied to the data</span>

<span class="sd">    Args:</span>
<span class="sd">        t (Callable[[str], Any]): Transform of input token</span>

<span class="sd">    Returns:</span>
<span class="sd">        CorpusDataset: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.datasets.CorpusLMDataset" class="doc doc-heading">
        <code>CorpusLMDataset</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusLMDataset.__getitem__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Get a source and target token from the corpus</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>idx</code></td>
        <td><code>int</code></td>
        <td><p>Token position</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>source=coprus[idx], target=corpus[idx+1]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__getitem__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">idx</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Get a source and target token from the corpus</span>

<span class="sd">    Args:</span>
<span class="sd">        idx (int): Token position</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: source=coprus[idx], target=corpus[idx+1]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">[</span><span class="n">idx</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">target</span><span class="p">[</span><span class="n">idx</span><span class="p">]</span>
    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="p">:</span>
        <span class="n">src</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">src</span><span class="p">)</span>
        <span class="n">tgt</span> <span class="o">=</span> <span class="n">t</span><span class="p">(</span><span class="n">tgt</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">src</span><span class="p">,</span> <span class="n">tgt</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusLMDataset.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Wraps a tokenized dataset which is provided as a list of tokens</p>
<p>Targets = source shifted one token to the left (next token prediction)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>corpus</code></td>
        <td><code>List[str] or WordCorpus</code></td>
        <td><p>List of tokens</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">corpus</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Wraps a tokenized dataset which is provided as a list of tokens</span>

<span class="sd">    Targets = source shifted one token to the left (next token prediction)</span>

<span class="sd">    Args:</span>
<span class="sd">        corpus (List[str] or WordCorpus): List of tokens</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">source</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[:</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">target</span> <span class="o">=</span> <span class="n">corpus</span><span class="p">[</span><span class="mi">1</span><span class="p">:]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span> <span class="o">=</span> <span class="p">[]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusLMDataset.__len__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Length of corpus</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>Corpus Length</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__len__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Length of corpus</span>

<span class="sd">    Returns:</span>
<span class="sd">        int: Corpus Length</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="nb">int</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">source</span><span class="p">))</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.datasets.CorpusLMDataset.map" class="doc doc-heading">
<code class="highlight language-python"><span class="nb">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Append a transform to self.transforms, in order to be applied to the data</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>t</code></td>
        <td><code>Callable[[str], Any]</code></td>
        <td><p>Transform of input token</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>CorpusLMDataset</code></td>
      <td><p>self</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/datasets.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">map</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Append a transform to self.transforms, in order to be applied to the data</span>

<span class="sd">    Args:</span>
<span class="sd">        t (Callable[[str], Any]): Transform of input token</span>

<span class="sd">    Returns:</span>
<span class="sd">        CorpusLMDataset: self</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">transforms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="k">return</span> <span class="bp">self</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.data.transforms"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.HuggingFaceTokenizer" class="doc doc-heading">
        <code>HuggingFaceTokenizer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.HuggingFaceTokenizer.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call to tokenize function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>str</code></td>
        <td><p>Input string</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of token ids</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Call to tokenize function</span>

<span class="sd">    Args:</span>
<span class="sd">        x (str): Input string</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of token ids</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode</span><span class="p">(</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">add_special_tokens</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="mi">65536</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.HuggingFaceTokenizer.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="s1">&#39;bert-base-uncased&#39;</span><span class="p">,</span> <span class="n">add_special_tokens</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Apply one of huggingface tokenizers to a string</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>lower</code></td>
        <td><code>bool</code></td>
        <td><p>Lowercase string. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>model</code></td>
        <td><code>str</code></td>
        <td><p>Select transformer model. Defaults to "bert-base-uncased".</p></td>
        <td><code>&#39;bert-base-uncased&#39;</code></td>
      </tr>
      <tr>
        <td><code>add_special_tokens</code></td>
        <td><code>bool</code></td>
        <td><p>Insert special tokens to tokenized string. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">lower</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;bert-base-uncased&quot;</span><span class="p">,</span>
    <span class="n">add_special_tokens</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply one of huggingface tokenizers to a string</span>

<span class="sd">    Args:</span>
<span class="sd">        lower (bool): Lowercase string. Defaults to True.</span>
<span class="sd">        model (str): Select transformer model. Defaults to &quot;bert-base-uncased&quot;.</span>
<span class="sd">        add_special_tokens (bool): Insert special tokens to tokenized string. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">AutoTokenizer</span><span class="o">.</span><span class="n">from_pretrained</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">do_lower_case</span><span class="o">=</span><span class="n">lower</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">vocab</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">add_special_tokens</span> <span class="o">=</span> <span class="n">add_special_tokens</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.HuggingFaceTokenizer.detokenize" class="doc doc-heading">
<code class="highlight language-python"><span class="n">detokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Convert list of token ids to list of tokens</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>List[int]</code></td>
        <td><p>List of token ids</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>List[str]: List of tokens</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">detokenize</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Convert list of token ids to list of tokens</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[int]): List of token ids</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[str]: List of tokens</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">convert_ids_to_tokens</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.ReplaceUnknownToken" class="doc doc-heading">
        <code>ReplaceUnknownToken</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ReplaceUnknownToken.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Convert <unk> in list of tokens to [UNK]</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>List[str]</code></td>
        <td><p>List of tokens</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>List[str]: List of tokens</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Convert &lt;unk&gt; in list of tokens to [UNK]</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[str]): List of tokens</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[str]: List of tokens</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">w</span> <span class="k">if</span> <span class="n">w</span> <span class="o">!=</span> <span class="bp">self</span><span class="o">.</span><span class="n">old_unk</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">new_unk</span> <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ReplaceUnknownToken.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">old_unk</span><span class="o">=</span><span class="s1">&#39;&lt;unk&gt;&#39;</span><span class="p">,</span> <span class="n">new_unk</span><span class="o">=</span><span class="s1">&#39;[UNK]&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Replace existing unknown tokens in the vocab to [UNK]. Useful for wikitext</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>old_unk</code></td>
        <td><code>str</code></td>
        <td><p>Unk token in corpus. Defaults to "<unk>".</p></td>
        <td><code>&#39;&lt;unk&gt;&#39;</code></td>
      </tr>
      <tr>
        <td><code>new_unk</code></td>
        <td><code>str</code></td>
        <td><p>Desired unk value. Defaults to SPECIAL_TOKENS.UNK.value.</p></td>
        <td><code>&#39;[UNK]&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">old_unk</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;&lt;unk&gt;&quot;</span><span class="p">,</span>
    <span class="n">new_unk</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="o">.</span><span class="n">UNK</span><span class="o">.</span><span class="n">value</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Replace existing unknown tokens in the vocab to [UNK]. Useful for wikitext</span>

<span class="sd">    Args:</span>
<span class="sd">        old_unk (str): Unk token in corpus. Defaults to &quot;&lt;unk&gt;&quot;.</span>
<span class="sd">        new_unk (str): Desired unk value. Defaults to SPECIAL_TOKENS.UNK.value.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">old_unk</span> <span class="o">=</span> <span class="n">old_unk</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">new_unk</span> <span class="o">=</span> <span class="n">new_unk</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.SentencepieceTokenizer" class="doc doc-heading">
        <code>SentencepieceTokenizer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.SentencepieceTokenizer.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call to tokenize function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>str</code></td>
        <td><p>Input string</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of tokens ids</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Call to tokenize function</span>

<span class="sd">    Args:</span>
<span class="sd">        x (str): Input string</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of tokens ids</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lower</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">ids</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">encode_as_ids</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span>
    <span class="k">return</span> <span class="n">ids</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.SentencepieceTokenizer.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">model</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">append_eos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">specials</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Tokenize sentence using pretrained sentencepiece model</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>lower</code></td>
        <td><code>bool</code></td>
        <td><p>Lowercase string. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>model</code></td>
        <td><code>Optional[Any]</code></td>
        <td><p>Sentencepiece model. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>prepend_bos</code></td>
        <td><code>bool</code></td>
        <td><p>Prepend BOS for seq2seq. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>append_eos</code></td>
        <td><code>bool</code></td>
        <td><p>Append EOS for seq2seq. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>specials</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens. Defaults to SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">lower</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">model</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Any</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">prepend_bos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">append_eos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">specials</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Tokenize sentence using pretrained sentencepiece model</span>

<span class="sd">    Args:</span>
<span class="sd">        lower (bool): Lowercase string. Defaults to True.</span>
<span class="sd">        model (Optional[Any]): Sentencepiece model. Defaults to None.</span>
<span class="sd">        prepend_bos (bool): Prepend BOS for seq2seq. Defaults to False.</span>
<span class="sd">        append_eos (bool): Append EOS for seq2seq. Defaults to False.</span>
<span class="sd">        specials (Optional[SPECIAL_TOKENS]): Special tokens. Defaults to SPECIAL_TOKENS.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span> <span class="o">=</span> <span class="n">spm</span><span class="o">.</span><span class="n">SentencePieceProcessor</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">Load</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">specials</span> <span class="o">=</span> <span class="n">specials</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lower</span> <span class="o">=</span> <span class="n">lower</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">vocab_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">get_piece_size</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">prepend_bos</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">piece_to_id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">specials</span><span class="o">.</span><span class="n">BOS</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
    <span class="k">if</span> <span class="n">append_eos</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">piece_to_id</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">specials</span><span class="o">.</span><span class="n">EOS</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.SpacyTokenizer" class="doc doc-heading">
        <code>SpacyTokenizer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.SpacyTokenizer.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Call to tokenize function</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>str</code></td>
        <td><p>Input string</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[str]</code></td>
      <td><p>List[str]: List of tokens</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="nb">str</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Call to tokenize function</span>

<span class="sd">    Args:</span>
<span class="sd">        x (str): Input string</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[str]: List of tokens</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">lower</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span> <span class="o">+</span> <span class="p">[</span><span class="n">y</span><span class="o">.</span><span class="n">text</span> <span class="k">for</span> <span class="n">y</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">nlp</span><span class="o">.</span><span class="n">tokenizer</span><span class="p">(</span><span class="n">x</span><span class="p">)]</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span>
    <span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.SpacyTokenizer.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">lower</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">prepend_bos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">append_eos</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">specials</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">,</span> <span class="n">lang</span><span class="o">=</span><span class="s1">&#39;en_core_web_sm&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Apply spacy tokenizer to str</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>lower</code></td>
        <td><code>bool</code></td>
        <td><p>Lowercase string. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>prepend_bos</code></td>
        <td><code>bool</code></td>
        <td><p>Prepend BOS for seq2seq. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>append_eos</code></td>
        <td><code>bool</code></td>
        <td><p>Append EOS for seq2seq. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>specials</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens. Defaults to SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
      <tr>
        <td><code>lang</code></td>
        <td><code>str</code></td>
        <td><p>Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to "en_core_web_md".</p></td>
        <td><code>&#39;en_core_web_sm&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">lower</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">prepend_bos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">append_eos</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">specials</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
    <span class="n">lang</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Apply spacy tokenizer to str</span>

<span class="sd">    Args:</span>
<span class="sd">        lower (bool): Lowercase string. Defaults to True.</span>
<span class="sd">        prepend_bos (bool): Prepend BOS for seq2seq. Defaults to False.</span>
<span class="sd">        append_eos (bool): Append EOS for seq2seq. Defaults to False.</span>
<span class="sd">        specials (Optional[SPECIAL_TOKENS]): Special tokens. Defaults to SPECIAL_TOKENS.</span>
<span class="sd">        lang (str): Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to &quot;en_core_web_md&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lower</span> <span class="o">=</span> <span class="n">lower</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">specials</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">lang</span> <span class="o">=</span> <span class="n">lang</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">if</span> <span class="n">prepend_bos</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pre_id</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">specials</span><span class="o">.</span><span class="n">BOS</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">append_eos</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">post_id</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">specials</span><span class="o">.</span><span class="n">EOS</span><span class="o">.</span><span class="n">value</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nlp</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">get_nlp</span><span class="p">(</span><span class="n">name</span><span class="o">=</span><span class="n">lang</span><span class="p">,</span> <span class="n">specials</span><span class="o">=</span><span class="n">specials</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.SpacyTokenizer.get_nlp" class="doc doc-heading">
<code class="highlight language-python"><span class="n">get_nlp</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s1">&#39;en_core_web_sm&#39;</span><span class="p">,</span> <span class="n">specials</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Get spacy nlp object for given lang and add SPECIAL_TOKENS</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>name</code></td>
        <td><code>str</code></td>
        <td><p>Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to "en_core_web_md".</p></td>
        <td><code>&#39;en_core_web_sm&#39;</code></td>
      </tr>
      <tr>
        <td><code>specials</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens. Defaults to SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Language</code></td>
      <td><p>spacy.Language: spacy text-processing pipeline</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">get_nlp</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">name</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;en_core_web_sm&quot;</span><span class="p">,</span>
    <span class="n">specials</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">spacy</span><span class="o">.</span><span class="n">Language</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Get spacy nlp object for given lang and add SPECIAL_TOKENS</span>

<span class="sd">    Args:</span>
<span class="sd">        name (str): Spacy language, e.g. el_core_web_sm, en_core_web_sm etc. Defaults to &quot;en_core_web_md&quot;.</span>
<span class="sd">        specials (Optional[SPECIAL_TOKENS]): Special tokens. Defaults to SPECIAL_TOKENS.</span>

<span class="sd">    Returns:</span>
<span class="sd">        spacy.Language: spacy text-processing pipeline</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">nlp</span> <span class="o">=</span> <span class="n">spacy</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">name</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">specials</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">for</span> <span class="n">token</span> <span class="ow">in</span> <span class="n">specials</span><span class="o">.</span><span class="n">to_list</span><span class="p">():</span>
            <span class="n">nlp</span><span class="o">.</span><span class="n">tokenizer</span><span class="o">.</span><span class="n">add_special_case</span><span class="p">(</span><span class="n">token</span><span class="p">,</span> <span class="p">[{</span><span class="n">ORTH</span><span class="p">:</span> <span class="n">token</span><span class="p">}])</span>
    <span class="k">return</span> <span class="n">nlp</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.ToTensor" class="doc doc-heading">
        <code>ToTensor</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ToTensor.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Convert list of tokens or list of features to tensor</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>List[Any]</code></td>
        <td><p>List of tokens or features</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Resulting tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">Any</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Convert list of tokens or list of features to tensor</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[Any]): List of tokens or features</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Resulting tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">mktensor</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">device</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dtype</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ToTensor.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="o">=</span><span class="s1">&#39;cpu&#39;</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">int64</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>To tensor convertor</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>device</code></td>
        <td><code>str</code></td>
        <td><p>Device to map the tensor. Defaults to "cpu".</p></td>
        <td><code>&#39;cpu&#39;</code></td>
      </tr>
      <tr>
        <td><code>dtype</code></td>
        <td><code>dtype</code></td>
        <td><p>Type of resulting tensor. Defaults to torch.long.</p></td>
        <td><code>torch.int64</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">device</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cpu&quot;</span><span class="p">,</span> <span class="n">dtype</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">long</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;To tensor convertor</span>

<span class="sd">    Args:</span>
<span class="sd">        device (str): Device to map the tensor. Defaults to &quot;cpu&quot;.</span>
<span class="sd">        dtype (torch.dtype): Type of resulting tensor. Defaults to torch.long.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">device</span> <span class="o">=</span> <span class="n">device</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dtype</span> <span class="o">=</span> <span class="n">dtype</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.data.transforms.ToTokenIds" class="doc doc-heading">
        <code>ToTokenIds</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ToTokenIds.__call__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Convert list of tokens to list of token ids</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>List[str]</code></td>
        <td><p>List of tokens</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[int]</code></td>
      <td><p>List[int]: List of token ids</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__call__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">str</span><span class="p">])</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Convert list of tokens to list of token ids</span>

<span class="sd">    Args:</span>
<span class="sd">        x (List[str]): List of tokens</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[int]: List of token ids</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="n">w</span><span class="p">]</span> <span class="k">if</span> <span class="n">w</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span><span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">unk_value</span><span class="p">]</span>
        <span class="k">for</span> <span class="n">w</span> <span class="ow">in</span> <span class="n">x</span>
    <span class="p">]</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.data.transforms.ToTokenIds.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">word2idx</span><span class="p">,</span> <span class="n">specials</span><span class="o">=&lt;</span><span class="n">enum</span> <span class="s1">&#39;SPECIAL_TOKENS&#39;</span><span class="o">&gt;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Convert List of tokens to list of token ids</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>word2idx</code></td>
        <td><code>Dict[str, int]</code></td>
        <td><p>Word to index mapping</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>specials</code></td>
        <td><code>Optional[slp.config.nlp.SPECIAL_TOKENS]</code></td>
        <td><p>Special tokens. Defaults to SPECIAL_TOKENS.</p></td>
        <td><code>&lt;enum &#39;SPECIAL_TOKENS&#39;&gt;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/data/transforms.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">word2idx</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="nb">int</span><span class="p">],</span>
    <span class="n">specials</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">SPECIAL_TOKENS</span><span class="p">]</span> <span class="o">=</span> <span class="n">SPECIAL_TOKENS</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Convert List of tokens to list of token ids</span>

<span class="sd">    Args:</span>
<span class="sd">        word2idx (Dict[str, int]): Word to index mapping</span>
<span class="sd">        specials (Optional[SPECIAL_TOKENS]): Special tokens. Defaults to SPECIAL_TOKENS.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">word2idx</span> <span class="o">=</span> <span class="n">word2idx</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">unk_value</span> <span class="o">=</span> <span class="n">specials</span><span class="o">.</span><span class="n">UNK</span><span class="o">.</span><span class="n">value</span> <span class="k">if</span> <span class="n">specials</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="s2">&quot;[UNK]&quot;</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="../config/" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Configuration
            </div>
          </div>
        </a>
      
      
        <a href="../plbind/" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Pytorch Lightning Bindings
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../assets/javascripts/workers/search.fb4a9340.min.js", "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.5cf3e710.min.js"></script>
      
        <script src="../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>