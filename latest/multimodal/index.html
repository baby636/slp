
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.0.5">
    
    
      
        <title>Multimodal Modules - slp Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.77f3fd56.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.7fa14f5b.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#multimodal-modules" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="slp Documentation" class="md-header__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            slp Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Multimodal Modules
            
          </span>
        </div>
      </div>
    </div>
    <div class="md-header__options">
      
    </div>
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="slp Documentation" class="md-nav__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    slp Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        slp
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../get-started/" class="md-nav__link">
        Getting started
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../tuning/" class="md-nav__link">
        Hyperparameter tuning
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../config/" class="md-nav__link">
        Configuration
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../data-utils/" class="md-nav__link">
        Data manipulation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../plbind/" class="md-nav__link">
        Pytorch Lightning Bindings
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../modules/" class="md-nav__link">
        Generic Modules
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Multimodal Modules
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Multimodal Modules
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#fusers" class="md-nav__link">
    Fusers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse" class="md-nav__link">
    slp.modules.fuse
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.SUPPORTED_FUSERS" class="md-nav__link">
    SUPPORTED_FUSERS
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.SUPPORTED_POOLERS" class="md-nav__link">
    SUPPORTED_POOLERS
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.AttentionFuser" class="md-nav__link">
    AttentionFuser
  </a>
  
    <nav class="md-nav" aria-label="AttentionFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.AttentionFuser.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.AttentionFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFuser" class="md-nav__link">
    BaseFuser
  </a>
  
    <nav class="md-nav" aria-label="BaseFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFuser.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFuser.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFuser.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFusionPipeline" class="md-nav__link">
    BaseFusionPipeline
  </a>
  
    <nav class="md-nav" aria-label="BaseFusionPipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFusionPipeline.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFusionPipeline.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseTimestepsPooler" class="md-nav__link">
    BaseTimestepsPooler
  </a>
  
    <nav class="md-nav" aria-label="BaseTimestepsPooler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseTimestepsPooler.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseTimestepsPooler.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseTimestepsPooler.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BilinearFuser" class="md-nav__link">
    BilinearFuser
  </a>
  
    <nav class="md-nav" aria-label="BilinearFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BilinearFuser.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BilinearFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalAttentionFuser" class="md-nav__link">
    BimodalAttentionFuser
  </a>
  
    <nav class="md-nav" aria-label="BimodalAttentionFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalAttentionFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalBilinearFuser" class="md-nav__link">
    BimodalBilinearFuser
  </a>
  
    <nav class="md-nav" aria-label="BimodalBilinearFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalBilinearFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalCombinatorialFuser" class="md-nav__link">
    BimodalCombinatorialFuser
  </a>
  
    <nav class="md-nav" aria-label="BimodalCombinatorialFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalCombinatorialFuser.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalCombinatorialFuser.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.CatFuser" class="md-nav__link">
    CatFuser
  </a>
  
    <nav class="md-nav" aria-label="CatFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.CatFuser.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.CatFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection" class="md-nav__link">
    Conv1dProjection
  </a>
  
    <nav class="md-nav" aria-label="Conv1dProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--inputs" class="md-nav__link">
    Inputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--text-b-l-300" class="md-nav__link">
    text: (B, L, 300)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--audio-b-l-74" class="md-nav__link">
    audio: (B, L, 74)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--visual-b-l-35" class="md-nav__link">
    visual: (B, L, 35)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--outputs" class="md-nav__link">
    Outputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--text_p-b-l-100" class="md-nav__link">
    text_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--audio_p-b-l-100" class="md-nav__link">
    audio_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--visual_p-b-l-100" class="md-nav__link">
    visual_p: (B, L, 100)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.FuseAggregateTimesteps" class="md-nav__link">
    FuseAggregateTimesteps
  </a>
  
    <nav class="md-nav" aria-label="FuseAggregateTimesteps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.FuseAggregateTimesteps.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.FuseAggregateTimesteps.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.FuseAggregateTimesteps.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection" class="md-nav__link">
    LinearProjection
  </a>
  
    <nav class="md-nav" aria-label="LinearProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--inputs" class="md-nav__link">
    Inputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--text-b-l-300" class="md-nav__link">
    text: (B, L, 300)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--audio-b-l-74" class="md-nav__link">
    audio: (B, L, 74)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--visual-b-l-35" class="md-nav__link">
    visual: (B, L, 35)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--outputs" class="md-nav__link">
    Outputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--text_p-b-l-100" class="md-nav__link">
    text_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--audio_p-b-l-100" class="md-nav__link">
    audio_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--visual_p-b-l-100" class="md-nav__link">
    visual_p: (B, L, 100)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection" class="md-nav__link">
    ModalityProjection
  </a>
  
    <nav class="md-nav" aria-label="ModalityProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--inputs" class="md-nav__link">
    Inputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--text-b-l-300" class="md-nav__link">
    text: (B, L, 300)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--audio-b-l-74" class="md-nav__link">
    audio: (B, L, 74)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--visual-b-l-35" class="md-nav__link">
    visual: (B, L, 35)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--outputs" class="md-nav__link">
    Outputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--text_p-b-l-100" class="md-nav__link">
    text_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--audio_p-b-l-100" class="md-nav__link">
    audio_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--visual_p-b-l-100" class="md-nav__link">
    visual_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--inputs" class="md-nav__link">
    Inputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--text-b-l-300" class="md-nav__link">
    text: (B, L, 300)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--audio-b-l-74" class="md-nav__link">
    audio: (B, L, 74)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--visual-b-l-35" class="md-nav__link">
    visual: (B, L, 35)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--outputs" class="md-nav__link">
    Outputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--text_p-b-l-300" class="md-nav__link">
    text_p: (B, L, 300)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--audio_p-b-l-74" class="md-nav__link">
    audio_p: (B, L, 74)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--visual_p-b-l-35" class="md-nav__link">
    visual_p: (B, L, 35)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights" class="md-nav__link">
    ModalityWeights
  </a>
  
    <nav class="md-nav" aria-label="ModalityWeights">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--inputs" class="md-nav__link">
    Inputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--text-b-l-100" class="md-nav__link">
    text: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--audio-b-l-100" class="md-nav__link">
    audio: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--visual-b-l-100" class="md-nav__link">
    visual: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--outputs" class="md-nav__link">
    Outputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--text_p-b-l-100" class="md-nav__link">
    text_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--audio_p-b-l-100" class="md-nav__link">
    audio_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--visual_p-b-l-100" class="md-nav__link">
    visual_p: (B, L, 100)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.ProjectFuseAggregate" class="md-nav__link">
    ProjectFuseAggregate
  </a>
  
    <nav class="md-nav" aria-label="ProjectFuseAggregate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ProjectFuseAggregate.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ProjectFuseAggregate.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ProjectFuseAggregate.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.RnnPooler" class="md-nav__link">
    RnnPooler
  </a>
  
    <nav class="md-nav" aria-label="RnnPooler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.RnnPooler.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.RnnPooler.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.SumFuser" class="md-nav__link">
    SumFuser
  </a>
  
    <nav class="md-nav" aria-label="SumFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.SumFuser.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.SumFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.TimestepsPooler" class="md-nav__link">
    TimestepsPooler
  </a>
  
    <nav class="md-nav" aria-label="TimestepsPooler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.TimestepsPooler.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.TimestepsPooler.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.TrimodalCombinatorialFuser" class="md-nav__link">
    TrimodalCombinatorialFuser
  </a>
  
    <nav class="md-nav" aria-label="TrimodalCombinatorialFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.TrimodalCombinatorialFuser.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.TrimodalCombinatorialFuser.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.make_fuser" class="md-nav__link">
    make_fuser()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multimodal-encoders" class="md-nav__link">
    Multimodal encoders
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal" class="md-nav__link">
    slp.modules.multimodal
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.AudioEncoder" class="md-nav__link">
    AudioEncoder
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.AudioTextClassifier" class="md-nav__link">
    AudioTextClassifier
  </a>
  
    <nav class="md-nav" aria-label="AudioTextClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.AudioTextClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.AudioVisualClassifier" class="md-nav__link">
    AudioVisualClassifier
  </a>
  
    <nav class="md-nav" aria-label="AudioVisualClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.AudioVisualClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BaseEncoder" class="md-nav__link">
    BaseEncoder
  </a>
  
    <nav class="md-nav" aria-label="BaseEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BaseEncoder.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BaseEncoder.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BaseEncoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BimodalEncoder" class="md-nav__link">
    BimodalEncoder
  </a>
  
    <nav class="md-nav" aria-label="BimodalEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BimodalEncoder.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BimodalEncoder.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.GloveEncoder" class="md-nav__link">
    GloveEncoder
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MOSEIClassifier" class="md-nav__link">
    MOSEIClassifier
  </a>
  
    <nav class="md-nav" aria-label="MOSEIClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MOSEIClassifier.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaseline" class="md-nav__link">
    MultimodalBaseline
  </a>
  
    <nav class="md-nav" aria-label="MultimodalBaseline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaseline.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaseline.encoder_cfg" class="md-nav__link">
    encoder_cfg()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaseline.fuser_cfg" class="md-nav__link">
    fuser_cfg()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaselineClassifier" class="md-nav__link">
    MultimodalBaselineClassifier
  </a>
  
    <nav class="md-nav" aria-label="MultimodalBaselineClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaselineClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.TrimodalClassifier" class="md-nav__link">
    TrimodalClassifier
  </a>
  
    <nav class="md-nav" aria-label="TrimodalClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.TrimodalClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.TrimodalEncoder" class="md-nav__link">
    TrimodalEncoder
  </a>
  
    <nav class="md-nav" aria-label="TrimodalEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.TrimodalEncoder.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.TrimodalEncoder.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalClassifier" class="md-nav__link">
    UnimodalClassifier
  </a>
  
    <nav class="md-nav" aria-label="UnimodalClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalClassifier.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalEncoder" class="md-nav__link">
    UnimodalEncoder
  </a>
  
    <nav class="md-nav" aria-label="UnimodalEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalEncoder.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalEncoder.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.VisualEncoder" class="md-nav__link">
    VisualEncoder
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.VisualTextClassifier" class="md-nav__link">
    VisualTextClassifier
  </a>
  
    <nav class="md-nav" aria-label="VisualTextClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.VisualTextClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#m3" class="md-nav__link">
    M3
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmdrop" class="md-nav__link">
    slp.modules.mmdrop
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.HardMultimodalDropout" class="md-nav__link">
    HardMultimodalDropout
  </a>
  
    <nav class="md-nav" aria-label="HardMultimodalDropout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.HardMultimodalDropout.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.HardMultimodalDropout.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.MultimodalDropout" class="md-nav__link">
    MultimodalDropout
  </a>
  
    <nav class="md-nav" aria-label="MultimodalDropout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.MultimodalDropout.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.MultimodalDropout.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.SoftMultimodalDropout" class="md-nav__link">
    SoftMultimodalDropout
  </a>
  
    <nav class="md-nav" aria-label="SoftMultimodalDropout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.SoftMultimodalDropout.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.SoftMultimodalDropout.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.m3" class="md-nav__link">
    slp.modules.m3
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.m3.M3" class="md-nav__link">
    M3
  </a>
  
    <nav class="md-nav" aria-label="M3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3.encoder_cfg" class="md-nav__link">
    encoder_cfg()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3.fuser_cfg" class="md-nav__link">
    fuser_cfg()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.m3.M3Classifier" class="md-nav__link">
    M3Classifier
  </a>
  
    <nav class="md-nav" aria-label="M3Classifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3Classifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.m3.M3FuseAggregate" class="md-nav__link">
    M3FuseAggregate
  </a>
  
    <nav class="md-nav" aria-label="M3FuseAggregate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3FuseAggregate.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3FuseAggregate.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3FuseAggregate.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multimodal-feedback" class="md-nav__link">
    Multimodal Feedback
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback" class="md-nav__link">
    slp.modules.feedback
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.BaseFeedbackUnit" class="md-nav__link">
    BaseFeedbackUnit
  </a>
  
    <nav class="md-nav" aria-label="BaseFeedbackUnit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.BaseFeedbackUnit.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.BaseFeedbackUnit.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.BaseFeedbackUnit.make_mask_layer" class="md-nav__link">
    make_mask_layer()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.BoomFeedbackUnit" class="md-nav__link">
    BoomFeedbackUnit
  </a>
  
    <nav class="md-nav" aria-label="BoomFeedbackUnit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.BoomFeedbackUnit.make_mask_layer" class="md-nav__link">
    make_mask_layer()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.DownUpFeedbackUnit" class="md-nav__link">
    DownUpFeedbackUnit
  </a>
  
    <nav class="md-nav" aria-label="DownUpFeedbackUnit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.DownUpFeedbackUnit.make_mask_layer" class="md-nav__link">
    make_mask_layer()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.Feedback" class="md-nav__link">
    Feedback
  </a>
  
    <nav class="md-nav" aria-label="Feedback">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.Feedback.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.Feedback.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.GatedFeedbackUnit" class="md-nav__link">
    GatedFeedbackUnit
  </a>
  
    <nav class="md-nav" aria-label="GatedFeedbackUnit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.GatedFeedbackUnit.make_mask_layer" class="md-nav__link">
    make_mask_layer()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.RNNFeedbackUnit" class="md-nav__link">
    RNNFeedbackUnit
  </a>
  
    <nav class="md-nav" aria-label="RNNFeedbackUnit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.RNNFeedbackUnit.make_mask_layer" class="md-nav__link">
    make_mask_layer()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmlatch" class="md-nav__link">
    slp.modules.mmlatch
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatch" class="md-nav__link">
    MMLatch
  </a>
  
    <nav class="md-nav" aria-label="MMLatch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatch.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatch.encoder_cfg" class="md-nav__link">
    encoder_cfg()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatch.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatch.fuser_cfg" class="md-nav__link">
    fuser_cfg()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatchClassifier" class="md-nav__link">
    MMLatchClassifier
  </a>
  
    <nav class="md-nav" aria-label="MMLatchClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatchClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        SLP utility functions
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../apiref/" class="md-nav__link">
        API reference
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#fusers" class="md-nav__link">
    Fusers
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse" class="md-nav__link">
    slp.modules.fuse
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.SUPPORTED_FUSERS" class="md-nav__link">
    SUPPORTED_FUSERS
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.SUPPORTED_POOLERS" class="md-nav__link">
    SUPPORTED_POOLERS
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.AttentionFuser" class="md-nav__link">
    AttentionFuser
  </a>
  
    <nav class="md-nav" aria-label="AttentionFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.AttentionFuser.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.AttentionFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFuser" class="md-nav__link">
    BaseFuser
  </a>
  
    <nav class="md-nav" aria-label="BaseFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFuser.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFuser.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFuser.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFusionPipeline" class="md-nav__link">
    BaseFusionPipeline
  </a>
  
    <nav class="md-nav" aria-label="BaseFusionPipeline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFusionPipeline.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseFusionPipeline.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseTimestepsPooler" class="md-nav__link">
    BaseTimestepsPooler
  </a>
  
    <nav class="md-nav" aria-label="BaseTimestepsPooler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseTimestepsPooler.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseTimestepsPooler.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BaseTimestepsPooler.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BilinearFuser" class="md-nav__link">
    BilinearFuser
  </a>
  
    <nav class="md-nav" aria-label="BilinearFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BilinearFuser.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BilinearFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalAttentionFuser" class="md-nav__link">
    BimodalAttentionFuser
  </a>
  
    <nav class="md-nav" aria-label="BimodalAttentionFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalAttentionFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalBilinearFuser" class="md-nav__link">
    BimodalBilinearFuser
  </a>
  
    <nav class="md-nav" aria-label="BimodalBilinearFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalBilinearFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalCombinatorialFuser" class="md-nav__link">
    BimodalCombinatorialFuser
  </a>
  
    <nav class="md-nav" aria-label="BimodalCombinatorialFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalCombinatorialFuser.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.BimodalCombinatorialFuser.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.CatFuser" class="md-nav__link">
    CatFuser
  </a>
  
    <nav class="md-nav" aria-label="CatFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.CatFuser.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.CatFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection" class="md-nav__link">
    Conv1dProjection
  </a>
  
    <nav class="md-nav" aria-label="Conv1dProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--inputs" class="md-nav__link">
    Inputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--text-b-l-300" class="md-nav__link">
    text: (B, L, 300)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--audio-b-l-74" class="md-nav__link">
    audio: (B, L, 74)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--visual-b-l-35" class="md-nav__link">
    visual: (B, L, 35)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--outputs" class="md-nav__link">
    Outputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--text_p-b-l-100" class="md-nav__link">
    text_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--audio_p-b-l-100" class="md-nav__link">
    audio_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.Conv1dProjection.forward--visual_p-b-l-100" class="md-nav__link">
    visual_p: (B, L, 100)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.FuseAggregateTimesteps" class="md-nav__link">
    FuseAggregateTimesteps
  </a>
  
    <nav class="md-nav" aria-label="FuseAggregateTimesteps">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.FuseAggregateTimesteps.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.FuseAggregateTimesteps.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.FuseAggregateTimesteps.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection" class="md-nav__link">
    LinearProjection
  </a>
  
    <nav class="md-nav" aria-label="LinearProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--inputs" class="md-nav__link">
    Inputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--text-b-l-300" class="md-nav__link">
    text: (B, L, 300)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--audio-b-l-74" class="md-nav__link">
    audio: (B, L, 74)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--visual-b-l-35" class="md-nav__link">
    visual: (B, L, 35)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--outputs" class="md-nav__link">
    Outputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--text_p-b-l-100" class="md-nav__link">
    text_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--audio_p-b-l-100" class="md-nav__link">
    audio_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.LinearProjection.forward--visual_p-b-l-100" class="md-nav__link">
    visual_p: (B, L, 100)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection" class="md-nav__link">
    ModalityProjection
  </a>
  
    <nav class="md-nav" aria-label="ModalityProjection">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--inputs" class="md-nav__link">
    Inputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--text-b-l-300" class="md-nav__link">
    text: (B, L, 300)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--audio-b-l-74" class="md-nav__link">
    audio: (B, L, 74)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--visual-b-l-35" class="md-nav__link">
    visual: (B, L, 35)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--outputs" class="md-nav__link">
    Outputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--text_p-b-l-100" class="md-nav__link">
    text_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--audio_p-b-l-100" class="md-nav__link">
    audio_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--visual_p-b-l-100" class="md-nav__link">
    visual_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--inputs" class="md-nav__link">
    Inputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--text-b-l-300" class="md-nav__link">
    text: (B, L, 300)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--audio-b-l-74" class="md-nav__link">
    audio: (B, L, 74)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--visual-b-l-35" class="md-nav__link">
    visual: (B, L, 35)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--outputs" class="md-nav__link">
    Outputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--text_p-b-l-300" class="md-nav__link">
    text_p: (B, L, 300)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--audio_p-b-l-74" class="md-nav__link">
    audio_p: (B, L, 74)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityProjection.forward--visual_p-b-l-35" class="md-nav__link">
    visual_p: (B, L, 35)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights" class="md-nav__link">
    ModalityWeights
  </a>
  
    <nav class="md-nav" aria-label="ModalityWeights">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward" class="md-nav__link">
    forward()
  </a>
  
    <nav class="md-nav" aria-label="forward()">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--inputs" class="md-nav__link">
    Inputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--text-b-l-100" class="md-nav__link">
    text: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--audio-b-l-100" class="md-nav__link">
    audio: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--visual-b-l-100" class="md-nav__link">
    visual: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--outputs" class="md-nav__link">
    Outputs:
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--text_p-b-l-100" class="md-nav__link">
    text_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--audio_p-b-l-100" class="md-nav__link">
    audio_p: (B, L, 100)
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ModalityWeights.forward--visual_p-b-l-100" class="md-nav__link">
    visual_p: (B, L, 100)
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.ProjectFuseAggregate" class="md-nav__link">
    ProjectFuseAggregate
  </a>
  
    <nav class="md-nav" aria-label="ProjectFuseAggregate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ProjectFuseAggregate.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ProjectFuseAggregate.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.ProjectFuseAggregate.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.RnnPooler" class="md-nav__link">
    RnnPooler
  </a>
  
    <nav class="md-nav" aria-label="RnnPooler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.RnnPooler.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.RnnPooler.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.SumFuser" class="md-nav__link">
    SumFuser
  </a>
  
    <nav class="md-nav" aria-label="SumFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.SumFuser.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.SumFuser.fuse" class="md-nav__link">
    fuse()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.TimestepsPooler" class="md-nav__link">
    TimestepsPooler
  </a>
  
    <nav class="md-nav" aria-label="TimestepsPooler">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.TimestepsPooler.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.TimestepsPooler.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.TrimodalCombinatorialFuser" class="md-nav__link">
    TrimodalCombinatorialFuser
  </a>
  
    <nav class="md-nav" aria-label="TrimodalCombinatorialFuser">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.TrimodalCombinatorialFuser.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.fuse.TrimodalCombinatorialFuser.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.fuse.make_fuser" class="md-nav__link">
    make_fuser()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multimodal-encoders" class="md-nav__link">
    Multimodal encoders
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal" class="md-nav__link">
    slp.modules.multimodal
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.AudioEncoder" class="md-nav__link">
    AudioEncoder
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.AudioTextClassifier" class="md-nav__link">
    AudioTextClassifier
  </a>
  
    <nav class="md-nav" aria-label="AudioTextClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.AudioTextClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.AudioVisualClassifier" class="md-nav__link">
    AudioVisualClassifier
  </a>
  
    <nav class="md-nav" aria-label="AudioVisualClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.AudioVisualClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BaseEncoder" class="md-nav__link">
    BaseEncoder
  </a>
  
    <nav class="md-nav" aria-label="BaseEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BaseEncoder.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BaseEncoder.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BaseEncoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BimodalEncoder" class="md-nav__link">
    BimodalEncoder
  </a>
  
    <nav class="md-nav" aria-label="BimodalEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BimodalEncoder.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.BimodalEncoder.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.GloveEncoder" class="md-nav__link">
    GloveEncoder
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MOSEIClassifier" class="md-nav__link">
    MOSEIClassifier
  </a>
  
    <nav class="md-nav" aria-label="MOSEIClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MOSEIClassifier.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaseline" class="md-nav__link">
    MultimodalBaseline
  </a>
  
    <nav class="md-nav" aria-label="MultimodalBaseline">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaseline.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaseline.encoder_cfg" class="md-nav__link">
    encoder_cfg()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaseline.fuser_cfg" class="md-nav__link">
    fuser_cfg()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaselineClassifier" class="md-nav__link">
    MultimodalBaselineClassifier
  </a>
  
    <nav class="md-nav" aria-label="MultimodalBaselineClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.MultimodalBaselineClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.TrimodalClassifier" class="md-nav__link">
    TrimodalClassifier
  </a>
  
    <nav class="md-nav" aria-label="TrimodalClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.TrimodalClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.TrimodalEncoder" class="md-nav__link">
    TrimodalEncoder
  </a>
  
    <nav class="md-nav" aria-label="TrimodalEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.TrimodalEncoder.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.TrimodalEncoder.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalClassifier" class="md-nav__link">
    UnimodalClassifier
  </a>
  
    <nav class="md-nav" aria-label="UnimodalClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalClassifier.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalEncoder" class="md-nav__link">
    UnimodalEncoder
  </a>
  
    <nav class="md-nav" aria-label="UnimodalEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalEncoder.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.UnimodalEncoder.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.VisualEncoder" class="md-nav__link">
    VisualEncoder
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.multimodal.VisualTextClassifier" class="md-nav__link">
    VisualTextClassifier
  </a>
  
    <nav class="md-nav" aria-label="VisualTextClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.multimodal.VisualTextClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#m3" class="md-nav__link">
    M3
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmdrop" class="md-nav__link">
    slp.modules.mmdrop
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.HardMultimodalDropout" class="md-nav__link">
    HardMultimodalDropout
  </a>
  
    <nav class="md-nav" aria-label="HardMultimodalDropout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.HardMultimodalDropout.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.HardMultimodalDropout.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.MultimodalDropout" class="md-nav__link">
    MultimodalDropout
  </a>
  
    <nav class="md-nav" aria-label="MultimodalDropout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.MultimodalDropout.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.MultimodalDropout.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.SoftMultimodalDropout" class="md-nav__link">
    SoftMultimodalDropout
  </a>
  
    <nav class="md-nav" aria-label="SoftMultimodalDropout">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.SoftMultimodalDropout.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmdrop.SoftMultimodalDropout.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.m3" class="md-nav__link">
    slp.modules.m3
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.m3.M3" class="md-nav__link">
    M3
  </a>
  
    <nav class="md-nav" aria-label="M3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3.encoder_cfg" class="md-nav__link">
    encoder_cfg()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3.fuser_cfg" class="md-nav__link">
    fuser_cfg()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.m3.M3Classifier" class="md-nav__link">
    M3Classifier
  </a>
  
    <nav class="md-nav" aria-label="M3Classifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3Classifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.m3.M3FuseAggregate" class="md-nav__link">
    M3FuseAggregate
  </a>
  
    <nav class="md-nav" aria-label="M3FuseAggregate">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3FuseAggregate.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3FuseAggregate.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.m3.M3FuseAggregate.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#multimodal-feedback" class="md-nav__link">
    Multimodal Feedback
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback" class="md-nav__link">
    slp.modules.feedback
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.BaseFeedbackUnit" class="md-nav__link">
    BaseFeedbackUnit
  </a>
  
    <nav class="md-nav" aria-label="BaseFeedbackUnit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.BaseFeedbackUnit.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.BaseFeedbackUnit.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.BaseFeedbackUnit.make_mask_layer" class="md-nav__link">
    make_mask_layer()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.BoomFeedbackUnit" class="md-nav__link">
    BoomFeedbackUnit
  </a>
  
    <nav class="md-nav" aria-label="BoomFeedbackUnit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.BoomFeedbackUnit.make_mask_layer" class="md-nav__link">
    make_mask_layer()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.DownUpFeedbackUnit" class="md-nav__link">
    DownUpFeedbackUnit
  </a>
  
    <nav class="md-nav" aria-label="DownUpFeedbackUnit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.DownUpFeedbackUnit.make_mask_layer" class="md-nav__link">
    make_mask_layer()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.Feedback" class="md-nav__link">
    Feedback
  </a>
  
    <nav class="md-nav" aria-label="Feedback">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.Feedback.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.Feedback.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.GatedFeedbackUnit" class="md-nav__link">
    GatedFeedbackUnit
  </a>
  
    <nav class="md-nav" aria-label="GatedFeedbackUnit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.GatedFeedbackUnit.make_mask_layer" class="md-nav__link">
    make_mask_layer()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedback.RNNFeedbackUnit" class="md-nav__link">
    RNNFeedbackUnit
  </a>
  
    <nav class="md-nav" aria-label="RNNFeedbackUnit">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedback.RNNFeedbackUnit.make_mask_layer" class="md-nav__link">
    make_mask_layer()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmlatch" class="md-nav__link">
    slp.modules.mmlatch
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatch" class="md-nav__link">
    MMLatch
  </a>
  
    <nav class="md-nav" aria-label="MMLatch">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatch.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatch.encoder_cfg" class="md-nav__link">
    encoder_cfg()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatch.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatch.fuser_cfg" class="md-nav__link">
    fuser_cfg()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatchClassifier" class="md-nav__link">
    MMLatchClassifier
  </a>
  
    <nav class="md-nav" aria-label="MMLatchClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.mmlatch.MMLatchClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="multimodal-modules">Multimodal Modules</h1>
<p>We include strong baselines for multimodal fusion and state-of-the-art paper implementations.</p>
<h2 id="fusers">Fusers</h2>
<p>This module contains the implementation of basic fusion algorithms and fusion pipelines.</p>
<p>The fusers are implemented for arbitrary number of input modalities, unless otherwise stated and are geared towards sequential inputs.</p>
<p>A fusion pipeline consists generally of three stages</p>
<ul>
<li>Pre-fuse processing: Perform some common operations to all input modalities (e.g. project to a common dimension.)</li>
<li>Fuser: Fuse all modality representations into a single vector (e.g. concatenate all modality features using CatFuser).</li>
<li>Timesteps Pooling: Aggregate fused features for all timesteps into a single vector (e.g. add all timesteps with SumPooler)</li>
</ul>


  <div class="doc doc-object doc-module">

<a id="slp.modules.fuse"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h2 id="slp.modules.fuse.SUPPORTED_FUSERS" class="doc doc-heading">
<code class="highlight language-python"><span class="n">SUPPORTED_FUSERS</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="n">slp</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">fuse</span><span class="o">.</span><span class="n">BaseFuser</span><span class="p">]]</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Currently implemented fusers</p>
    </div>

  </div>



  <div class="doc doc-object doc-attribute">



<h2 id="slp.modules.fuse.SUPPORTED_POOLERS" class="doc doc-heading">
<code class="highlight language-python"><span class="n">SUPPORTED_POOLERS</span><span class="p">:</span> <span class="n">Mapping</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Type</span><span class="p">[</span><span class="n">slp</span><span class="o">.</span><span class="n">modules</span><span class="o">.</span><span class="n">fuse</span><span class="o">.</span><span class="n">BaseTimestepsPooler</span><span class="p">]]</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Supported poolers</p>
    </div>

  </div>




  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.AttentionFuser" class="doc doc-heading">
        <code>AttentionFuser</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.AttentionFuser.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="n">use_all_trimodal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Fuse all combinations of three modalities using a base module using bilinear fusion</p>
<p>If input modalities are a, t, v, then the output is</p>
<p>Where f is TwowayAttention and g is Attention modules and values with [] are optional
o = t || a || v || f(t, a) || f(v, a) || f(t, v) || g(t, f(v, a)) || [ g(v, f(t,a)) ] || [g(a, f(t,v))]</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of feature dimensions</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of input modalities (should be 3)</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>use_all_trimodal</code></td>
        <td><code>bool</code></td>
        <td><p>Use all optional trimodal combinations</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>residual</code></td>
        <td><code>bool</code></td>
        <td><p>Use residual connection in TwowayAttention. Defaults to True</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability</p></td>
        <td><code>0.1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">use_all_trimodal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fuse all combinations of three modalities using a base module using bilinear fusion</span>

<span class="sd">    If input modalities are a, t, v, then the output is</span>

<span class="sd">    Where f is TwowayAttention and g is Attention modules and values with [] are optional</span>
<span class="sd">    o = t || a || v || f(t, a) || f(v, a) || f(t, v) || g(t, f(v, a)) || [ g(v, f(t,a)) ] || [g(a, f(t,v))]</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): Number of feature dimensions</span>
<span class="sd">        n_modalities (int): Number of input modalities (should be 3)</span>
<span class="sd">        use_all_trimodal (bool): Use all optional trimodal combinations</span>
<span class="sd">        residual (bool): Use residual connection in TwowayAttention. Defaults to True</span>
<span class="sd">        dropout (float): Dropout probability</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;dropout&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="n">kwargs</span><span class="p">[</span><span class="s2">&quot;residual&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">residual</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AttentionFuser</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">feature_size</span><span class="p">,</span>
        <span class="n">n_modalities</span><span class="p">,</span>
        <span class="n">use_all_trimodal</span><span class="o">=</span><span class="n">use_all_trimodal</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.AttentionFuser.fuse" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fuse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Perform attention fusion on input modalities</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Input tensors to fuse. This module accepts 3 input modalities. [B, L, D]</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Unpadded tensors lengths</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: fused output vector [B, L, 7<em>D] or [B, L, 9</em>D]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fuse</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Perform attention fusion on input modalities</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: Input tensors to fuse. This module accepts 3 input modalities. [B, L, D]</span>
<span class="sd">        lengths (Optional[torch.Tensor]): Unpadded tensors lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: fused output vector [B, L, 7*D] or [B, L, 9*D]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">txt</span><span class="p">,</span> <span class="n">au</span><span class="p">,</span> <span class="n">vi</span> <span class="o">=</span> <span class="n">mods</span>
    <span class="n">ta</span><span class="p">,</span> <span class="n">at</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ta</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">au</span><span class="p">)</span>
    <span class="n">va</span><span class="p">,</span> <span class="n">av</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">va</span><span class="p">(</span><span class="n">vi</span><span class="p">,</span> <span class="n">au</span><span class="p">)</span>
    <span class="n">tv</span><span class="p">,</span> <span class="n">vt</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tv</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">vi</span><span class="p">)</span>

    <span class="n">va</span> <span class="o">=</span> <span class="n">va</span> <span class="o">+</span> <span class="n">av</span>
    <span class="n">tv</span> <span class="o">=</span> <span class="n">vt</span> <span class="o">+</span> <span class="n">tv</span>
    <span class="n">ta</span> <span class="o">=</span> <span class="n">ta</span> <span class="o">+</span> <span class="n">at</span>

    <span class="n">tav</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tav</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">va</span><span class="p">)</span>

    <span class="n">out_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">txt</span><span class="p">,</span> <span class="n">au</span><span class="p">,</span> <span class="n">vi</span><span class="p">,</span> <span class="n">ta</span><span class="p">,</span> <span class="n">tv</span><span class="p">,</span> <span class="n">va</span><span class="p">,</span> <span class="n">tav</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_all_trimodal</span><span class="p">:</span>
        <span class="n">vat</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vat</span><span class="p">(</span><span class="n">vi</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">ta</span><span class="p">)</span>
        <span class="n">atv</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atv</span><span class="p">(</span><span class="n">au</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">tv</span><span class="p">)</span>

        <span class="n">out_list</span> <span class="o">=</span> <span class="n">out_list</span> <span class="o">+</span> <span class="p">[</span><span class="n">vat</span><span class="p">,</span> <span class="n">atv</span><span class="p">]</span>

    <span class="c1"># B x L x 7*D or B x L x 9*D</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">out_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fused</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.BaseFuser" class="doc doc-heading">
        <code>BaseFuser</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.BaseFuser.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Output feature size.</p>
<p>Each fuser specifies its output feature size</p>
    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BaseFuser.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">extra_kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Base fuser class.</p>
<p>Our fusion methods are separated in direct and combinatorial.
An example for direct fusion is concatenation, where feature vectors of N modalities
are concatenated into a fused vector.
When performing combinatorial fusion all crossmodal relations are examined (e.g. text -&gt; audio,
text -&gt; visual, audio -&gt; visaul etc.)
In the current implementation, combinatorial fusion is implemented for 3 input modalities</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>Assume all modality representations have the same feature_size</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of input modalities</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**extra_kwargs</code></td>
        <td><code>dict</code></td>
        <td><p>Extra keyword arguments to maintain interoperability of children
classes</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="o">**</span><span class="n">extra_kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base fuser class.</span>

<span class="sd">    Our fusion methods are separated in direct and combinatorial.</span>
<span class="sd">    An example for direct fusion is concatenation, where feature vectors of N modalities</span>
<span class="sd">    are concatenated into a fused vector.</span>
<span class="sd">    When performing combinatorial fusion all crossmodal relations are examined (e.g. text -&gt; audio,</span>
<span class="sd">    text -&gt; visual, audio -&gt; visaul etc.)</span>
<span class="sd">    In the current implementation, combinatorial fusion is implemented for 3 input modalities</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): Assume all modality representations have the same feature_size</span>
<span class="sd">        n_modalities (int): Number of input modalities</span>
<span class="sd">        **extra_kwargs (dict): Extra keyword arguments to maintain interoperability of children</span>
<span class="sd">            classes</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BaseFuser</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feature_size</span> <span class="o">=</span> <span class="n">feature_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_modalities</span> <span class="o">=</span> <span class="n">n_modalities</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BaseFuser.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Fuse the modality representations</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>List of modality tensors [B, L, D]</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Lengths of each modality</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Fused tensor [B, L, self.out_size]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Fuse the modality representations</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: List of modality tensors [B, L, D]</span>
<span class="sd">        lengths (Optional[Tensor]): Lengths of each modality</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Fused tensor [B, L, self.out_size]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fuse</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fused</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BaseFuser.fuse" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fuse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Abstract method to fuse the modality representations</p>
<p>Children classes should implement this method</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>List of modality tensors</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Lengths of each modality</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Fused tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@abstractmethod</span>
<span class="k">def</span> <span class="nf">fuse</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Abstract method to fuse the modality representations</span>

<span class="sd">    Children classes should implement this method</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: List of modality tensors</span>
<span class="sd">        lengths (Optional[Tensor]): Lengths of each modality</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Fused tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.BaseFusionPipeline" class="doc doc-heading">
        <code>BaseFusionPipeline</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.BaseFusionPipeline.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Define the feature size of the returned tensor</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: The feature dimension of the output tensor</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BaseFusionPipeline.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Base class for a fusion pipeline</p>
<p>Inherit this class to implement a fusion pipeline</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for a fusion pipeline</span>

<span class="sd">    Inherit this class to implement a fusion pipeline</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BaseFusionPipeline</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.BaseTimestepsPooler" class="doc doc-heading">
        <code>BaseTimestepsPooler</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.BaseTimestepsPooler.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Define the feature size of the returned tensor</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: The feature dimension of the output tensor</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BaseTimestepsPooler.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Abstract base class for Timesteps Poolers</p>
<p>Timesteps Poolers aggregate the features for different timesteps</p>
<p>Given a tensor with dimensions [BatchSize, Length, Dim]
they return an aggregated tensor with dimensions [BatchSize, Dim]</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Input tensors are in batch first configuration. Leave this as true
except if you know what you are doing</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td><code></code></td>
        <td><p>Variable keyword arguments for subclasses</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Abstract base class for Timesteps Poolers</span>

<span class="sd">    Timesteps Poolers aggregate the features for different timesteps</span>

<span class="sd">    Given a tensor with dimensions [BatchSize, Length, Dim]</span>
<span class="sd">    they return an aggregated tensor with dimensions [BatchSize, Dim]</span>


<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): Feature dimension</span>
<span class="sd">        batch_first (bool): Input tensors are in batch first configuration. Leave this as true</span>
<span class="sd">            except if you know what you are doing</span>
<span class="sd">        **kwargs: Variable keyword arguments for subclasses</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BaseTimestepsPooler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">pooling_dim</span> <span class="o">=</span> <span class="mi">0</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_first</span> <span class="k">else</span> <span class="mi">1</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">feature_size</span> <span class="o">=</span> <span class="n">feature_size</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BaseTimestepsPooler.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Pool features of input tensor across timesteps</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, D] Input sequence</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional unpadded sequence lengths for input tensor</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: [B, D] Output aggregated features across timesteps</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Pool features of input tensor across timesteps</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, D] Input sequence</span>
<span class="sd">        lengths (Optional[torch.Tensor]): Optional unpadded sequence lengths for input tensor</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, D] Output aggregated features across timesteps</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">x</span>

    <span class="k">if</span> <span class="n">x</span><span class="o">.</span><span class="n">ndim</span> <span class="o">!=</span> <span class="mi">3</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;Expected 3 dimensional tensor [B, L, D] or [L, B, D]&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_pool</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.BilinearFuser" class="doc doc-heading">
        <code>BilinearFuser</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BilinearFuser.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="n">use_all_trimodal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Fuse all combinations of three modalities using a base module using bilinear fusion</p>
<p>If input modalities are a, t, v, then the output is
o = t || a || v || f(t, a) || f(v, a) || f(t, v) || g(t, f(v, a)) || [ g(v, f(t,a)) ] || [g(a, f(t,v))]</p>
<p>Where f and g are the nn.Bilinear function and values with [] are optional</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of feature dimensions</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of input modalities (should be 3)</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>use_all_trimodal</code></td>
        <td><code>bool</code></td>
        <td><p>Use all optional trimodal combinations</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">use_all_trimodal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fuse all combinations of three modalities using a base module using bilinear fusion</span>

<span class="sd">    If input modalities are a, t, v, then the output is</span>
<span class="sd">    o = t || a || v || f(t, a) || f(v, a) || f(t, v) || g(t, f(v, a)) || [ g(v, f(t,a)) ] || [g(a, f(t,v))]</span>

<span class="sd">    Where f and g are the nn.Bilinear function and values with [] are optional</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): Number of feature dimensions</span>
<span class="sd">        n_modalities (int): Number of input modalities (should be 3)</span>
<span class="sd">        use_all_trimodal (bool): Use all optional trimodal combinations</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BilinearFuser</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">feature_size</span><span class="p">,</span>
        <span class="n">n_modalities</span><span class="p">,</span>
        <span class="n">use_all_trimodal</span><span class="o">=</span><span class="n">use_all_trimodal</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BilinearFuser.fuse" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fuse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Perform bilinear fusion on input modalities</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Input tensors to fuse. This module accepts 3 input modalities. [B, L, D]</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Unpadded tensors lengths</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: fused output vector [B, L, 7<em>D] or [B, L, 9</em>D]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fuse</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Perform bilinear fusion on input modalities</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: Input tensors to fuse. This module accepts 3 input modalities. [B, L, D]</span>
<span class="sd">        lengths (Optional[torch.Tensor]): Unpadded tensors lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: fused output vector [B, L, 7*D] or [B, L, 9*D]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">txt</span><span class="p">,</span> <span class="n">au</span><span class="p">,</span> <span class="n">vi</span> <span class="o">=</span> <span class="n">mods</span>
    <span class="n">ta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ta</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">au</span><span class="p">)</span>
    <span class="n">va</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">va</span><span class="p">(</span><span class="n">vi</span><span class="p">,</span> <span class="n">au</span><span class="p">)</span>
    <span class="n">tv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tv</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">vi</span><span class="p">)</span>

    <span class="n">tav</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">tav</span><span class="p">(</span><span class="n">txt</span><span class="p">,</span> <span class="n">va</span><span class="p">)</span>

    <span class="n">out_list</span> <span class="o">=</span> <span class="p">[</span><span class="n">txt</span><span class="p">,</span> <span class="n">au</span><span class="p">,</span> <span class="n">vi</span><span class="p">,</span> <span class="n">ta</span><span class="p">,</span> <span class="n">tv</span><span class="p">,</span> <span class="n">va</span><span class="p">,</span> <span class="n">tav</span><span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_all_trimodal</span><span class="p">:</span>
        <span class="n">vat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vat</span><span class="p">(</span><span class="n">vi</span><span class="p">,</span> <span class="n">ta</span><span class="p">)</span>
        <span class="n">atv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">atv</span><span class="p">(</span><span class="n">au</span><span class="p">,</span> <span class="n">tv</span><span class="p">)</span>

        <span class="n">out_list</span> <span class="o">=</span> <span class="n">out_list</span> <span class="o">+</span> <span class="p">[</span><span class="n">vat</span><span class="p">,</span> <span class="n">atv</span><span class="p">]</span>

    <span class="c1"># B x L x 7*D or B x L x 9*D</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">out_list</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fused</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.BimodalAttentionFuser" class="doc doc-heading">
        <code>BimodalAttentionFuser</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BimodalAttentionFuser.fuse" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fuse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Perform attention fusion on input modalities</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Input tensors to fuse. This module accepts 2 input modalities. [B, L, D]</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Unpadded tensors lengths</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: fused output vector [B, L, 3*D]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fuse</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Perform attention fusion on input modalities</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: Input tensors to fuse. This module accepts 2 input modalities. [B, L, D]</span>
<span class="sd">        lengths (Optional[torch.Tensor]): Unpadded tensors lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: fused output vector [B, L, 3*D]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mods</span>
    <span class="n">xy</span><span class="p">,</span> <span class="n">yx</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="n">xy</span> <span class="o">+</span> <span class="n">yx</span>
    <span class="c1"># B x L x 3*D</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xy</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fused</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.BimodalBilinearFuser" class="doc doc-heading">
        <code>BimodalBilinearFuser</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BimodalBilinearFuser.fuse" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fuse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Perform bilinear fusion on input modalities</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Input tensors to fuse. This module accepts 2 input modalities. [B, L, D]</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Unpadded tensors lengths</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: fused output vector [B, L, 3*D]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fuse</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Perform bilinear fusion on input modalities</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: Input tensors to fuse. This module accepts 2 input modalities. [B, L, D]</span>
<span class="sd">        lengths (Optional[torch.Tensor]): Unpadded tensors lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: fused output vector [B, L, 3*D]</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">mods</span>
    <span class="n">xy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xy</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

    <span class="c1"># B x L x 3*D</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">xy</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fused</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.BimodalCombinatorialFuser" class="doc doc-heading">
        <code>BimodalCombinatorialFuser</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.BimodalCombinatorialFuser.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Fused vector feature dimension</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: 3 * feature_size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.BimodalCombinatorialFuser.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Fuse all combinations of three modalities using a base module</p>
<p>If input modalities are x, y, then the output is
o = x || y || f(x, y)</p>
<p>Where f is a network module (e.g. attention)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of feature dimensions</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of input modalities (should be 3)</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fuse all combinations of three modalities using a base module</span>

<span class="sd">    If input modalities are x, y, then the output is</span>
<span class="sd">    o = x || y || f(x, y)</span>

<span class="sd">    Where f is a network module (e.g. attention)</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): Number of feature dimensions</span>
<span class="sd">        n_modalities (int): Number of input modalities (should be 3)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BimodalCombinatorialFuser</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_n_modalities</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">xy</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bimodal_fusion_module</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.CatFuser" class="doc doc-heading">
        <code>CatFuser</code>



</h2>

    <div class="doc doc-contents ">

      <p>Fuse by concatenating modality representations</p>
<p>o = m1 || m2 || m3 ...</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>Assume all modality representations have the same feature_size</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of input modalities</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**extra_kwargs</code></td>
        <td><code>dict</code></td>
        <td><p>Extra keyword arguments to maintain interoperability of children
classes</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>



  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.CatFuser.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>d_out = n_modalities * d_in</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: output feature size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.CatFuser.fuse" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fuse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Concatenate input tensors into a single tensor</p>

<p><strong>Examples:</strong></p>
    <p>fuser = CatFuser(5, 2)
x = torch.rand(16, 6, 5)  # (B, L, D)
y = torch.rand(16, 6, 5)  # (B, L, D)
out = fuser(x, y)  # (B, L, 2 * D)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Variable number of input tensors</p></td>
        <td><code>()</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Concatenated input tensors</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fuse</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Concatenate input tensors into a single tensor</span>

<span class="sd">    Example:</span>
<span class="sd">        fuser = CatFuser(5, 2)</span>
<span class="sd">        x = torch.rand(16, 6, 5)  # (B, L, D)</span>
<span class="sd">        y = torch.rand(16, 6, 5)  # (B, L, D)</span>
<span class="sd">        out = fuser(x, y)  # (B, L, 2 * D)</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: Variable number of input tensors</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Concatenated input tensors</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">mods</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.Conv1dProjection" class="doc doc-heading">
        <code>Conv1dProjection</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.Conv1dProjection.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modality_sizes</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Project features for N modalities using 1D convolutions</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>modality_sizes</code></td>
        <td><code>List[int]</code></td>
        <td><p>List of number of features for each modality. E.g. for MOSEI:
[300, 74, 35]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>projection_size</code></td>
        <td><code>int</code></td>
        <td><p>Output number of features for each modality</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>kernel_size</code></td>
        <td><code>int</code></td>
        <td><p>Convolution kernel size</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>padding</code></td>
        <td><code>int</code></td>
        <td><p>Convlution amount of padding</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>bias</code></td>
        <td><code>bool</code></td>
        <td><p>Use bias in convolutional layers</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">modality_sizes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">projection_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Project features for N modalities using 1D convolutions</span>

<span class="sd">    Args:</span>
<span class="sd">        modality_sizes (List[int]): List of number of features for each modality. E.g. for MOSEI:</span>
<span class="sd">            [300, 74, 35]</span>
<span class="sd">        projection_size (int): Output number of features for each modality</span>
<span class="sd">        kernel_size (int): Convolution kernel size</span>
<span class="sd">        padding (int): Convlution amount of padding</span>
<span class="sd">        bias (bool): Use bias in convolutional layers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Conv1dProjection</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">Conv1d</span><span class="p">(</span>
                <span class="n">sz</span><span class="p">,</span>
                <span class="n">projection_size</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
                <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">sz</span> <span class="ow">in</span> <span class="n">modality_sizes</span>
        <span class="p">]</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.Conv1dProjection.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Project modality representations to a given number of features using Conv1d layers</p>

<p><strong>Examples:</strong></p>
    <h4 id="slp.modules.fuse.Conv1dProjection.forward--inputs">Inputs:</h4>
<h4 id="slp.modules.fuse.Conv1dProjection.forward--text-b-l-300">text: (B, L, 300)</h4>
<h4 id="slp.modules.fuse.Conv1dProjection.forward--audio-b-l-74">audio: (B, L, 74)</h4>
<h4 id="slp.modules.fuse.Conv1dProjection.forward--visual-b-l-35">visual: (B, L, 35)</h4>
<h4 id="slp.modules.fuse.Conv1dProjection.forward--outputs">Outputs:</h4>
<h4 id="slp.modules.fuse.Conv1dProjection.forward--text_p-b-l-100">text_p: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.Conv1dProjection.forward--audio_p-b-l-100">audio_p: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.Conv1dProjection.forward--visual_p-b-l-100">visual_p: (B, L, 100)</h4>
<p>c_proj = Conv1dProjection([300, 74, 35], 100)
text_p, audio_p, visual_p = c_proj(text, audio, visual)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Variable length tensors list</p></td>
        <td><code>()</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[torch.Tensor]</code></td>
      <td><p>List[torch.Tensor]: Variable length projected tensors list</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Project modality representations to a given number of features using Conv1d layers</span>
<span class="sd">    Example:</span>
<span class="sd">        # Inputs:</span>
<span class="sd">        #    text: (B, L, 300)</span>
<span class="sd">        #    audio: (B, L, 74)</span>
<span class="sd">        #    visual: (B, L, 35)</span>
<span class="sd">        # Outputs:</span>
<span class="sd">        #    text_p: (B, L, 100)</span>
<span class="sd">        #    audio_p: (B, L, 100)</span>
<span class="sd">        #    visual_p: (B, L, 100)</span>
<span class="sd">        c_proj = Conv1dProjection([300, 74, 35], 100)</span>
<span class="sd">        text_p, audio_p, visual_p = c_proj(text, audio, visual)</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: Variable length tensors list</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[torch.Tensor]: Variable length projected tensors list</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mods_o</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">m</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mods</span><span class="p">)</span>
    <span class="p">]</span>

    <span class="k">return</span> <span class="n">mods_o</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.FuseAggregateTimesteps" class="doc doc-heading">
        <code>FuseAggregateTimesteps</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.FuseAggregateTimesteps.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Define the feature size of the returned tensor</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: The feature dimension of the output tensor</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.FuseAggregateTimesteps.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fusion_method</span><span class="o">=</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">timesteps_pooling_method</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">fuser_kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Fuse input feature sequences and aggregate across timesteps</p>
<p>Fuser -&gt; TimestepsPooler</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>The input modality representations dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of input modalities</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>output_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Required output size. If not provided,
output_size = fuser.out_size</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>fusion_method</code></td>
        <td><code>str</code></td>
        <td><p>Select which fuser to use [cat|sum|attention|bilinear]</p></td>
        <td><code>&#39;cat&#39;</code></td>
      </tr>
      <tr>
        <td><code>timesteps_pooling_method</code></td>
        <td><code>str</code></td>
        <td><p>TimestepsPooler method [cat|sum|rnn]</p></td>
        <td><code>&#39;sum&#39;</code></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Input tensors are in batch first configuration. Leave this as true
except if you know what you are doing</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>**fuser_kwargs</code></td>
        <td><code>dict</code></td>
        <td><p>Extra keyword arguments to instantiate fuser</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fusion_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">timesteps_pooling_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">fuser_kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fuse input feature sequences and aggregate across timesteps</span>

<span class="sd">    Fuser -&gt; TimestepsPooler</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): The input modality representations dimension</span>
<span class="sd">        n_modalities (int): Number of input modalities</span>
<span class="sd">        output_size (Optional[int]): Required output size. If not provided,</span>
<span class="sd">            output_size = fuser.out_size</span>
<span class="sd">        fusion_method (str): Select which fuser to use [cat|sum|attention|bilinear]</span>
<span class="sd">        timesteps_pooling_method (str): TimestepsPooler method [cat|sum|rnn]</span>
<span class="sd">        batch_first (bool): Input tensors are in batch first configuration. Leave this as true</span>
<span class="sd">            except if you know what you are doing</span>
<span class="sd">        **fuser_kwargs (dict): Extra keyword arguments to instantiate fuser</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">FuseAggregateTimesteps</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="n">fusion_method</span><span class="o">=</span><span class="n">fusion_method</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">fuser</span> <span class="o">=</span> <span class="n">make_fuser</span><span class="p">(</span>
        <span class="n">fusion_method</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">fuser_kwargs</span>
    <span class="p">)</span>
    <span class="n">output_size</span> <span class="o">=</span> <span class="p">(</span>  <span class="c1"># bidirectional rnn. fused_size / 2 results to fused_size outputs</span>
        <span class="n">output_size</span> <span class="k">if</span> <span class="n">output_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">fuser</span><span class="o">.</span><span class="n">out_size</span> <span class="o">//</span> <span class="mi">2</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">timesteps_pooler</span> <span class="o">=</span> <span class="n">TimestepsPooler</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fuser</span><span class="o">.</span><span class="n">out_size</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="n">output_size</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">timesteps_pooling_method</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.FuseAggregateTimesteps.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Fuse the modality representations and aggregate across timesteps</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>List of modality tensors [B, L, D]</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Lengths of each modality</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Fused tensor [B, self.out_size]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Fuse the modality representations and aggregate across timesteps</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: List of modality tensors [B, L, D]</span>
<span class="sd">        lengths (Optional[Tensor]): Lengths of each modality</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Fused tensor [B, self.out_size]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fuser</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">timesteps_pooler</span><span class="p">(</span><span class="n">fused</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.LinearProjection" class="doc doc-heading">
        <code>LinearProjection</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.LinearProjection.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modality_sizes</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Project features for N modalities using feedforward layers</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>modality_sizes</code></td>
        <td><code>List[int]</code></td>
        <td><p>List of number of features for each modality. E.g. for MOSEI:
[300, 74, 35]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>bias</code></td>
        <td><code>bool</code></td>
        <td><p>Use bias in feedforward layers</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">modality_sizes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span> <span class="n">projection_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Project features for N modalities using feedforward layers</span>

<span class="sd">    Args:</span>
<span class="sd">        modality_sizes (List[int]): List of number of features for each modality. E.g. for MOSEI:</span>
<span class="sd">            [300, 74, 35]</span>
<span class="sd">        bias (bool): Use bias in feedforward layers</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">LinearProjection</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
        <span class="p">[</span><span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">sz</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span> <span class="k">for</span> <span class="n">sz</span> <span class="ow">in</span> <span class="n">modality_sizes</span><span class="p">]</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.LinearProjection.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Project modality representations to a given number of features using Linear layers</p>

<p><strong>Examples:</strong></p>
    <h4 id="slp.modules.fuse.LinearProjection.forward--inputs">Inputs:</h4>
<h4 id="slp.modules.fuse.LinearProjection.forward--text-b-l-300">text: (B, L, 300)</h4>
<h4 id="slp.modules.fuse.LinearProjection.forward--audio-b-l-74">audio: (B, L, 74)</h4>
<h4 id="slp.modules.fuse.LinearProjection.forward--visual-b-l-35">visual: (B, L, 35)</h4>
<h4 id="slp.modules.fuse.LinearProjection.forward--outputs">Outputs:</h4>
<h4 id="slp.modules.fuse.LinearProjection.forward--text_p-b-l-100">text_p: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.LinearProjection.forward--audio_p-b-l-100">audio_p: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.LinearProjection.forward--visual_p-b-l-100">visual_p: (B, L, 100)</h4>
<p>l_proj = LinearProjection([300, 74, 35], 100)
text_p, audio_p, visual_p = l_proj(text, audio, visual)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Variable length tensor list</p></td>
        <td><code>()</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[torch.Tensor]</code></td>
      <td><p>List[torch.Tensor]: Variable length projected tensors list</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Project modality representations to a given number of features using Linear layers</span>
<span class="sd">    Example:</span>
<span class="sd">        # Inputs:</span>
<span class="sd">        #    text: (B, L, 300)</span>
<span class="sd">        #    audio: (B, L, 74)</span>
<span class="sd">        #    visual: (B, L, 35)</span>
<span class="sd">        # Outputs:</span>
<span class="sd">        #    text_p: (B, L, 100)</span>
<span class="sd">        #    audio_p: (B, L, 100)</span>
<span class="sd">        #    visual_p: (B, L, 100)</span>
<span class="sd">        l_proj = LinearProjection([300, 74, 35], 100)</span>
<span class="sd">        text_p, audio_p, visual_p = l_proj(text, audio, visual)</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: Variable length tensor list</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[torch.Tensor]: Variable length projected tensors list</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mods_o</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">m</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mods</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">mods_o</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>





  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.ModalityProjection" class="doc doc-heading">
        <code>ModalityProjection</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.ModalityProjection.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modality_sizes</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">padding</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Adapter module to project features for N modalities using 1D convolutions or feedforward</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>modality_sizes</code></td>
        <td><code>List[int]</code></td>
        <td><p>List of number of features for each modality. E.g. for MOSEI:
[300, 74, 35]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>projection_size</code></td>
        <td><code>int</code></td>
        <td><p>Output number of features for each modality</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>kernel_size</code></td>
        <td><code>int</code></td>
        <td><p>Convolution kernel size. Used when mode=="conv"</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>padding</code></td>
        <td><code>int</code></td>
        <td><p>Convlution amount of padding. Used when mode=="conv"</p></td>
        <td><code>0</code></td>
      </tr>
      <tr>
        <td><code>bias</code></td>
        <td><code>bool</code></td>
        <td><p>Use bias</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>mode</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Projection method.
linear -&gt; LinearProjection
conv|conv1d|convolutional -&gt; Conv1dProjection</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">modality_sizes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">projection_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">padding</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span>
    <span class="n">bias</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">mode</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Adapter module to project features for N modalities using 1D convolutions or feedforward</span>

<span class="sd">    Args:</span>
<span class="sd">        modality_sizes (List[int]): List of number of features for each modality. E.g. for MOSEI:</span>
<span class="sd">            [300, 74, 35]</span>
<span class="sd">        projection_size (int): Output number of features for each modality</span>
<span class="sd">        kernel_size (int): Convolution kernel size. Used when mode==&quot;conv&quot;</span>
<span class="sd">        padding (int): Convlution amount of padding. Used when mode==&quot;conv&quot;</span>
<span class="sd">        bias (bool): Use bias</span>
<span class="sd">        mode (Optional[str]): Projection method.</span>
<span class="sd">            linear -&gt; LinearProjection</span>
<span class="sd">            conv|conv1d|convolutional -&gt; Conv1dProjection</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ModalityProjection</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">Union</span><span class="p">[</span><span class="n">LinearProjection</span><span class="p">,</span> <span class="n">Conv1dProjection</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;linear&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">LinearProjection</span><span class="p">(</span><span class="n">modality_sizes</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">)</span>
    <span class="k">elif</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;conv&quot;</span> <span class="ow">or</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;conv1d&quot;</span> <span class="ow">or</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;convolutional&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">Conv1dProjection</span><span class="p">(</span>
            <span class="n">modality_sizes</span><span class="p">,</span>
            <span class="n">projection_size</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
            <span class="n">padding</span><span class="o">=</span><span class="n">padding</span><span class="p">,</span>
            <span class="n">bias</span><span class="o">=</span><span class="n">bias</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span>
            <span class="s2">&quot;Supported mode=[linear|conv|conv1d|convolutional].&quot;</span>
            <span class="s2">&quot;conv, conv1d and convolutional are equivalent.&quot;</span>
        <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.ModalityProjection.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Project modality representations to a given number of features</p>

<p><strong>Examples:</strong></p>
    <h4 id="slp.modules.fuse.ModalityProjection.forward--inputs">Inputs:</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--text-b-l-300">text: (B, L, 300)</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--audio-b-l-74">audio: (B, L, 74)</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--visual-b-l-35">visual: (B, L, 35)</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--outputs">Outputs:</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--text_p-b-l-100">text_p: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--audio_p-b-l-100">audio_p: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--visual_p-b-l-100">visual_p: (B, L, 100)</h4>
<p>l_proj = ModalityProjection([300, 74, 35], 100, mode="linear")
text_p, audio_p, visual_p = l_proj(text, audio, visual)</p>

<p><strong>Examples:</strong></p>
    <h4 id="slp.modules.fuse.ModalityProjection.forward--inputs">Inputs:</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--text-b-l-300">text: (B, L, 300)</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--audio-b-l-74">audio: (B, L, 74)</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--visual-b-l-35">visual: (B, L, 35)</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--outputs">Outputs:</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--text_p-b-l-300">text_p: (B, L, 300)</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--audio_p-b-l-74">audio_p: (B, L, 74)</h4>
<h4 id="slp.modules.fuse.ModalityProjection.forward--visual_p-b-l-35">visual_p: (B, L, 35)</h4>
<p>l_proj = ModalityProjection([300, 74, 35], 100, mode=None)
text_p, audio_p, visual_p = l_proj(text, audio, visual)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Variable length tensor list</p></td>
        <td><code>()</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[torch.Tensor]</code></td>
      <td><p>List[torch.Tensor]: Variable length projected tensors list</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Project modality representations to a given number of features</span>
<span class="sd">    Example:</span>
<span class="sd">        # Inputs:</span>
<span class="sd">        #    text: (B, L, 300)</span>
<span class="sd">        #    audio: (B, L, 74)</span>
<span class="sd">        #    visual: (B, L, 35)</span>
<span class="sd">        # Outputs:</span>
<span class="sd">        #    text_p: (B, L, 100)</span>
<span class="sd">        #    audio_p: (B, L, 100)</span>
<span class="sd">        #    visual_p: (B, L, 100)</span>
<span class="sd">        l_proj = ModalityProjection([300, 74, 35], 100, mode=&quot;linear&quot;)</span>
<span class="sd">        text_p, audio_p, visual_p = l_proj(text, audio, visual)</span>

<span class="sd">    Example:</span>
<span class="sd">        # Inputs:</span>
<span class="sd">        #    text: (B, L, 300)</span>
<span class="sd">        #    audio: (B, L, 74)</span>
<span class="sd">        #    visual: (B, L, 35)</span>
<span class="sd">        # Outputs:</span>
<span class="sd">        #    text_p: (B, L, 300)</span>
<span class="sd">        #    audio_p: (B, L, 74)</span>
<span class="sd">        #    visual_p: (B, L, 35)</span>
<span class="sd">        l_proj = ModalityProjection([300, 74, 35], 100, mode=None)</span>
<span class="sd">        text_p, audio_p, visual_p = l_proj(text, audio, visual)</span>


<span class="sd">    Args:</span>
<span class="sd">        *mods: Variable length tensor list</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[torch.Tensor]: Variable length projected tensors list</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">return</span> <span class="nb">list</span><span class="p">(</span><span class="n">mods</span><span class="p">)</span>
    <span class="n">mods_o</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mods_o</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.ModalityWeights" class="doc doc-heading">
        <code>ModalityWeights</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.ModalityWeights.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Multiply each modality features with a learnable weight</p>
<p>i: modality index
learnable_weight[i] = softmax(Linear(modality_features[i]))
output_modality[i] = learnable_weight * modality_features[i]</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>All modalities are assumed to be projected into a space with the same
number of features.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multiply each modality features with a learnable weight</span>

<span class="sd">    i: modality index</span>
<span class="sd">    learnable_weight[i] = softmax(Linear(modality_features[i]))</span>
<span class="sd">    output_modality[i] = learnable_weight * modality_features[i]</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): All modalities are assumed to be projected into a space with the same</span>
<span class="sd">            number of features.</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ModalityWeights</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mod_w</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.ModalityWeights.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Use learnable weights to multiply modality features</p>

<p><strong>Examples:</strong></p>
    <h4 id="slp.modules.fuse.ModalityWeights.forward--inputs">Inputs:</h4>
<h4 id="slp.modules.fuse.ModalityWeights.forward--text-b-l-100">text: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.ModalityWeights.forward--audio-b-l-100">audio: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.ModalityWeights.forward--visual-b-l-100">visual: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.ModalityWeights.forward--outputs">Outputs:</h4>
<h4 id="slp.modules.fuse.ModalityWeights.forward--text_p-b-l-100">text_p: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.ModalityWeights.forward--audio_p-b-l-100">audio_p: (B, L, 100)</h4>
<h4 id="slp.modules.fuse.ModalityWeights.forward--visual_p-b-l-100">visual_p: (B, L, 100)</h4>
<p>mw = ModalityWeights(100)
text_w, audio_w, visual_w = mw(text, audio, visual)</p>
      <p>The operation is summarized as:</p>
<p>w_x = softmax(W * x + b)
w_y = softmax(W * y + b)
x_out = w_x * x
y_out = w_y * y</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Variable length tensor list</p></td>
        <td><code>()</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[torch.Tensor]</code></td>
      <td><p>List[torch.Tensor]: Variable length reweighted tensors list</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Use learnable weights to multiply modality features</span>

<span class="sd">    Example:</span>
<span class="sd">        # Inputs:</span>
<span class="sd">        #    text: (B, L, 100)</span>
<span class="sd">        #    audio: (B, L, 100)</span>
<span class="sd">        #    visual: (B, L, 100)</span>
<span class="sd">        # Outputs:</span>
<span class="sd">        #    text_p: (B, L, 100)</span>
<span class="sd">        #    audio_p: (B, L, 100)</span>
<span class="sd">        #    visual_p: (B, L, 100)</span>
<span class="sd">        mw = ModalityWeights(100)</span>
<span class="sd">        text_w, audio_w, visual_w = mw(text, audio, visual)</span>

<span class="sd">    The operation is summarized as:</span>

<span class="sd">    w_x = softmax(W * x + b)</span>
<span class="sd">    w_y = softmax(W * y + b)</span>
<span class="sd">    x_out = w_x * x</span>
<span class="sd">    y_out = w_y * y</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: Variable length tensor list</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[torch.Tensor]: Variable length reweighted tensors list</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mod_w</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">x</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">x</span> <span class="ow">in</span> <span class="n">mods</span><span class="p">],</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">))</span>
    <span class="n">weight</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">weight</span><span class="p">,</span> <span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">mods_o</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="p">[</span><span class="n">m</span> <span class="o">*</span> <span class="n">weight</span><span class="p">[:,</span> <span class="n">i</span><span class="p">,</span> <span class="o">...</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mods</span><span class="p">)]</span>

    <span class="k">return</span> <span class="n">mods_o</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.ProjectFuseAggregate" class="doc doc-heading">
        <code>ProjectFuseAggregate</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.ProjectFuseAggregate.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Define the feature size of the returned tensor</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: The feature dimension of the output tensor</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.ProjectFuseAggregate.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">modality_sizes</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">projection_type</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fusion_method</span><span class="o">=</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">timesteps_pooling_method</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">modality_weights</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">fuser_kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Project input feature sequences, fuse and aggregate across timesteps</p>
<p>ModalityProjection -&gt; Optional[ModalityWeights] -&gt; Fuser -&gt; TimestepsPooler</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>modality_sizes</code></td>
        <td><code>List[int]</code></td>
        <td><p>List of input modality representations dimensions</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>projection_size</code></td>
        <td><code>int</code></td>
        <td><p>Project all modalities to have this feature size</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>projection_type</code></td>
        <td><code>Optional[str]</code></td>
        <td><p>Optional projection method [linear|conv]</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>fusion_method</code></td>
        <td><code>str</code></td>
        <td><p>Select which fuser to use [cat|sum|attention|bilinear]</p></td>
        <td><code>&#39;cat&#39;</code></td>
      </tr>
      <tr>
        <td><code>timesteps_pooling_method</code></td>
        <td><code>str</code></td>
        <td><p>TimestepsPooler method [cat|sum|rnn]</p></td>
        <td><code>&#39;sum&#39;</code></td>
      </tr>
      <tr>
        <td><code>modality_weights</code></td>
        <td><code>bool</code></td>
        <td><p>Multiply projected modality representations with learnable
weights. Default value is False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Input tensors are in batch first configuration. Leave this as true
except if you know what you are doing</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>**fuser_kwargs</code></td>
        <td><code>dict</code></td>
        <td><p>Extra keyword arguments to instantiate fuser</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">modality_sizes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">projection_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">projection_type</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">str</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fusion_method</span><span class="o">=</span><span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">timesteps_pooling_method</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span>
    <span class="n">modality_weights</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">fuser_kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Project input feature sequences, fuse and aggregate across timesteps</span>

<span class="sd">    ModalityProjection -&gt; Optional[ModalityWeights] -&gt; Fuser -&gt; TimestepsPooler</span>

<span class="sd">    Args:</span>
<span class="sd">        modality_sizes (List[int]): List of input modality representations dimensions</span>
<span class="sd">        projection_size (int): Project all modalities to have this feature size</span>
<span class="sd">        projection_type (Optional[str]): Optional projection method [linear|conv]</span>
<span class="sd">        fusion_method (str): Select which fuser to use [cat|sum|attention|bilinear]</span>
<span class="sd">        timesteps_pooling_method (str): TimestepsPooler method [cat|sum|rnn]</span>
<span class="sd">        modality_weights (bool): Multiply projected modality representations with learnable</span>
<span class="sd">            weights. Default value is False.</span>
<span class="sd">        batch_first (bool): Input tensors are in batch first configuration. Leave this as true</span>
<span class="sd">            except if you know what you are doing</span>
<span class="sd">        **fuser_kwargs (dict): Extra keyword arguments to instantiate fuser</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">ProjectFuseAggregate</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">n_modalities</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">modality_sizes</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="kc">None</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">modality_weights</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">projection_type</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="o">=</span> <span class="n">ModalityProjection</span><span class="p">(</span>
            <span class="n">modality_sizes</span><span class="p">,</span> <span class="n">projection_size</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">projection_type</span>
        <span class="p">)</span>

        <span class="k">if</span> <span class="n">modality_weights</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">modality_weights</span> <span class="o">=</span> <span class="n">ModalityWeights</span><span class="p">(</span><span class="n">projection_size</span><span class="p">)</span>

    <span class="n">fuser_kwargs</span><span class="p">[</span><span class="s2">&quot;output_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">projection_size</span>
    <span class="n">fuser_kwargs</span><span class="p">[</span><span class="s2">&quot;fusion_method&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fusion_method</span>
    <span class="n">fuser_kwargs</span><span class="p">[</span><span class="s2">&quot;timesteps_pooling_method&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">timesteps_pooling_method</span>
    <span class="n">fuser_kwargs</span><span class="p">[</span><span class="s2">&quot;batch_first&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_first</span>

    <span class="k">if</span> <span class="s2">&quot;n_modalities&quot;</span> <span class="ow">in</span> <span class="n">fuser_kwargs</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">fuser_kwargs</span><span class="p">[</span><span class="s2">&quot;n_modalities&quot;</span><span class="p">]</span>

    <span class="k">if</span> <span class="s2">&quot;projection_size&quot;</span> <span class="ow">in</span> <span class="n">fuser_kwargs</span><span class="p">:</span>
        <span class="k">del</span> <span class="n">fuser_kwargs</span><span class="p">[</span><span class="s2">&quot;projection_size&quot;</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">fuse_aggregate</span> <span class="o">=</span> <span class="n">FuseAggregateTimesteps</span><span class="p">(</span>
        <span class="n">projection_size</span><span class="p">,</span>
        <span class="n">n_modalities</span><span class="p">,</span>
        <span class="o">**</span><span class="n">fuser_kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.ProjectFuseAggregate.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Project modality representations to a common dimension, fuse and aggregate across timesteps</p>
<p>Optionally use modality weights</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>List of modality tensors [B, L, D]</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Lengths of each modality</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Fused tensor [B, self.out_size]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Project modality representations to a common dimension, fuse and aggregate across timesteps</span>

<span class="sd">    Optionally use modality weights</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: List of modality tensors [B, L, D]</span>
<span class="sd">        lengths (Optional[Tensor]): Lengths of each modality</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Fused tensor [B, self.out_size]</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mods</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">projection</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">modality_weights</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mods</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">modality_weights</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">)</span>
    <span class="n">fused</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fuse_aggregate</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fused</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.RnnPooler" class="doc doc-heading">
        <code>RnnPooler</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.RnnPooler.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Define the feature size of the returned tensor</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: The feature dimension of the output tensor</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.RnnPooler.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">merge_bi</span><span class="o">=</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Aggregate features of the input tensor using an AttentiveRNN</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Hidden dimension</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Input tensors are in batch first configuration. Leave this as true
except if you know what you are doing</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use bidirectional RNN. Defaults to True</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>merge_bi</code></td>
        <td><code>str</code></td>
        <td><p>How bidirectional states are merged. Defaults to "cat"</p></td>
        <td><code>&#39;cat&#39;</code></td>
      </tr>
      <tr>
        <td><code>attention</code></td>
        <td><code>bool</code></td>
        <td><p>Use attention for the RNN output states</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td><code></code></td>
        <td><p>Variable keyword arguments</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">merge_bi</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Aggregate features of the input tensor using an AttentiveRNN</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): Feature dimension</span>
<span class="sd">        hidden_size (int): Hidden dimension</span>
<span class="sd">        batch_first (bool): Input tensors are in batch first configuration. Leave this as true</span>
<span class="sd">            except if you know what you are doing</span>
<span class="sd">        bidirectional (bool): Use bidirectional RNN. Defaults to True</span>
<span class="sd">        merge_bi (str): How bidirectional states are merged. Defaults to &quot;cat&quot;</span>
<span class="sd">        attention (bool): Use attention for the RNN output states</span>
<span class="sd">        **kwargs: Variable keyword arguments</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">RnnPooler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span> <span class="k">if</span> <span class="n">hidden_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span> <span class="k">else</span> <span class="n">feature_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">AttentiveRNN</span><span class="p">(</span>
        <span class="n">feature_size</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">merge_bi</span><span class="o">=</span><span class="n">merge_bi</span><span class="p">,</span>
        <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
        <span class="n">return_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>  <span class="c1"># We want to aggregate all hidden states.</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.SumFuser" class="doc doc-heading">
        <code>SumFuser</code>



</h2>

    <div class="doc doc-contents ">

      <p>Fuse by adding modality representations</p>
<p>o = m1 + m2 + m3 ...</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>Assume all modality representations have the same feature_size</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of input modalities</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**extra_kwargs</code></td>
        <td><code>dict</code></td>
        <td><p>Extra keyword arguments to maintain interoperability of children
classes</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>



  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.SumFuser.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>d_out = d_in</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: output feature size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.SumFuser.fuse" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fuse</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Sum input tensors into a single tensor</p>

<p><strong>Examples:</strong></p>
    <p>fuser = SumFuser(5, 2)
x = torch.rand(16, 6, 5)  # (B, L, D)
y = torch.rand(16, 6, 5)  # (B, L, D)
out = fuser(x, y)  # (B, L, D)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Variable number of input tensors</p></td>
        <td><code>()</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Summed input tensors</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">fuse</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Sum input tensors into a single tensor</span>

<span class="sd">    Example:</span>
<span class="sd">        fuser = SumFuser(5, 2)</span>
<span class="sd">        x = torch.rand(16, 6, 5)  # (B, L, D)</span>
<span class="sd">        y = torch.rand(16, 6, 5)  # (B, L, D)</span>
<span class="sd">        out = fuser(x, y)  # (B, L, D)</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: Variable number of input tensors</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Summed input tensors</span>

<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">return</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">([</span><span class="n">m</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="n">mods</span><span class="p">],</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.TimestepsPooler" class="doc doc-heading">
        <code>TimestepsPooler</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.TimestepsPooler.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Define the feature size of the returned tensor</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: The feature dimension of the output tensor</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.TimestepsPooler.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Aggregate features from all timesteps into a single representation.</p>
<p>Three methods supported:
    sum: Sum features from all timesteps
    mean: Average features from all timesteps
    max: Max pool features from all timesteps
    rnn: Use the output from an attentive RNN</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>The number of features for the input fused representations</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Input tensors are in batch first configuration. Leave this as true
except if you know what you are doing</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>mode</code></td>
        <td><code>str</code></td>
        <td><p>The timestep pooling method
sum: Sum hidden states
mean: Average hidden states
max: Max pool features from all hidden states
rnn: Use the output of an Attentive RNN</p></td>
        <td><code>&#39;sum&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Aggregate features from all timesteps into a single representation.</span>

<span class="sd">    Three methods supported:</span>
<span class="sd">        sum: Sum features from all timesteps</span>
<span class="sd">        mean: Average features from all timesteps</span>
<span class="sd">        max: Max pool features from all timesteps</span>
<span class="sd">        rnn: Use the output from an attentive RNN</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): The number of features for the input fused representations</span>
<span class="sd">        batch_first (bool): Input tensors are in batch first configuration. Leave this as true</span>
<span class="sd">            except if you know what you are doing</span>
<span class="sd">        mode (str): The timestep pooling method</span>
<span class="sd">            sum: Sum hidden states</span>
<span class="sd">            mean: Average hidden states</span>
<span class="sd">            max: Max pool features from all hidden states</span>
<span class="sd">            rnn: Use the output of an Attentive RNN</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TimestepsPooler</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">feature_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>
    <span class="k">assert</span> <span class="p">(</span>
        <span class="n">mode</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">mode</span> <span class="ow">in</span> <span class="n">SUPPORTED_POOLERS</span>
    <span class="p">),</span> <span class="s2">&quot;Unsupported timestep pooling method.  Available methods: {SUPPORTED_POOLERS.keys()}&quot;</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pooler</span> <span class="o">=</span> <span class="n">SUPPORTED_POOLERS</span><span class="p">[</span><span class="n">mode</span><span class="p">](</span>
            <span class="n">feature_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
        <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.fuse.TrimodalCombinatorialFuser" class="doc doc-heading">
        <code>TrimodalCombinatorialFuser</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.fuse.TrimodalCombinatorialFuser.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Fused vector feature dimension</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: 7 * feature_size if use_all_trimodal==False else 9*feature_size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.fuse.TrimodalCombinatorialFuser.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="n">use_all_trimodal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Fuse all combinations of three modalities using a base module</p>
<p>If input modalities are a, t, v, then the output is
o = t || a || v || f(t, a) || f(v, a) || f(t, v) || g(t, f(v, a)) || [ g(v, f(t,a)) ] || [g(a, f(t,v))]</p>
<p>Where f and g network modules (e.g. attention) and values with [] are optional</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of feature dimensions</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of input modalities (should be 3)</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>use_all_trimodal</code></td>
        <td><code>bool</code></td>
        <td><p>Use all optional trimodal combinations</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">use_all_trimodal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Fuse all combinations of three modalities using a base module</span>

<span class="sd">    If input modalities are a, t, v, then the output is</span>
<span class="sd">    o = t || a || v || f(t, a) || f(v, a) || f(t, v) || g(t, f(v, a)) || [ g(v, f(t,a)) ] || [g(a, f(t,v))]</span>

<span class="sd">    Where f and g network modules (e.g. attention) and values with [] are optional</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): Number of feature dimensions</span>
<span class="sd">        n_modalities (int): Number of input modalities (should be 3)</span>
<span class="sd">        use_all_trimodal (bool): Use all optional trimodal combinations</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TrimodalCombinatorialFuser</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">_check_n_modalities</span><span class="p">(</span><span class="n">n</span><span class="o">=</span><span class="mi">3</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_all_trimodal</span> <span class="o">=</span> <span class="n">use_all_trimodal</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">ta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bimodal_fusion_module</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">va</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bimodal_fusion_module</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">tv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_bimodal_fusion_module</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">tav</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trimodal_fusion_module</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">use_all_trimodal</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">vat</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trimodal_fusion_module</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">atv</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_trimodal_fusion_module</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="slp.modules.fuse.make_fuser" class="doc doc-heading">
<code class="highlight language-python"><span class="n">make_fuser</span><span class="p">(</span><span class="n">fusion_method</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Helper function to instantiate a fuser given a string fusion_method parameter</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>fusion_method</code></td>
        <td><code>str</code></td>
        <td><p>One of the supported fusion methods [cat|add|bilinear|attention]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>The input modality representations dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of input modalities</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td><code></code></td>
        <td><p>Variable keyword arguments to pass to the instantiated fuser</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/fuse.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">make_fuser</span><span class="p">(</span><span class="n">fusion_method</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Helper function to instantiate a fuser given a string fusion_method parameter</span>

<span class="sd">    Args:</span>
<span class="sd">        fusion_method (str): One of the supported fusion methods [cat|add|bilinear|attention]</span>
<span class="sd">        feature_size (int): The input modality representations dimension</span>
<span class="sd">        n_modalities (int): Number of input modalities</span>
<span class="sd">        **kwargs: Variable keyword arguments to pass to the instantiated fuser</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="k">if</span> <span class="n">fusion_method</span> <span class="ow">not</span> <span class="ow">in</span> <span class="n">SUPPORTED_FUSERS</span><span class="o">.</span><span class="n">keys</span><span class="p">():</span>
        <span class="k">raise</span> <span class="ne">NotImplementedError</span><span class="p">(</span>
            <span class="sa">f</span><span class="s2">&quot;The supported fusers are </span><span class="si">{</span><span class="n">SUPPORTED_FUSERS</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span><span class="si">}</span><span class="s2">. You provided </span><span class="si">{</span><span class="n">fusion_method</span><span class="si">}</span><span class="s2">&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;bilinear&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">n_modalities</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">BimodalBilinearFuser</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">n_modalities</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">BilinearFuser</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;bilinear implemented for 2 or 3 modalities&quot;</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">fusion_method</span> <span class="o">==</span> <span class="s2">&quot;attention&quot;</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">n_modalities</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">BimodalAttentionFuser</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">elif</span> <span class="n">n_modalities</span> <span class="o">==</span> <span class="mi">3</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">AttentionFuser</span><span class="p">(</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">raise</span> <span class="ne">ValueError</span><span class="p">(</span><span class="s2">&quot;attention implemented for 2 or 3 modalities&quot;</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">SUPPORTED_FUSERS</span><span class="p">[</span><span class="n">fusion_method</span><span class="p">](</span><span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>

<h2 id="multimodal-encoders">Multimodal encoders</h2>
<p>These modules implement mid and late fusion. The general structure of a multimodal encoder contains:</p>
<ul>
<li>N Unimodal encoders (e.g. RNNs), where N is the number of input modalities</li>
<li>A fusion pipeline</li>
</ul>
<p>We furthermore implement Multimodal classifiers, which consist of a multimodal encoder followed by an <code>nn.Linear</code> layer.</p>
<p>A special mention should be added for our <code>MultimodalBaseline</code>. This baseline consists of RNN encoders followed by an attention fuser and an RNN timesteps poolwer in multimodal tasks and is tuned on CMU-MOSEI. The default configuration is provided through static methods and achieve strong performance.</p>


  <div class="doc doc-object doc-module">

<a id="slp.modules.multimodal"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.AudioEncoder" class="doc doc-heading">
        <code>AudioEncoder</code>



</h2>

    <div class="doc doc-contents ">

      <p>Alias for Unimodal Encoder</p>



    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.AudioTextClassifier" class="doc doc-heading">
        <code>AudioTextClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.AudioTextClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mods</span> <span class="o">=</span> <span class="p">[</span><span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;audio&quot;</span><span class="p">]]</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.AudioVisualClassifier" class="doc doc-heading">
        <code>AudioVisualClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.AudioVisualClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mods</span> <span class="o">=</span> <span class="p">[</span><span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;visual&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;audio&quot;</span><span class="p">]]</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">[</span><span class="s2">&quot;visual&quot;</span><span class="p">])</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.BaseEncoder" class="doc doc-heading">
        <code>BaseEncoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.multimodal.BaseEncoder.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>An encoder returns its output size</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: The output feature size of the encoder</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.BaseEncoder.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Base class implementing a multimodal encoder</p>
<p>A BaseEncoder child encodes and fuses the modality  features
and returns representations ready to be provided to a classification layer</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class implementing a multimodal encoder</span>

<span class="sd">    A BaseEncoder child encodes and fuses the modality  features</span>
<span class="sd">    and returns representations ready to be provided to a classification layer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BaseEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">args</span> <span class="o">=</span> <span class="n">args</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kwargs</span> <span class="o">=</span> <span class="n">kwargs</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clf</span> <span class="o">=</span> <span class="kc">None</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.BaseEncoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Encode + fuse</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Variable input modality tensors [B, L, D]</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>The unpadded tensor lengths. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: The fused tensor [B, D]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Encode + fuse</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods (torch.Tensor): Variable input modality tensors [B, L, D]</span>
<span class="sd">        lengths (Optional[torch.Tensor], optional): The unpadded tensor lengths. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: The fused tensor [B, D]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">encoded</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fuse</span><span class="p">(</span><span class="o">*</span><span class="n">encoded</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fused</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.BimodalEncoder" class="doc doc-heading">
        <code>BimodalEncoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.multimodal.BimodalEncoder.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Output feature size</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Output feature size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.BimodalEncoder.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder1_args</span><span class="p">,</span> <span class="n">encoder2_args</span><span class="p">,</span> <span class="n">fuser_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Two modality encoder</p>
<p>Encode + Fuse two input modalities</p>
<p>Example encoder_args:
    {
        "input_size": 35,
        "hidden_size": 100,
        "layers": 1,
        "bidirectional": True,
        "dropout": 0.2,
        "rnn_type": "lstm",
        "attention": True,
    }</p>
<p>Example fuser_args:
    {
        "n_modalities": 3,
        "dropout": 0.2,
        "output_size": 100,
        "hidden_size": 100,
        "fusion_method": "cat",
        "timesteps_pooling_method": "rnn",
    }</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>encoder1_args</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Configuration for first encoder</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>encoder2_args</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Configuration for second encoder</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>fuser_args</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Configuration for fuser</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">encoder1_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">encoder2_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">fuser_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Two modality encoder</span>

<span class="sd">    Encode + Fuse two input modalities</span>

<span class="sd">    Example encoder_args:</span>
<span class="sd">        {</span>
<span class="sd">            &quot;input_size&quot;: 35,</span>
<span class="sd">            &quot;hidden_size&quot;: 100,</span>
<span class="sd">            &quot;layers&quot;: 1,</span>
<span class="sd">            &quot;bidirectional&quot;: True,</span>
<span class="sd">            &quot;dropout&quot;: 0.2,</span>
<span class="sd">            &quot;rnn_type&quot;: &quot;lstm&quot;,</span>
<span class="sd">            &quot;attention&quot;: True,</span>
<span class="sd">        }</span>

<span class="sd">    Example fuser_args:</span>
<span class="sd">        {</span>
<span class="sd">            &quot;n_modalities&quot;: 3,</span>
<span class="sd">            &quot;dropout&quot;: 0.2,</span>
<span class="sd">            &quot;output_size&quot;: 100,</span>
<span class="sd">            &quot;hidden_size&quot;: 100,</span>
<span class="sd">            &quot;fusion_method&quot;: &quot;cat&quot;,</span>
<span class="sd">            &quot;timesteps_pooling_method&quot;: &quot;rnn&quot;,</span>
<span class="sd">        }</span>

<span class="sd">    Args:</span>
<span class="sd">        encoder1_args (Dict[str, Any]): Configuration for first encoder</span>
<span class="sd">        encoder2_args (Dict[str, Any]): Configuration for second encoder</span>
<span class="sd">        fuser_args (Dict[str, Any]): Configuration for fuser</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BimodalEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">encoder1_args</span><span class="p">,</span>
        <span class="n">encoder2_args</span><span class="p">,</span>
        <span class="n">fuser_args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="s2">&quot;input_projection&quot;</span> <span class="ow">in</span> <span class="n">fuser_args</span> <span class="ow">and</span> <span class="n">fuser_args</span><span class="p">[</span><span class="s2">&quot;input_projection&quot;</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span> <span class="o">=</span> <span class="n">ModalityProjection</span><span class="p">(</span>
            <span class="p">[</span><span class="n">encoder1_args</span><span class="p">[</span><span class="s2">&quot;input_size&quot;</span><span class="p">],</span> <span class="n">encoder2_args</span><span class="p">[</span><span class="s2">&quot;input_size&quot;</span><span class="p">]],</span>
            <span class="n">fuser_args</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">],</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">fuser_args</span><span class="p">[</span><span class="s2">&quot;input_projection&quot;</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="n">encoder1_args</span><span class="p">[</span><span class="s2">&quot;return_hidden&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>
    <span class="n">encoder2_args</span><span class="p">[</span><span class="s2">&quot;return_hidden&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="kc">True</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder1</span> <span class="o">=</span> <span class="n">UnimodalEncoder</span><span class="p">(</span><span class="o">**</span><span class="n">encoder1_args</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder2</span> <span class="o">=</span> <span class="n">UnimodalEncoder</span><span class="p">(</span><span class="o">**</span><span class="n">encoder2_args</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">fuse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_fusion_pipeline</span><span class="p">(</span>
        <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder1</span><span class="o">.</span><span class="n">out_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder2</span><span class="o">.</span><span class="n">out_size</span><span class="p">],</span> <span class="o">**</span><span class="n">fuser_args</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.GloveEncoder" class="doc doc-heading">
        <code>GloveEncoder</code>



</h2>

    <div class="doc doc-contents ">

      <p>Alias for Unimodal Encoder</p>



    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.MOSEIClassifier" class="doc doc-heading">
        <code>MOSEIClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.MOSEIClassifier.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Encode and classify multimodal inputs</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>encoder</code></td>
        <td><code>BaseEncoder</code></td>
        <td><p>The encoder module</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_classes</code></td>
        <td><code>int</code></td>
        <td><p>The number of target classes</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability</p></td>
        <td><code>0.2</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">:</span> <span class="n">BaseEncoder</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Encode and classify multimodal inputs</span>

<span class="sd">    Args:</span>
<span class="sd">        encoder (BaseEncoder): The encoder module</span>
<span class="sd">        num_classes (int): The number of target classes</span>
<span class="sd">        dropout (float): Dropout probability</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MOSEIClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">enc</span> <span class="o">=</span> <span class="n">encoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clf</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="o">.</span><span class="n">out_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.MultimodalBaseline" class="doc doc-heading">
        <code>MultimodalBaseline</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.MultimodalBaseline.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_size</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">audio_size</span><span class="o">=</span><span class="mi">74</span><span class="p">,</span> <span class="n">visual_size</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">merge_bi</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">,</span> <span class="n">encoder_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fuser_residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_all_trimodal</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Multimodal baseline architecture</p>
<p>This baseline composes of three unimodal RNNs followed by an Attention Fuser and an RNN timestep pooler.
The default configuration is tuned for good performance on MOSEI.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>text_size</code></td>
        <td><code>int</code></td>
        <td><p>Text input size. Defaults to 300.</p></td>
        <td><code>300</code></td>
      </tr>
      <tr>
        <td><code>audio_size</code></td>
        <td><code>int</code></td>
        <td><p>Audio input size. Defaults to 74.</p></td>
        <td><code>74</code></td>
      </tr>
      <tr>
        <td><code>visual_size</code></td>
        <td><code>int</code></td>
        <td><p>Visual input size. Defaults to 35.</p></td>
        <td><code>35</code></td>
      </tr>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>Hidden dimension. Defaults to 100.</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout rate. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>encoder_layers</code></td>
        <td><code>float</code></td>
        <td><p>Number of encoder layers. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use bidirectional RNNs. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>merge_bi</code></td>
        <td><code>str</code></td>
        <td><p>Bidirectional merging method in the encoders. Defaults to "sum".</p></td>
        <td><code>&#39;sum&#39;</code></td>
      </tr>
      <tr>
        <td><code>rnn_type</code></td>
        <td><code>str</code></td>
        <td><p>RNN type [lstm|gru]. Defaults to "lstm".</p></td>
        <td><code>&#39;lstm&#39;</code></td>
      </tr>
      <tr>
        <td><code>encoder_attention</code></td>
        <td><code>bool</code></td>
        <td><p>Use attention in the encoder RNNs. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>fuser_residual</code></td>
        <td><code>bool</code></td>
        <td><p>Use vilbert like residual in the attention fuser. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>use_all_trimodal</code></td>
        <td><code>bool</code></td>
        <td><p>Use all trimodal interactions for the Attention fuser. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">text_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span>
    <span class="n">audio_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">74</span><span class="p">,</span>
    <span class="n">visual_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">35</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">encoder_layers</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">merge_bi</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">encoder_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">fuser_residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">use_all_trimodal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multimodal baseline architecture</span>

<span class="sd">    This baseline composes of three unimodal RNNs followed by an Attention Fuser and an RNN timestep pooler.</span>
<span class="sd">    The default configuration is tuned for good performance on MOSEI.</span>

<span class="sd">    Args:</span>
<span class="sd">        text_size (int, optional): Text input size. Defaults to 300.</span>
<span class="sd">        audio_size (int, optional): Audio input size. Defaults to 74.</span>
<span class="sd">        visual_size (int, optional): Visual input size. Defaults to 35.</span>
<span class="sd">        hidden_size (int, optional): Hidden dimension. Defaults to 100.</span>
<span class="sd">        dropout (float, optional): Dropout rate. Defaults to 0.2.</span>
<span class="sd">        encoder_layers (float, optional): Number of encoder layers. Defaults to 1.</span>
<span class="sd">        bidirectional (bool, optional): Use bidirectional RNNs. Defaults to True.</span>
<span class="sd">        merge_bi (str, optional): Bidirectional merging method in the encoders. Defaults to &quot;sum&quot;.</span>
<span class="sd">        rnn_type (str, optional): RNN type [lstm|gru]. Defaults to &quot;lstm&quot;.</span>
<span class="sd">        encoder_attention (bool, optional): Use attention in the encoder RNNs. Defaults to True.</span>
<span class="sd">        fuser_residual (bool, optional): Use vilbert like residual in the attention fuser. Defaults to True.</span>
<span class="sd">        use_all_trimodal (bool, optional): Use all trimodal interactions for the Attention fuser. Defaults to False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="p">{</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">hidden_size</span><span class="p">,</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">dropout</span><span class="p">,</span>
        <span class="s2">&quot;layers&quot;</span><span class="p">:</span> <span class="n">encoder_layers</span><span class="p">,</span>
        <span class="s2">&quot;attention&quot;</span><span class="p">:</span> <span class="n">encoder_attention</span><span class="p">,</span>
        <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">bidirectional</span><span class="p">,</span>
        <span class="s2">&quot;rnn_type&quot;</span><span class="p">:</span> <span class="n">rnn_type</span><span class="p">,</span>
        <span class="s2">&quot;merge_bi&quot;</span><span class="p">:</span> <span class="n">merge_bi</span><span class="p">,</span>
    <span class="p">}</span>

    <span class="n">text_cfg</span> <span class="o">=</span> <span class="n">MultimodalBaseline</span><span class="o">.</span><span class="n">encoder_cfg</span><span class="p">(</span><span class="n">text_size</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="p">)</span>
    <span class="n">audio_cfg</span> <span class="o">=</span> <span class="n">MultimodalBaseline</span><span class="o">.</span><span class="n">encoder_cfg</span><span class="p">(</span><span class="n">audio_size</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="p">)</span>
    <span class="n">visual_cfg</span> <span class="o">=</span> <span class="n">MultimodalBaseline</span><span class="o">.</span><span class="n">encoder_cfg</span><span class="p">(</span><span class="n">visual_size</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="p">)</span>
    <span class="n">fuser_cfg</span> <span class="o">=</span> <span class="n">MultimodalBaseline</span><span class="o">.</span><span class="n">fuser_cfg</span><span class="p">(</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">residual</span><span class="o">=</span><span class="n">fuser_residual</span><span class="p">,</span>
        <span class="n">use_all_trimodal</span><span class="o">=</span><span class="n">use_all_trimodal</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="nb">super</span><span class="p">(</span><span class="n">MultimodalBaseline</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">text_cfg</span><span class="p">,</span> <span class="n">audio_cfg</span><span class="p">,</span> <span class="n">visual_cfg</span><span class="p">,</span> <span class="n">fuser_cfg</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.MultimodalBaseline.encoder_cfg" class="doc doc-heading">
<code class="highlight language-python"><span class="n">encoder_cfg</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Static method to create the encoder configuration</p>
<p>The default configuration is provided here
This configuration corresponds to the official paper implementation
and is tuned for CMU MOSEI.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>input_size</code></td>
        <td><code>int</code></td>
        <td><p>Input modality size</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**cfg</code></td>
        <td><code></code></td>
        <td><p>Optional keyword arguments</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, Any]</code></td>
      <td><p>Dict[str, Any]: The encoder configuration</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">encoder_cfg</span><span class="p">(</span><span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Static method to create the encoder configuration</span>

<span class="sd">    The default configuration is provided here</span>
<span class="sd">    This configuration corresponds to the official paper implementation</span>
<span class="sd">    and is tuned for CMU MOSEI.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Input modality size</span>
<span class="sd">        **cfg: Optional keyword arguments</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, Any]: The encoder configuration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="n">input_size</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
        <span class="s2">&quot;layers&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layers&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bidirectional&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
        <span class="s2">&quot;rnn_type&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;rnn_type&quot;</span><span class="p">,</span> <span class="s2">&quot;lstm&quot;</span><span class="p">),</span>
        <span class="s2">&quot;attention&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;merge_bi&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;merge_bi&quot;</span><span class="p">,</span> <span class="s2">&quot;sum&quot;</span><span class="p">),</span>
    <span class="p">}</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.MultimodalBaseline.fuser_cfg" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fuser_cfg</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Static method to create the fuser configuration</p>
<p>The default configuration is provided here
This configuration corresponds to the official paper implementation
and is tuned for CMU MOSEI.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>**cfg</code></td>
        <td><code></code></td>
        <td><p>Optional keyword arguments</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, Any]</code></td>
      <td><p>Dict[str, Any]: The fuser configuration</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">fuser_cfg</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Static method to create the fuser configuration</span>

<span class="sd">    The default configuration is provided here</span>
<span class="sd">    This configuration corresponds to the official paper implementation</span>
<span class="sd">    and is tuned for CMU MOSEI.</span>

<span class="sd">    Args:</span>
<span class="sd">        **cfg: Optional keyword arguments</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, Any]: The fuser configuration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;n_modalities&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
        <span class="s2">&quot;output_size&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
        <span class="s2">&quot;fusion_method&quot;</span><span class="p">:</span> <span class="s2">&quot;attention&quot;</span><span class="p">,</span>
        <span class="s2">&quot;timesteps_pooling_method&quot;</span><span class="p">:</span> <span class="s2">&quot;rnn&quot;</span><span class="p">,</span>
        <span class="s2">&quot;residual&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;residual&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;use_all_trimodal&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;use_all_trimodal&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">}</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.MultimodalBaselineClassifier" class="doc doc-heading">
        <code>MultimodalBaselineClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.MultimodalBaselineClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mods</span> <span class="o">=</span> <span class="p">[</span><span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;audio&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;visual&quot;</span><span class="p">]]</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.TrimodalClassifier" class="doc doc-heading">
        <code>TrimodalClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.TrimodalClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mods</span> <span class="o">=</span> <span class="p">[</span><span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;audio&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;visual&quot;</span><span class="p">]]</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.TrimodalEncoder" class="doc doc-heading">
        <code>TrimodalEncoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.multimodal.TrimodalEncoder.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Output feature size</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Output feature size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.TrimodalEncoder.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder1_args</span><span class="p">,</span> <span class="n">encoder2_args</span><span class="p">,</span> <span class="n">encoder3_args</span><span class="p">,</span> <span class="n">fuser_args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Two modality encoder</p>
<p>Encode + Fuse three input modalities</p>
<p>Example encoder_args:
    {
        "input_size": 35,
        "hidden_size": 100,
        "layers": 1,
        "bidirectional": True,
        "dropout": 0.2,
        "rnn_type": "lstm",
        "attention": True,
    }</p>
<p>Example fuser_args:
    {
        "n_modalities": 3,
        "dropout": 0.2,
        "output_size": 100,
        "hidden_size": 100,
        "fusion_method": "cat",
        "timesteps_pooling_method": "rnn",
    }</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>encoder1_args</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Configuration for first encoder</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>encoder2_args</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Configuration for second encoder</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>encoder3_args</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Configuration for third encoder</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>fuser_args</code></td>
        <td><code>Dict[str, Any]</code></td>
        <td><p>Configuration for fuser</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">encoder1_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">encoder2_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">encoder3_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="n">fuser_args</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Two modality encoder</span>

<span class="sd">    Encode + Fuse three input modalities</span>

<span class="sd">    Example encoder_args:</span>
<span class="sd">        {</span>
<span class="sd">            &quot;input_size&quot;: 35,</span>
<span class="sd">            &quot;hidden_size&quot;: 100,</span>
<span class="sd">            &quot;layers&quot;: 1,</span>
<span class="sd">            &quot;bidirectional&quot;: True,</span>
<span class="sd">            &quot;dropout&quot;: 0.2,</span>
<span class="sd">            &quot;rnn_type&quot;: &quot;lstm&quot;,</span>
<span class="sd">            &quot;attention&quot;: True,</span>
<span class="sd">        }</span>

<span class="sd">    Example fuser_args:</span>
<span class="sd">        {</span>
<span class="sd">            &quot;n_modalities&quot;: 3,</span>
<span class="sd">            &quot;dropout&quot;: 0.2,</span>
<span class="sd">            &quot;output_size&quot;: 100,</span>
<span class="sd">            &quot;hidden_size&quot;: 100,</span>
<span class="sd">            &quot;fusion_method&quot;: &quot;cat&quot;,</span>
<span class="sd">            &quot;timesteps_pooling_method&quot;: &quot;rnn&quot;,</span>
<span class="sd">        }</span>

<span class="sd">    Args:</span>
<span class="sd">        encoder1_args (Dict[str, Any]): Configuration for first encoder</span>
<span class="sd">        encoder2_args (Dict[str, Any]): Configuration for second encoder</span>
<span class="sd">        encoder3_args (Dict[str, Any]): Configuration for third encoder</span>
<span class="sd">        fuser_args (Dict[str, Any]): Configuration for fuser</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TrimodalEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">encoder1_args</span><span class="p">,</span>
        <span class="n">encoder2_args</span><span class="p">,</span>
        <span class="n">encoder3_args</span><span class="p">,</span>
        <span class="n">fuser_args</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="s2">&quot;input_projection&quot;</span> <span class="ow">in</span> <span class="n">fuser_args</span> <span class="ow">and</span> <span class="n">fuser_args</span><span class="p">[</span><span class="s2">&quot;input_projection&quot;</span><span class="p">]:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">input_projection</span> <span class="o">=</span> <span class="n">ModalityProjection</span><span class="p">(</span>
            <span class="p">[</span><span class="n">encoder1_args</span><span class="p">[</span><span class="s2">&quot;input_size&quot;</span><span class="p">],</span> <span class="n">encoder2_args</span><span class="p">[</span><span class="s2">&quot;input_size&quot;</span><span class="p">]],</span>
            <span class="n">fuser_args</span><span class="p">[</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">],</span>
            <span class="n">mode</span><span class="o">=</span><span class="n">fuser_args</span><span class="p">[</span><span class="s2">&quot;input_projection&quot;</span><span class="p">],</span>
        <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder1</span> <span class="o">=</span> <span class="n">UnimodalEncoder</span><span class="p">(</span><span class="o">**</span><span class="n">encoder1_args</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder2</span> <span class="o">=</span> <span class="n">UnimodalEncoder</span><span class="p">(</span><span class="o">**</span><span class="n">encoder2_args</span><span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">encoder3</span> <span class="o">=</span> <span class="n">UnimodalEncoder</span><span class="p">(</span><span class="o">**</span><span class="n">encoder3_args</span><span class="p">)</span>
    <span class="c1"># encoder3_args[&quot;input_size&quot;], encoder3_args[&quot;hidden_size&quot;], **encoder3_args</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">fuse</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_make_fusion_pipeline</span><span class="p">(</span>
        <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">encoder1</span><span class="o">.</span><span class="n">out_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder2</span><span class="o">.</span><span class="n">out_size</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder3</span><span class="o">.</span><span class="n">out_size</span><span class="p">],</span>
        <span class="o">**</span><span class="n">fuser_args</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.UnimodalClassifier" class="doc doc-heading">
        <code>UnimodalClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.UnimodalClassifier.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Encode and classify unimodal inputs</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>input_size</code></td>
        <td><code>int</code></td>
        <td><p>The input modality feature size</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>Hidden size for RNN</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_classes</code></td>
        <td><code>int</code></td>
        <td><p>The number of target classes</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>layers</code></td>
        <td><code>int</code></td>
        <td><p>Number of RNN layers</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use biRNN</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>rnn_type</code></td>
        <td><code>str</code></td>
        <td><p>[lstm|gru]</p></td>
        <td><code>&#39;lstm&#39;</code></td>
      </tr>
      <tr>
        <td><code>attention</code></td>
        <td><code>bool</code></td>
        <td><p>Use attention on hidden states</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Encode and classify unimodal inputs</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): The input modality feature size</span>
<span class="sd">        hidden_size (int): Hidden size for RNN</span>
<span class="sd">        num_classes (int): The number of target classes</span>
<span class="sd">        layers (int): Number of RNN layers</span>
<span class="sd">        bidirectional (bool): Use biRNN</span>
<span class="sd">        dropout (float): Dropout probability</span>
<span class="sd">        rnn_type (str): [lstm|gru]</span>
<span class="sd">        attention (bool): Use attention on hidden states</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">enc</span> <span class="o">=</span> <span class="n">UnimodalEncoder</span><span class="p">(</span>
        <span class="n">input_size</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
        <span class="n">aggregate_encoded</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">UnimodalClassifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">enc</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.UnimodalClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.UnimodalEncoder" class="doc doc-heading">
        <code>UnimodalEncoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.multimodal.UnimodalEncoder.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Output feature size</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: Output feature size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.UnimodalEncoder.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">merge_bi</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">aggregate_encoded</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Single modality encoder</p>
<p>Encode a single modality using an Attentive RNN</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>input_size</code></td>
        <td><code>int</code></td>
        <td><p>Input feature size</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>RNN hidden size</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>layers</code></td>
        <td><code>int</code></td>
        <td><p>Number of RNN layers. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use bidirectional RNN. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>rnn_type</code></td>
        <td><code>str</code></td>
        <td><p>lstm or gru. Defaults to "lstm".</p></td>
        <td><code>&#39;lstm&#39;</code></td>
      </tr>
      <tr>
        <td><code>attention</code></td>
        <td><code>bool</code></td>
        <td><p>Use attention over hidden states. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>merge_bi</code></td>
        <td><code>str</code></td>
        <td><p>How to merge hidden states [sum|cat]. Defaults to sum.</p></td>
        <td><code>&#39;sum&#39;</code></td>
      </tr>
      <tr>
        <td><code>aggregate_encoded</code></td>
        <td><code>bool</code></td>
        <td><p>Aggregate hidden states. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">merge_bi</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
    <span class="n">aggregate_encoded</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Single modality encoder</span>

<span class="sd">    Encode a single modality using an Attentive RNN</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Input feature size</span>
<span class="sd">        hidden_size (int): RNN hidden size</span>
<span class="sd">        layers (int, optional): Number of RNN layers. Defaults to 1.</span>
<span class="sd">        bidirectional (bool, optional): Use bidirectional RNN. Defaults to True.</span>
<span class="sd">        dropout (float, optional): Dropout probability. Defaults to 0.2.</span>
<span class="sd">        rnn_type (str, optional): lstm or gru. Defaults to &quot;lstm&quot;.</span>
<span class="sd">        attention (bool, optional): Use attention over hidden states. Defaults to True.</span>
<span class="sd">        merge_bi (str, optional): How to merge hidden states [sum|cat]. Defaults to sum.</span>
<span class="sd">        aggregate_encoded (bool, optional): Aggregate hidden states. Defaults to False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">UnimodalEncoder</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">input_size</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
        <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">aggregate_encoded</span> <span class="o">=</span> <span class="n">aggregate_encoded</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">AttentiveRNN</span><span class="p">(</span>
        <span class="n">input_size</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">merge_bi</span><span class="o">=</span><span class="n">merge_bi</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">packed_sequence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
        <span class="n">return_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.VisualEncoder" class="doc doc-heading">
        <code>VisualEncoder</code>



</h2>

    <div class="doc doc-contents ">

      <p>Alias for Unimodal Encoder</p>



    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.multimodal.VisualTextClassifier" class="doc doc-heading">
        <code>VisualTextClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.multimodal.VisualTextClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/multimodal.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mods</span> <span class="o">=</span> <span class="p">[</span><span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;visual&quot;</span><span class="p">]]</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>

<h2 id="m3">M3</h2>


  <div class="doc doc-object doc-module">

<a id="slp.modules.mmdrop"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.mmdrop.HardMultimodalDropout" class="doc doc-heading">
        <code>HardMultimodalDropout</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmdrop.HardMultimodalDropout.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_modalities</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">p_mod</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>MMDrop initial implementation</p>
<p>For each sample in a batch drop one of the modalities with probability p</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>p</code></td>
        <td><code>float</code></td>
        <td><p>drop probability</p></td>
        <td><code>0.5</code></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>number of modalities</p></td>
        <td><code>3</code></td>
      </tr>
      <tr>
        <td><code>p_mod</code></td>
        <td><code>Optional[List[float]]</code></td>
        <td><p>Drop probabilities for each modality</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/mmdrop.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">p_mod</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;MMDrop initial implementation</span>

<span class="sd">    For each sample in a batch drop one of the modalities with probability p</span>

<span class="sd">    Args:</span>
<span class="sd">        p (float): drop probability</span>
<span class="sd">        n_modalities (int): number of modalities</span>
<span class="sd">        p_mod (Optional[List[float]]): Drop probabilities for each modality</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">HardMultimodalDropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_modalities</span> <span class="o">=</span> <span class="n">n_modalities</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">p_mod</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">n_modalities</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_modalities</span><span class="p">)]</span>

    <span class="k">if</span> <span class="n">p_mod</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_mod</span> <span class="o">=</span> <span class="n">p_mod</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmdrop.HardMultimodalDropout.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Naive mmdrop forward</p>
<p>Iterate over batch and randomly choose modality to drop</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>mods</code></td>
        <td><code>varargs torch.Tensor</code></td>
        <td><p>[B, L, D_m] Modality representations</p></td>
        <td><code>()</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(List[torch.Tensor])</code></td>
      <td><p>The modality representations. Some of them are dropped</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/mmdrop.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Naive mmdrop forward</span>

<span class="sd">    Iterate over batch and randomly choose modality to drop</span>

<span class="sd">    Args:</span>
<span class="sd">        mods (varargs torch.Tensor): [B, L, D_m] Modality representations</span>

<span class="sd">    Returns:</span>
<span class="sd">        (List[torch.Tensor]): The modality representations. Some of them are dropped</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mods</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">mods</span><span class="p">)</span>

    <span class="c1"># List of [B, L, D]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">:</span>
            <span class="c1"># Drop different modality for each sample in batch</span>

            <span class="k">for</span> <span class="n">batch</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">mods</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">)):</span>
                <span class="n">m</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">(</span>
                    <span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_modalities</span><span class="p">)),</span> <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p_mod</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span>
                <span class="p">)[</span><span class="mi">0</span><span class="p">]</span>

                <span class="c1"># m = random.randint(0, self.n_modalities - 1)</span>
                <span class="n">mask</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">ones_like</span><span class="p">(</span><span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">])</span>
                <span class="n">mask</span><span class="p">[</span><span class="n">batch</span><span class="p">]</span> <span class="o">=</span> <span class="mf">0.0</span>
                <span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
            <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">mods</span><span class="p">)):</span>
                <span class="n">keep_prob</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_modalities</span><span class="p">)</span>
                <span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span> <span class="o">/</span> <span class="n">keep_prob</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">mods</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.mmdrop.MultimodalDropout" class="doc doc-heading">
        <code>MultimodalDropout</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmdrop.MultimodalDropout.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_modalities</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">p_mod</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>mmdrop wrapper class</p>
<p>Drop p * 100 % of features of a specific modality over batch</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>p</code></td>
        <td><code>float</code></td>
        <td><p>drop probability</p></td>
        <td><code>0.5</code></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>number of modalities</p></td>
        <td><code>3</code></td>
      </tr>
      <tr>
        <td><code>p_mod</code></td>
        <td><code>Optional[List[float]]</code></td>
        <td><p>Drop probabilities for each modality</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>mode</code></td>
        <td><code>str</code></td>
        <td><p>Hard or soft mmdrop</p></td>
        <td><code>&#39;hard&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/mmdrop.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span>
    <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span>
    <span class="n">p_mod</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mode</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;hard&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;mmdrop wrapper class</span>

<span class="sd">    Drop p * 100 % of features of a specific modality over batch</span>

<span class="sd">    Args:</span>
<span class="sd">        p (float): drop probability</span>
<span class="sd">        n_modalities (int): number of modalities</span>
<span class="sd">        p_mod (Optional[List[float]]): Drop probabilities for each modality</span>
<span class="sd">        mode (str): Hard or soft mmdrop</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultimodalDropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">assert</span> <span class="n">mode</span> <span class="ow">in</span> <span class="p">[</span>
        <span class="s2">&quot;hard&quot;</span><span class="p">,</span>
        <span class="s2">&quot;soft&quot;</span><span class="p">,</span>
    <span class="p">],</span> <span class="s2">&quot;Allowed mode for MultimodalDropout [&#39;hard&#39; | &#39;soft&#39;]&quot;</span>

    <span class="k">if</span> <span class="n">mode</span> <span class="o">==</span> <span class="s2">&quot;hard&quot;</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span> <span class="o">=</span> <span class="n">HardMultimodalDropout</span><span class="p">(</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">n_modalities</span><span class="o">=</span><span class="n">n_modalities</span><span class="p">,</span> <span class="n">p_mod</span><span class="o">=</span><span class="n">p_mod</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span> <span class="o">=</span> <span class="n">SoftMultimodalDropout</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
            <span class="n">p</span><span class="o">=</span><span class="n">p</span><span class="p">,</span> <span class="n">n_modalities</span><span class="o">=</span><span class="n">n_modalities</span><span class="p">,</span> <span class="n">p_mod</span><span class="o">=</span><span class="n">p_mod</span>
        <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmdrop.MultimodalDropout.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>mmdrop wrapper forward</p>
<p>Perform hard or soft mmdrop</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>mods</code></td>
        <td><code>varargs torch.Tensor</code></td>
        <td><p>[B, L, D_m] Modality representations</p></td>
        <td><code>()</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(List[torch.Tensor])</code></td>
      <td><p>The modality representations. Some of them are dropped</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/mmdrop.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;mmdrop wrapper forward</span>

<span class="sd">    Perform hard or soft mmdrop</span>

<span class="sd">    Args:</span>
<span class="sd">        mods (varargs torch.Tensor): [B, L, D_m] Modality representations</span>

<span class="sd">    Returns:</span>
<span class="sd">        (List[torch.Tensor]): The modality representations. Some of them are dropped</span>

<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.mmdrop.SoftMultimodalDropout" class="doc doc-heading">
        <code>SoftMultimodalDropout</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmdrop.SoftMultimodalDropout.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="mf">0.5</span><span class="p">,</span> <span class="n">n_modalities</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">p_mod</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Soft mmdrop implementation</p>
<p>Drop p * 100 % of features of a specific modality over batch</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>p</code></td>
        <td><code>float</code></td>
        <td><p>drop probability</p></td>
        <td><code>0.5</code></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>number of modalities</p></td>
        <td><code>3</code></td>
      </tr>
      <tr>
        <td><code>p_mod</code></td>
        <td><code>Optional[List[float]]</code></td>
        <td><p>Drop probabilities for each modality</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/mmdrop.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">p</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.5</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">3</span><span class="p">,</span> <span class="n">p_mod</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Soft mmdrop implementation</span>

<span class="sd">    Drop p * 100 % of features of a specific modality over batch</span>

<span class="sd">    Args:</span>
<span class="sd">        p (float): drop probability</span>
<span class="sd">        n_modalities (int): number of modalities</span>
<span class="sd">        p_mod (Optional[List[float]]): Drop probabilities for each modality</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SoftMultimodalDropout</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">=</span> <span class="n">p</span>  <span class="c1"># p_drop</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_modalities</span> <span class="o">=</span> <span class="n">n_modalities</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">p_mod</span> <span class="o">=</span> <span class="p">[</span><span class="mf">1.0</span> <span class="o">/</span> <span class="n">n_modalities</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_modalities</span><span class="p">)]</span>

    <span class="k">if</span> <span class="n">p_mod</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">p_mod</span> <span class="o">=</span> <span class="n">p_mod</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmdrop.SoftMultimodalDropout.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Soft mmdrop forward</p>
<p>Sample a binomial mask to mask a random modality in this batch</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>mods</code></td>
        <td><code>varargs torch.Tensor</code></td>
        <td><p>[B, L, D_m] Modality representations</p></td>
        <td><code>()</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>(List[torch.Tensor])</code></td>
      <td><p>The modality representations. Some of them are dropped</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/mmdrop.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Soft mmdrop forward</span>

<span class="sd">    Sample a binomial mask to mask a random modality in this batch</span>

<span class="sd">    Args:</span>
<span class="sd">        mods (varargs torch.Tensor): [B, L, D_m] Modality representations</span>

<span class="sd">    Returns:</span>
<span class="sd">        (List[torch.Tensor]): The modality representations. Some of them are dropped</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mods</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="n">mods</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
        <span class="c1"># m = random.randint(0, self.n_modalities - 1)</span>
        <span class="n">m</span> <span class="o">=</span> <span class="n">random</span><span class="o">.</span><span class="n">choices</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_modalities</span><span class="p">)),</span> <span class="n">weights</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">p_mod</span><span class="p">,</span> <span class="n">k</span><span class="o">=</span><span class="mi">1</span><span class="p">)[</span>
            <span class="mi">0</span>
        <span class="p">]</span>

        <span class="n">binomial</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">distributions</span><span class="o">.</span><span class="n">binomial</span><span class="o">.</span><span class="n">Binomial</span><span class="p">(</span><span class="n">probs</span><span class="o">=</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span><span class="p">)</span>
        <span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="n">binomial</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">]</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_modalities</span><span class="p">):</span>
            <span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">=</span> <span class="n">mods</span><span class="p">[</span><span class="n">m</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mf">1.0</span> <span class="o">/</span> <span class="p">(</span><span class="mi">1</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">p</span> <span class="o">/</span> <span class="bp">self</span><span class="o">.</span><span class="n">n_modalities</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">mods</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.m3"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.m3.M3" class="doc doc-heading">
        <code>M3</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.m3.M3.encoder_cfg" class="doc doc-heading">
<code class="highlight language-python"><span class="n">encoder_cfg</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Static method to create the encoder configuration</p>
<p>The default configuration is provided here
This configuration corresponds to the official paper implementation
and is tuned for CMU MOSEI.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>input_size</code></td>
        <td><code>int</code></td>
        <td><p>Input modality size</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**cfg</code></td>
        <td><code></code></td>
        <td><p>Optional keyword arguments</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, Any]</code></td>
      <td><p>Dict[str, Any]: The encoder configuration</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/m3.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">encoder_cfg</span><span class="p">(</span><span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Static method to create the encoder configuration</span>

<span class="sd">    The default configuration is provided here</span>
<span class="sd">    This configuration corresponds to the official paper implementation</span>
<span class="sd">    and is tuned for CMU MOSEI.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Input modality size</span>
<span class="sd">        **cfg: Optional keyword arguments</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, Any]: The encoder configuration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="n">input_size</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
        <span class="s2">&quot;layers&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layers&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bidirectional&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
        <span class="s2">&quot;rnn_type&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;rnn_type&quot;</span><span class="p">,</span> <span class="s2">&quot;lstm&quot;</span><span class="p">),</span>
        <span class="s2">&quot;attention&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">}</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.m3.M3.fuser_cfg" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fuser_cfg</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Static method to create the fuser configuration</p>
<p>The default configuration is provided here
This configuration corresponds to the official paper implementation
and is tuned for CMU MOSEI.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>**cfg</code></td>
        <td><code></code></td>
        <td><p>Optional keyword arguments</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, Any]</code></td>
      <td><p>Dict[str, Any]: The fuser configuration</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/m3.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">fuser_cfg</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Static method to create the fuser configuration</span>

<span class="sd">    The default configuration is provided here</span>
<span class="sd">    This configuration corresponds to the official paper implementation</span>
<span class="sd">    and is tuned for CMU MOSEI.</span>

<span class="sd">    Args:</span>
<span class="sd">        **cfg: Optional keyword arguments</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, Any]: The fuser configuration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;n_modalities&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
        <span class="s2">&quot;output_size&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
        <span class="s2">&quot;fusion_method&quot;</span><span class="p">:</span> <span class="s2">&quot;attention&quot;</span><span class="p">,</span>
        <span class="s2">&quot;timesteps_pooling_method&quot;</span><span class="p">:</span> <span class="s2">&quot;rnn&quot;</span><span class="p">,</span>
        <span class="s2">&quot;residual&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;residual&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;use_all_trimodal&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;use_all_trimodal&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;mmdrop_prob&quot;</span><span class="p">:</span> <span class="mf">0.2</span><span class="p">,</span>
        <span class="s2">&quot;mmdrop_individual_mod_prob&quot;</span><span class="p">:</span> <span class="kc">None</span><span class="p">,</span>
        <span class="s2">&quot;mmdrop_algorithm&quot;</span><span class="p">:</span> <span class="s2">&quot;hard&quot;</span><span class="p">,</span>
    <span class="p">}</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.m3.M3Classifier" class="doc doc-heading">
        <code>M3Classifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.m3.M3Classifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/m3.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mods</span> <span class="o">=</span> <span class="p">[</span><span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;audio&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;visual&quot;</span><span class="p">]]</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.m3.M3FuseAggregate" class="doc doc-heading">
        <code>M3FuseAggregate</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.m3.M3FuseAggregate.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Define the feature size of the returned tensor</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: The feature dimension of the output tensor</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.m3.M3FuseAggregate.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">feature_size</span><span class="p">,</span> <span class="n">n_modalities</span><span class="p">,</span> <span class="n">output_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">fusion_method</span><span class="o">=</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">timesteps_pooling_method</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">mmdrop_prob</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">mmdrop_individual_mod_prob</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">mmdrop_algorithm</span><span class="o">=</span><span class="s1">&#39;hard&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">fuser_kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>MultimodalDropout, Fuse input feature sequences and aggregate across timesteps</p>
<p>MultimodalDropout -&gt; Fuser -&gt; TimestepsPooler</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>feature_size</code></td>
        <td><code>int</code></td>
        <td><p>The input modality representations dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of input modalities</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>output_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Required output size. If not provided,
output_size = fuser.out_size</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>fusion_method</code></td>
        <td><code>str</code></td>
        <td><p>Select which fuser to use [cat|sum|attention|bilinear]</p></td>
        <td><code>&#39;cat&#39;</code></td>
      </tr>
      <tr>
        <td><code>timesteps_pooling_method</code></td>
        <td><code>str</code></td>
        <td><p>TimestepsPooler method [cat|sum|rnn]</p></td>
        <td><code>&#39;sum&#39;</code></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Input tensors are in batch first configuration. Leave this as true
except if you know what you are doing</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>mmdrop_prob</code></td>
        <td><code>float</code></td>
        <td><p>The probability for multimodal dropout. Defaults to 0.2</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>mmdrop_individual_mod_prob</code></td>
        <td><code>Optional[List[float]]</code></td>
        <td><p>Drop probabilities for each modality
for multimodal dropout. If None all modalities are dropped with equal probability</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>mmdrop_algorithm</code></td>
        <td><code>str</code></td>
        <td><p>Choose multimodal dropout algorithm [hard|soft]. Defaults to hard</p></td>
        <td><code>&#39;hard&#39;</code></td>
      </tr>
      <tr>
        <td><code>**fuser_kwargs</code></td>
        <td><code>dict</code></td>
        <td><p>Extra keyword arguments to instantiate fuser</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/m3.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">feature_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">n_modalities</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">output_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">fusion_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">timesteps_pooling_method</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">mmdrop_prob</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">mmdrop_individual_mod_prob</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">List</span><span class="p">[</span><span class="nb">float</span><span class="p">]]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">mmdrop_algorithm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;hard&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">fuser_kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;MultimodalDropout, Fuse input feature sequences and aggregate across timesteps</span>

<span class="sd">    MultimodalDropout -&gt; Fuser -&gt; TimestepsPooler</span>

<span class="sd">    Args:</span>
<span class="sd">        feature_size (int): The input modality representations dimension</span>
<span class="sd">        n_modalities (int): Number of input modalities</span>
<span class="sd">        output_size (Optional[int]): Required output size. If not provided,</span>
<span class="sd">            output_size = fuser.out_size</span>
<span class="sd">        fusion_method (str): Select which fuser to use [cat|sum|attention|bilinear]</span>
<span class="sd">        timesteps_pooling_method (str): TimestepsPooler method [cat|sum|rnn]</span>
<span class="sd">        batch_first (bool): Input tensors are in batch first configuration. Leave this as true</span>
<span class="sd">            except if you know what you are doing</span>
<span class="sd">        mmdrop_prob (float): The probability for multimodal dropout. Defaults to 0.2</span>
<span class="sd">        mmdrop_individual_mod_prob (Optional[List[float]]): Drop probabilities for each modality</span>
<span class="sd">            for multimodal dropout. If None all modalities are dropped with equal probability</span>
<span class="sd">        mmdrop_algorithm (str): Choose multimodal dropout algorithm [hard|soft]. Defaults to hard</span>
<span class="sd">        **fuser_kwargs (dict): Extra keyword arguments to instantiate fuser</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">M3FuseAggregate</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">m3</span> <span class="o">=</span> <span class="n">MultimodalDropout</span><span class="p">(</span>
        <span class="n">p</span><span class="o">=</span><span class="n">mmdrop_prob</span><span class="p">,</span>
        <span class="n">n_modalities</span><span class="o">=</span><span class="n">n_modalities</span><span class="p">,</span>
        <span class="n">p_mod</span><span class="o">=</span><span class="n">mmdrop_individual_mod_prob</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">mmdrop_algorithm</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">fuser_kwargs</span><span class="p">[</span><span class="s2">&quot;output_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">output_size</span>
    <span class="n">fuser_kwargs</span><span class="p">[</span><span class="s2">&quot;fusion_method&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">fusion_method</span>
    <span class="n">fuser_kwargs</span><span class="p">[</span><span class="s2">&quot;timesteps_pooling_method&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">timesteps_pooling_method</span>
    <span class="n">fuser_kwargs</span><span class="p">[</span><span class="s2">&quot;batch_first&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">batch_first</span>

    <span class="k">if</span> <span class="s2">&quot;n_modalities&quot;</span> <span class="ow">in</span> <span class="n">fuser_kwargs</span><span class="p">:</span>
        <span class="n">fuser_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;n_modalities&quot;</span><span class="p">)</span>  <span class="c1"># Avoid multiple arguments</span>

    <span class="k">if</span> <span class="s2">&quot;projection_size&quot;</span> <span class="ow">in</span> <span class="n">fuser_kwargs</span><span class="p">:</span>
        <span class="n">fuser_kwargs</span><span class="o">.</span><span class="n">pop</span><span class="p">(</span><span class="s2">&quot;projection_size&quot;</span><span class="p">)</span>  <span class="c1"># Avoid multiple arguments</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">fuse_aggregate</span> <span class="o">=</span> <span class="n">FuseAggregateTimesteps</span><span class="p">(</span>
        <span class="n">feature_size</span><span class="p">,</span>
        <span class="n">n_modalities</span><span class="p">,</span>
        <span class="o">**</span><span class="n">fuser_kwargs</span><span class="p">,</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.m3.M3FuseAggregate.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Fuse the modality representations and aggregate across timesteps</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>List of modality tensors [B, L, D]</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Lengths of each modality</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Fused tensor [B, self.out_size]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/m3.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Fuse the modality representations and aggregate across timesteps</span>

<span class="sd">    Args:</span>
<span class="sd">        *mods: List of modality tensors [B, L, D]</span>
<span class="sd">        lengths (Optional[Tensor]): Lengths of each modality</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Fused tensor [B, self.out_size]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mods_masked</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">m3</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">)</span>
    <span class="n">fused</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fuse_aggregate</span><span class="p">(</span><span class="o">*</span><span class="n">mods_masked</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fused</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>

<h2 id="multimodal-feedback">Multimodal Feedback</h2>


  <div class="doc doc-object doc-module">

<a id="slp.modules.feedback"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-class">



<h2 id="slp.modules.feedback.BaseFeedbackUnit" class="doc doc-heading">
        <code>BaseFeedbackUnit</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedback.BaseFeedbackUnit.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span> <span class="n">n_top_modalities</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Base class for feedback unit</p>
<p>Feedback units are responsible for projecting top-level crossmodal
representations to bottom-level features and applying the top-down masks</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>top_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the top-level representations</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>target_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the bottom-level features</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>n_top_modalities</code></td>
        <td><code>int</code></td>
        <td><p>Number of modalities to use for feedback</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedback.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">target_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">n_top_modalities</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Base class for feedback unit</span>

<span class="sd">    Feedback units are responsible for projecting top-level crossmodal</span>
<span class="sd">    representations to bottom-level features and applying the top-down masks</span>

<span class="sd">    Args:</span>
<span class="sd">        top_size (int): Feature size of the top-level representations</span>
<span class="sd">        target_size (int): Feature size of the bottom-level features</span>
<span class="sd">        n_top_modalities (int): Number of modalities to use for feedback</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">BaseFeedbackUnit</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">n_</span> <span class="o">=</span> <span class="n">n_top_modalities</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">mask_layers</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">make_mask_layer</span><span class="p">(</span><span class="n">top_size</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">n_</span><span class="p">)</span>
        <span class="p">]</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedback.BaseFeedbackUnit.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x_bottom</span><span class="p">,</span> <span class="o">*</span><span class="n">mods_top</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Apply the top-down masks to the input feature vector</p>
<p>x = x * top_down_mask</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x_bottom</code></td>
        <td><code>Tensor</code></td>
        <td><p>Bottom-level features [B, L, target_size]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>*mods_top</code></td>
        <td><code>Tensor</code></td>
        <td><p>Top-level modality representations</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Original unpadded tensor lengths. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Masked low level feature tensor [B, L, target_size]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedback.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x_bottom</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="o">*</span><span class="n">mods_top</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Apply the top-down masks to the input feature vector</span>

<span class="sd">    x = x * top_down_mask</span>

<span class="sd">    Args:</span>
<span class="sd">        x_bottom (torch.Tensor): Bottom-level features [B, L, target_size]</span>
<span class="sd">        *mods_top (torch.Tensor): Top-level modality representations</span>
<span class="sd">        lengths (Optional[torch.Tensor], optional): Original unpadded tensor lengths. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Masked low level feature tensor [B, L, target_size]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">mask</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_get_feedback_mask</span><span class="p">(</span><span class="o">*</span><span class="n">mods_top</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>
    <span class="n">x_bottom</span> <span class="o">=</span> <span class="n">x_bottom</span> <span class="o">*</span> <span class="n">mask</span>

    <span class="k">return</span> <span class="n">x_bottom</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedback.BaseFeedbackUnit.make_mask_layer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">make_mask_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Abstract method to instantiate the layer to use for top-down feedback</p>
<p>To be implemented by subclasses</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>top_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the top-level representations</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>target_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the bottom-level features</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td><code></code></td>
        <td><p>extra configuration for the feedback layer</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>nn.Module: The instanstiated feedback layer</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedback.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@abc</span><span class="o">.</span><span class="n">abstractmethod</span>
<span class="k">def</span> <span class="nf">make_mask_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">target_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Abstract method to instantiate the layer to use for top-down feedback</span>

<span class="sd">    To be implemented by subclasses</span>

<span class="sd">    Args:</span>
<span class="sd">        top_size (int): Feature size of the top-level representations</span>
<span class="sd">        target_size (int): Feature size of the bottom-level features</span>
<span class="sd">        **kwargs: extra configuration for the feedback layer</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: The instanstiated feedback layer</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">pass</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.feedback.BoomFeedbackUnit" class="doc doc-heading">
        <code>BoomFeedbackUnit</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedback.BoomFeedbackUnit.make_mask_layer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">make_mask_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Use an boom module for top-down projection</p>
<p>A boom module is a two-layer MLP where the inner projection size is
much larger than the input and output size. (similar to Position feedforward in transformers)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>top_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the top-level representations</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>target_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the bottom-level features</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td><code></code></td>
        <td><p>extra configuration for the feedback layer</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>nn.Module</code></td>
      <td><p>slp.modules.feedforward.TwoLayer instance</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedback.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">make_mask_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Use an boom module for top-down projection</span>

<span class="sd">    A boom module is a two-layer MLP where the inner projection size is</span>
<span class="sd">    much larger than the input and output size. (similar to Position feedforward in transformers)</span>

<span class="sd">    Args:</span>
<span class="sd">        top_size (int): Feature size of the top-level representations</span>
<span class="sd">        target_size (int): Feature size of the bottom-level features</span>
<span class="sd">        **kwargs: extra configuration for the feedback layer</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: slp.modules.feedforward.TwoLayer instance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">TwoLayer</span><span class="p">(</span>
        <span class="n">top_size</span><span class="p">,</span>
        <span class="mi">2</span> <span class="o">*</span> <span class="n">top_size</span><span class="p">,</span>
        <span class="n">target_size</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="s2">&quot;gelu&quot;</span><span class="p">),</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.feedback.DownUpFeedbackUnit" class="doc doc-heading">
        <code>DownUpFeedbackUnit</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedback.DownUpFeedbackUnit.make_mask_layer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">make_mask_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Use an down-up module for top-down projection</p>
<p>A down-up module is a two-layer MLP where the inner projection size is
much smaller than the input and output size. (Similar to adapyers)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>top_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the top-level representations</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>target_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the bottom-level features</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td><code></code></td>
        <td><p>extra configuration for the feedback layer</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>nn.Module</code></td>
      <td><p>slp.modules.feedforward.TwoLayer instance</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedback.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">make_mask_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Use an down-up module for top-down projection</span>

<span class="sd">    A down-up module is a two-layer MLP where the inner projection size is</span>
<span class="sd">    much smaller than the input and output size. (Similar to adapyers)</span>

<span class="sd">    Args:</span>
<span class="sd">        top_size (int): Feature size of the top-level representations</span>
<span class="sd">        target_size (int): Feature size of the bottom-level features</span>
<span class="sd">        **kwargs: extra configuration for the feedback layer</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: slp.modules.feedforward.TwoLayer instance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">TwoLayer</span><span class="p">(</span>
        <span class="n">top_size</span><span class="p">,</span>
        <span class="n">top_size</span> <span class="o">//</span> <span class="mi">5</span><span class="p">,</span>
        <span class="n">target_size</span><span class="p">,</span>
        <span class="n">activation</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;activation&quot;</span><span class="p">,</span> <span class="s2">&quot;gelu&quot;</span><span class="p">),</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.feedback.Feedback" class="doc doc-heading">
        <code>Feedback</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedback.Feedback.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">,</span> <span class="n">bottom_modality_sizes</span><span class="p">,</span> <span class="n">use_self</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">mask_type</span><span class="o">=</span><span class="s1">&#39;rnn&#39;</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Feedback module</p>
<p>Given a list of low-level features and top-level representations for n modalities:</p>
<ul>
<li>Create top-down masks for each modality</li>
<li>Apply top-down masks to the low level features</li>
<li>Return masked low-level features</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>top_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size for top-level representations (Common across modalities)</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>bottom_modality_sizes</code></td>
        <td><code>List[int]</code></td>
        <td><p>List of feature sizes for each low-level modality feature</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>use_self</code></td>
        <td><code>bool</code></td>
        <td><p>Include the self modality when creating the top-down mask. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>mask_type</code></td>
        <td><code>str</code></td>
        <td><p>Which feedback unit to use [rnn|gated|boom|downup]. Defaults to "rnn".</p></td>
        <td><code>&#39;rnn&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedback.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">top_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">bottom_modality_sizes</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="nb">int</span><span class="p">],</span>
    <span class="n">use_self</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">mask_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rnn&quot;</span><span class="p">,</span>
    <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Feedback module</span>

<span class="sd">    Given a list of low-level features and top-level representations for n modalities:</span>

<span class="sd">    * Create top-down masks for each modality</span>
<span class="sd">    * Apply top-down masks to the low level features</span>
<span class="sd">    * Return masked low-level features</span>

<span class="sd">    Args:</span>
<span class="sd">        top_size (int): Feature size for top-level representations (Common across modalities)</span>
<span class="sd">        bottom_modality_sizes (List[int]): List of feature sizes for each low-level modality feature</span>
<span class="sd">        use_self (bool, optional): Include the self modality when creating the top-down mask. Defaults to False.</span>
<span class="sd">        mask_type (str, optional): Which feedback unit to use [rnn|gated|boom|downup]. Defaults to &quot;rnn&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Feedback</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="n">n_top_modalities</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">bottom_modality_sizes</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">use_self</span> <span class="o">=</span> <span class="n">use_self</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">use_self</span><span class="p">:</span>
        <span class="n">n_top_modalities</span> <span class="o">=</span> <span class="n">n_top_modalities</span> <span class="o">-</span> <span class="mi">1</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">feedback_units</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ModuleList</span><span class="p">(</span>
        <span class="p">[</span>
            <span class="n">_make_feedback_unit</span><span class="p">(</span>
                <span class="n">top_size</span><span class="p">,</span>
                <span class="n">bottom_modality_sizes</span><span class="p">[</span><span class="n">i</span><span class="p">],</span>
                <span class="n">n_top_modalities</span><span class="p">,</span>
                <span class="n">mask_type</span><span class="o">=</span><span class="n">mask_type</span><span class="p">,</span>
                <span class="o">**</span><span class="n">kwargs</span><span class="p">,</span>
            <span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">bottom_modality_sizes</span><span class="p">))</span>
        <span class="p">]</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedback.Feedback.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mods_bottom</span><span class="p">,</span> <span class="n">mods_top</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Create and apply the top-down masks to mods_bottom</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>mods_bottom</code></td>
        <td><code>List[torch.Tensor]</code></td>
        <td><p>Low-level features for each modality</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>mods_top</code></td>
        <td><code>List[torch.Tensor]</code></td>
        <td><p>High-level representations for each modality</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Original unpadded sequence lengths. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>List[torch.Tensor]</code></td>
      <td><p>List[torch.Tensor]: Masked low level features for each modality</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedback.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">mods_bottom</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">mods_top</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span>
    <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Create and apply the top-down masks to mods_bottom</span>

<span class="sd">    Args:</span>
<span class="sd">        mods_bottom (List[torch.Tensor]): Low-level features for each modality</span>
<span class="sd">        mods_top (List[torch.Tensor]): High-level representations for each modality</span>
<span class="sd">        lengths (Optional[torch.Tensor], optional): Original unpadded sequence lengths. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        List[torch.Tensor]: Masked low level features for each modality</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">bm</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">mods_bottom</span><span class="p">):</span>
        <span class="n">top</span> <span class="o">=</span> <span class="n">mods_top</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">use_self</span> <span class="k">else</span> <span class="n">mods_top</span><span class="p">[:</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="n">mods_top</span><span class="p">[</span><span class="n">i</span> <span class="o">+</span> <span class="mi">1</span> <span class="p">:]</span>
        <span class="n">masked</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback_units</span><span class="p">[</span><span class="n">i</span><span class="p">](</span><span class="n">bm</span><span class="p">,</span> <span class="o">*</span><span class="n">top</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>
        <span class="n">out</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">masked</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.feedback.GatedFeedbackUnit" class="doc doc-heading">
        <code>GatedFeedbackUnit</code>



</h2>

    <div class="doc doc-contents ">

      <p>Apply feedback mask using simple gating mechanism</p>
<div class="arithmatex">\[x_bottom = x_bottom * \frac{1}{2} [\sigma(W1 * y_top) + \sigma(W2 * z_top)]\]</div>




  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedback.GatedFeedbackUnit.make_mask_layer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">make_mask_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Use a simple nn.Linear layer for top-down projection</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>top_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the top-level representations</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>target_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the bottom-level features</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td><code></code></td>
        <td><p>extra configuration for the feedback layer</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>nn.Module: nn.Linear instance with dropout</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedback.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">make_mask_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">target_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Use a simple nn.Linear layer for top-down projection</span>


<span class="sd">    Args:</span>
<span class="sd">        top_size (int): Feature size of the top-level representations</span>
<span class="sd">        target_size (int): Feature size of the bottom-level features</span>
<span class="sd">        **kwargs: extra configuration for the feedback layer</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: nn.Linear instance with dropout</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">nn</span><span class="o">.</span><span class="n">Sequential</span><span class="p">(</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">top_size</span><span class="p">,</span> <span class="n">target_size</span><span class="p">),</span>
        <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">p</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">)),</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.feedback.RNNFeedbackUnit" class="doc doc-heading">
        <code>RNNFeedbackUnit</code>



</h2>

    <div class="doc doc-contents ">

      <p>Apply feedback mask using top-down RNN layers</p>
<div class="arithmatex">\[x_bottom = x_bottom * \frac{1}{2} [\sigma(RNN(y_top)) + \sigma(RNN(z_top))]\]</div>




  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedback.RNNFeedbackUnit.make_mask_layer" class="doc doc-heading">
<code class="highlight language-python"><span class="n">make_mask_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">,</span> <span class="n">target_size</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Use an RNN for top-down projection</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>top_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the top-level representations</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>target_size</code></td>
        <td><code>int</code></td>
        <td><p>Feature size of the bottom-level features</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**kwargs</code></td>
        <td><code></code></td>
        <td><p>extra configuration for the feedback layer</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Module</code></td>
      <td><p>nn.Module: slp.modules.rnn.AttentiveRNN instance</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedback.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">make_mask_layer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">top_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">target_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Use an RNN for top-down projection</span>


<span class="sd">    Args:</span>
<span class="sd">        top_size (int): Feature size of the top-level representations</span>
<span class="sd">        target_size (int): Feature size of the bottom-level features</span>
<span class="sd">        **kwargs: extra configuration for the feedback layer</span>

<span class="sd">    Returns:</span>
<span class="sd">        nn.Module: slp.modules.rnn.AttentiveRNN instance</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="n">AttentiveRNN</span><span class="p">(</span>
        <span class="n">top_size</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="n">target_size</span><span class="p">,</span>
        <span class="n">attention</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
        <span class="n">return_hidden</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bidirectional&quot;</span><span class="p">,</span> <span class="kc">False</span><span class="p">),</span>
        <span class="n">merge_bi</span><span class="o">=</span><span class="s2">&quot;sum&quot;</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">kwargs</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;rnn_type&quot;</span><span class="p">,</span> <span class="s2">&quot;lstm&quot;</span><span class="p">),</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.mmlatch"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.mmlatch.MMLatch" class="doc doc-heading">
        <code>MMLatch</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmlatch.MMLatch.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">text_size</span><span class="o">=</span><span class="mi">300</span><span class="p">,</span> <span class="n">audio_size</span><span class="o">=</span><span class="mi">74</span><span class="p">,</span> <span class="n">visual_size</span><span class="o">=</span><span class="mi">35</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">encoder_layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">merge_bi</span><span class="o">=</span><span class="s1">&#39;sum&#39;</span><span class="p">,</span> <span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">,</span> <span class="n">encoder_attention</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">fuser_residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_all_trimodal</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">feedback</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">use_self_feedback</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">feedback_algorithm</span><span class="o">=</span><span class="s1">&#39;rnn&#39;</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>MMLatch implementation</p>
<p>Multimodal baseline + feedback</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>text_size</code></td>
        <td><code>int</code></td>
        <td><p>Text input size. Defaults to 300.</p></td>
        <td><code>300</code></td>
      </tr>
      <tr>
        <td><code>audio_size</code></td>
        <td><code>int</code></td>
        <td><p>Audio input size. Defaults to 74.</p></td>
        <td><code>74</code></td>
      </tr>
      <tr>
        <td><code>visual_size</code></td>
        <td><code>int</code></td>
        <td><p>Visual input size. Defaults to 35.</p></td>
        <td><code>35</code></td>
      </tr>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>Hidden dimension. Defaults to 100.</p></td>
        <td><code>100</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout rate. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>encoder_layers</code></td>
        <td><code>float</code></td>
        <td><p>Number of encoder layers. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use bidirectional RNNs. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>merge_bi</code></td>
        <td><code>str</code></td>
        <td><p>Bidirectional merging method in the encoders. Defaults to "sum".</p></td>
        <td><code>&#39;sum&#39;</code></td>
      </tr>
      <tr>
        <td><code>rnn_type</code></td>
        <td><code>str</code></td>
        <td><p>RNN type [lstm|gru]. Defaults to "lstm".</p></td>
        <td><code>&#39;lstm&#39;</code></td>
      </tr>
      <tr>
        <td><code>encoder_attention</code></td>
        <td><code>bool</code></td>
        <td><p>Use attention in the encoder RNNs. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>fuser_residual</code></td>
        <td><code>bool</code></td>
        <td><p>Use vilbert like residual in the attention fuser. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>use_all_trimodal</code></td>
        <td><code>bool</code></td>
        <td><p>Use all trimodal interactions for the Attention fuser. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>feedback</code></td>
        <td><code>bool</code></td>
        <td><p>Use top-down feedback. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>use_self_feedback</code></td>
        <td><code>bool</code></td>
        <td><p>If false use only crossmodal features for top-down feedback. If True also use the self modality. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>feedback_algorithm</code></td>
        <td><code>str</code></td>
        <td><p>Feedback module [rnn|boom|gated|downup]. Defaults to "rnn".</p></td>
        <td><code>&#39;rnn&#39;</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/mmlatch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">text_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">300</span><span class="p">,</span>
    <span class="n">audio_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">74</span><span class="p">,</span>
    <span class="n">visual_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">35</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">100</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">encoder_layers</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">merge_bi</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;sum&quot;</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">encoder_attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">fuser_residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">use_all_trimodal</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">feedback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">use_self_feedback</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">feedback_algorithm</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;rnn&quot;</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;MMLatch implementation</span>

<span class="sd">    Multimodal baseline + feedback</span>

<span class="sd">    Args:</span>
<span class="sd">        text_size (int, optional): Text input size. Defaults to 300.</span>
<span class="sd">        audio_size (int, optional): Audio input size. Defaults to 74.</span>
<span class="sd">        visual_size (int, optional): Visual input size. Defaults to 35.</span>
<span class="sd">        hidden_size (int, optional): Hidden dimension. Defaults to 100.</span>
<span class="sd">        dropout (float, optional): Dropout rate. Defaults to 0.2.</span>
<span class="sd">        encoder_layers (float, optional): Number of encoder layers. Defaults to 1.</span>
<span class="sd">        bidirectional (bool, optional): Use bidirectional RNNs. Defaults to True.</span>
<span class="sd">        merge_bi (str, optional): Bidirectional merging method in the encoders. Defaults to &quot;sum&quot;.</span>
<span class="sd">        rnn_type (str, optional): RNN type [lstm|gru]. Defaults to &quot;lstm&quot;.</span>
<span class="sd">        encoder_attention (bool, optional): Use attention in the encoder RNNs. Defaults to True.</span>
<span class="sd">        fuser_residual (bool, optional): Use vilbert like residual in the attention fuser. Defaults to True.</span>
<span class="sd">        use_all_trimodal (bool, optional): Use all trimodal interactions for the Attention fuser. Defaults to False.</span>
<span class="sd">        feedback (bool, optional): Use top-down feedback. Defaults to True.</span>
<span class="sd">        use_self_feedback (bool, optional): If false use only crossmodal features for top-down feedback. If True also use the self modality. Defaults to False.</span>
<span class="sd">        feedback_algorithm (str, optional): Feedback module [rnn|boom|gated|downup]. Defaults to &quot;rnn&quot;.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MMLatch</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span>
        <span class="n">text_size</span><span class="o">=</span><span class="n">text_size</span><span class="p">,</span>
        <span class="n">audio_size</span><span class="o">=</span><span class="n">audio_size</span><span class="p">,</span>
        <span class="n">visual_size</span><span class="o">=</span><span class="n">visual_size</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="o">=</span><span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">encoder_layers</span><span class="o">=</span><span class="n">encoder_layers</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">merge_bi</span><span class="o">=</span><span class="n">merge_bi</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">encoder_attention</span><span class="o">=</span><span class="n">encoder_attention</span><span class="p">,</span>
        <span class="n">fuser_residual</span><span class="o">=</span><span class="n">fuser_residual</span><span class="p">,</span>
        <span class="n">use_all_trimodal</span><span class="o">=</span><span class="n">use_all_trimodal</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">feedback</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span> <span class="o">=</span> <span class="n">Feedback</span><span class="p">(</span>
            <span class="n">hidden_size</span><span class="p">,</span>
            <span class="p">[</span><span class="n">text_size</span><span class="p">,</span> <span class="n">audio_size</span><span class="p">,</span> <span class="n">visual_size</span><span class="p">],</span>
            <span class="n">use_self</span><span class="o">=</span><span class="n">use_self_feedback</span><span class="p">,</span>
            <span class="n">mask_type</span><span class="o">=</span><span class="n">feedback_algorithm</span><span class="p">,</span>
        <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmlatch.MMLatch.encoder_cfg" class="doc doc-heading">
<code class="highlight language-python"><span class="n">encoder_cfg</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Static method to create the encoder configuration</p>
<p>The default configuration is provided here
This configuration corresponds to the official paper implementation
and is tuned for CMU MOSEI.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>input_size</code></td>
        <td><code>int</code></td>
        <td><p>Input modality size</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>**cfg</code></td>
        <td><code></code></td>
        <td><p>Optional keyword arguments</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, Any]</code></td>
      <td><p>Dict[str, Any]: The encoder configuration</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/mmlatch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">encoder_cfg</span><span class="p">(</span><span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="o">**</span><span class="n">cfg</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Static method to create the encoder configuration</span>

<span class="sd">    The default configuration is provided here</span>
<span class="sd">    This configuration corresponds to the official paper implementation</span>
<span class="sd">    and is tuned for CMU MOSEI.</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Input modality size</span>
<span class="sd">        **cfg: Optional keyword arguments</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, Any]: The encoder configuration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;input_size&quot;</span><span class="p">:</span> <span class="n">input_size</span><span class="p">,</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
        <span class="s2">&quot;layers&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;layers&quot;</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
        <span class="s2">&quot;bidirectional&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;bidirectional&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
        <span class="s2">&quot;rnn_type&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;rnn_type&quot;</span><span class="p">,</span> <span class="s2">&quot;lstm&quot;</span><span class="p">),</span>
        <span class="s2">&quot;attention&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;attention&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">}</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmlatch.MMLatch.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="o">*</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Encode + fuse</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>*mods</code></td>
        <td><code>Tensor</code></td>
        <td><p>Variable input modality tensors [B, L, D]</p></td>
        <td><code>()</code></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>The unpadded tensor lengths. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: The fused tensor [B, D]</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/mmlatch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">mods</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">encoded</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">mods_feedback</span><span class="p">:</span> <span class="n">List</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feedback</span><span class="p">(</span>
            <span class="n">mods</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span>
        <span class="p">)</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_encode</span><span class="p">(</span><span class="o">*</span><span class="n">mods_feedback</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>

    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_fuse</span><span class="p">(</span><span class="o">*</span><span class="n">encoded</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">fused</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmlatch.MMLatch.fuser_cfg" class="doc doc-heading">
<code class="highlight language-python"><span class="n">fuser_cfg</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-staticmethod"><code>staticmethod</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Static method to create the fuser configuration</p>
<p>The default configuration is provided here
This configuration corresponds to the official paper implementation
and is tuned for CMU MOSEI.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>**cfg</code></td>
        <td><code></code></td>
        <td><p>Optional keyword arguments</p></td>
        <td><code>{}</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, Any]</code></td>
      <td><p>Dict[str, Any]: The fuser configuration</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/mmlatch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="nd">@staticmethod</span>
<span class="k">def</span> <span class="nf">fuser_cfg</span><span class="p">(</span><span class="o">**</span><span class="n">cfg</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;Static method to create the fuser configuration</span>

<span class="sd">    The default configuration is provided here</span>
<span class="sd">    This configuration corresponds to the official paper implementation</span>
<span class="sd">    and is tuned for CMU MOSEI.</span>

<span class="sd">    Args:</span>
<span class="sd">        **cfg: Optional keyword arguments</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, Any]: The fuser configuration</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="p">{</span>
        <span class="s2">&quot;n_modalities&quot;</span><span class="p">:</span> <span class="mi">3</span><span class="p">,</span>
        <span class="s2">&quot;dropout&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;dropout&quot;</span><span class="p">,</span> <span class="mf">0.2</span><span class="p">),</span>
        <span class="s2">&quot;output_size&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
        <span class="s2">&quot;hidden_size&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;hidden_size&quot;</span><span class="p">,</span> <span class="mi">100</span><span class="p">),</span>
        <span class="s2">&quot;fusion_method&quot;</span><span class="p">:</span> <span class="s2">&quot;attention&quot;</span><span class="p">,</span>
        <span class="s2">&quot;timesteps_pooling_method&quot;</span><span class="p">:</span> <span class="s2">&quot;rnn&quot;</span><span class="p">,</span>
        <span class="s2">&quot;residual&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;residual&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
        <span class="s2">&quot;use_all_trimodal&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="o">.</span><span class="n">get</span><span class="p">(</span><span class="s2">&quot;use_all_trimodal&quot;</span><span class="p">,</span> <span class="kc">True</span><span class="p">),</span>
    <span class="p">}</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.mmlatch.MMLatchClassifier" class="doc doc-heading">
        <code>MMLatchClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.mmlatch.MMLatchClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/mmlatch.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">mod_dict</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">],</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="n">mods</span> <span class="o">=</span> <span class="p">[</span><span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;audio&quot;</span><span class="p">],</span> <span class="n">mod_dict</span><span class="p">[</span><span class="s2">&quot;visual&quot;</span><span class="p">]]</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">enc</span><span class="p">(</span><span class="o">*</span><span class="n">mods</span><span class="p">,</span> <span class="n">lengths</span><span class="o">=</span><span class="n">lengths</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">])</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="../modules/" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Generic Modules
            </div>
          </div>
        </a>
      
      
        <a href="../utils/" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              SLP utility functions
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../assets/javascripts/workers/search.fb4a9340.min.js", "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.5cf3e710.min.js"></script>
      
        <script src="../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>