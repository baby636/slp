
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.0.5">
    
    
      
        <title>Generic Modules - slp Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.77f3fd56.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.7fa14f5b.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#generic-modules" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="slp Documentation" class="md-header__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            slp Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Generic Modules
            
          </span>
        </div>
      </div>
    </div>
    <div class="md-header__options">
      
    </div>
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="slp Documentation" class="md-nav__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    slp Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        slp
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../get-started/" class="md-nav__link">
        Getting started
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../tuning/" class="md-nav__link">
        Hyperparameter tuning
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../config/" class="md-nav__link">
        Configuration
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../data-utils/" class="md-nav__link">
        Data manipulation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../plbind/" class="md-nav__link">
        Pytorch Lightning Bindings
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Generic Modules
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Generic Modules
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention" class="md-nav__link">
    slp.modules.attention
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention" class="md-nav__link">
    Attention
  </a>
  
    <nav class="md-nav" aria-label="Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention" class="md-nav__link">
    MultiheadAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention" class="md-nav__link">
    MultiheadSelfAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadSelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention" class="md-nav__link">
    MultiheadTwowayAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadTwowayAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention" class="md-nav__link">
    SelfAttention
  </a>
  
    <nav class="md-nav" aria-label="SelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.attention" class="md-nav__link">
    attention()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.attention_scores" class="md-nav__link">
    attention_scores()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.merge_heads" class="md-nav__link">
    merge_heads()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.nystrom_attention" class="md-nav__link">
    nystrom_attention()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.pad_for_nystrom" class="md-nav__link">
    pad_for_nystrom()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.reset_parameters" class="md-nav__link">
    reset_parameters()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.split_heads" class="md-nav__link">
    split_heads()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.twowayattention" class="md-nav__link">
    slp.modules.twowayattention
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.twowayattention.TwowayAttention" class="md-nav__link">
    TwowayAttention
  </a>
  
    <nav class="md-nav" aria-label="TwowayAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.twowayattention.TwowayAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier" class="md-nav__link">
    slp.modules.classifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier" class="md-nav__link">
    Classifier
  </a>
  
    <nav class="md-nav" aria-label="Classifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.MOSEITextClassifier" class="md-nav__link">
    MOSEITextClassifier
  </a>
  
    <nav class="md-nav" aria-label="MOSEITextClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.MOSEITextClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.RNNLateFusionClassifier" class="md-nav__link">
    RNNLateFusionClassifier
  </a>
  
    <nav class="md-nav" aria-label="RNNLateFusionClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.RNNLateFusionClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.TransformerLateFusionClassifier" class="md-nav__link">
    TransformerLateFusionClassifier
  </a>
  
    <nav class="md-nav" aria-label="TransformerLateFusionClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.TransformerLateFusionClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed" class="md-nav__link">
    slp.modules.embed
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed" class="md-nav__link">
    Embed
  </a>
  
    <nav class="md-nav" aria-label="Embed">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.init_embeddings" class="md-nav__link">
    init_embeddings()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding" class="md-nav__link">
    PositionalEncoding
  </a>
  
    <nav class="md-nav" aria-label="PositionalEncoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward" class="md-nav__link">
    slp.modules.feedforward
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF" class="md-nav__link">
    PositionwiseFF
  </a>
  
    <nav class="md-nav" aria-label="PositionwiseFF">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward.TwoLayer" class="md-nav__link">
    TwoLayer
  </a>
  
    <nav class="md-nav" aria-label="TwoLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.TwoLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm" class="md-nav__link">
    slp.modules.norm
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf" class="md-nav__link">
    LayerNormTf
  </a>
  
    <nav class="md-nav" aria-label="LayerNormTf">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm.ScaleNorm" class="md-nav__link">
    ScaleNorm
  </a>
  
    <nav class="md-nav" aria-label="ScaleNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.ScaleNorm.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.regularization" class="md-nav__link">
    slp.modules.regularization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise" class="md-nav__link">
    GaussianNoise
  </a>
  
    <nav class="md-nav" aria-label="GaussianNoise">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn" class="md-nav__link">
    slp.modules.rnn
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN" class="md-nav__link">
    AttentiveRNN
  </a>
  
    <nav class="md-nav" aria-label="AttentiveRNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN" class="md-nav__link">
    RNN
  </a>
  
    <nav class="md-nav" aria-label="RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN" class="md-nav__link">
    TokenRNN
  </a>
  
    <nav class="md-nav" aria-label="TokenRNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer" class="md-nav__link">
    slp.modules.transformer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Decoder" class="md-nav__link">
    Decoder
  </a>
  
    <nav class="md-nav" aria-label="Decoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Decoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.DecoderLayer" class="md-nav__link">
    DecoderLayer
  </a>
  
    <nav class="md-nav" aria-label="DecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.DecoderLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Encoder" class="md-nav__link">
    Encoder
  </a>
  
    <nav class="md-nav" aria-label="Encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Encoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderDecoder" class="md-nav__link">
    EncoderDecoder
  </a>
  
    <nav class="md-nav" aria-label="EncoderDecoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderDecoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderLayer" class="md-nav__link">
    EncoderLayer
  </a>
  
    <nav class="md-nav" aria-label="EncoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer1" class="md-nav__link">
    Sublayer1
  </a>
  
    <nav class="md-nav" aria-label="Sublayer1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer1.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer2" class="md-nav__link">
    Sublayer2
  </a>
  
    <nav class="md-nav" aria-label="Sublayer2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer2.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer3" class="md-nav__link">
    Sublayer3
  </a>
  
    <nav class="md-nav" aria-label="Sublayer3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer3.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Transformer" class="md-nav__link">
    Transformer
  </a>
  
    <nav class="md-nav" aria-label="Transformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Transformer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerSequenceEncoder" class="md-nav__link">
    TransformerSequenceEncoder
  </a>
  
    <nav class="md-nav" aria-label="TransformerSequenceEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerSequenceEncoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerTokenSequenceEncoder" class="md-nav__link">
    TransformerTokenSequenceEncoder
  </a>
  
    <nav class="md-nav" aria-label="TransformerTokenSequenceEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerTokenSequenceEncoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.reset_parameters" class="md-nav__link">
    reset_parameters()
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../multimodal/" class="md-nav__link">
        Multimodal Modules
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        SLP utility functions
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../apiref/" class="md-nav__link">
        API reference
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention" class="md-nav__link">
    slp.modules.attention
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention" class="md-nav__link">
    Attention
  </a>
  
    <nav class="md-nav" aria-label="Attention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.Attention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention" class="md-nav__link">
    MultiheadAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention" class="md-nav__link">
    MultiheadSelfAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadSelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadSelfAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention" class="md-nav__link">
    MultiheadTwowayAttention
  </a>
  
    <nav class="md-nav" aria-label="MultiheadTwowayAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.MultiheadTwowayAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention" class="md-nav__link">
    SelfAttention
  </a>
  
    <nav class="md-nav" aria-label="SelfAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.attention.SelfAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.attention" class="md-nav__link">
    attention()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.attention_scores" class="md-nav__link">
    attention_scores()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.merge_heads" class="md-nav__link">
    merge_heads()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.nystrom_attention" class="md-nav__link">
    nystrom_attention()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.pad_for_nystrom" class="md-nav__link">
    pad_for_nystrom()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.reset_parameters" class="md-nav__link">
    reset_parameters()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.attention.split_heads" class="md-nav__link">
    split_heads()
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.twowayattention" class="md-nav__link">
    slp.modules.twowayattention
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.twowayattention.TwowayAttention" class="md-nav__link">
    TwowayAttention
  </a>
  
    <nav class="md-nav" aria-label="TwowayAttention">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.twowayattention.TwowayAttention.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier" class="md-nav__link">
    slp.modules.classifier
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier" class="md-nav__link">
    Classifier
  </a>
  
    <nav class="md-nav" aria-label="Classifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.Classifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.MOSEITextClassifier" class="md-nav__link">
    MOSEITextClassifier
  </a>
  
    <nav class="md-nav" aria-label="MOSEITextClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.MOSEITextClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.RNNLateFusionClassifier" class="md-nav__link">
    RNNLateFusionClassifier
  </a>
  
    <nav class="md-nav" aria-label="RNNLateFusionClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.RNNLateFusionClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.classifier.TransformerLateFusionClassifier" class="md-nav__link">
    TransformerLateFusionClassifier
  </a>
  
    <nav class="md-nav" aria-label="TransformerLateFusionClassifier">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.classifier.TransformerLateFusionClassifier.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed" class="md-nav__link">
    slp.modules.embed
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed" class="md-nav__link">
    Embed
  </a>
  
    <nav class="md-nav" aria-label="Embed">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.Embed.init_embeddings" class="md-nav__link">
    init_embeddings()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding" class="md-nav__link">
    PositionalEncoding
  </a>
  
    <nav class="md-nav" aria-label="PositionalEncoding">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.embed.PositionalEncoding.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward" class="md-nav__link">
    slp.modules.feedforward
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF" class="md-nav__link">
    PositionwiseFF
  </a>
  
    <nav class="md-nav" aria-label="PositionwiseFF">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.PositionwiseFF.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.feedforward.TwoLayer" class="md-nav__link">
    TwoLayer
  </a>
  
    <nav class="md-nav" aria-label="TwoLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.feedforward.TwoLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm" class="md-nav__link">
    slp.modules.norm
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf" class="md-nav__link">
    LayerNormTf
  </a>
  
    <nav class="md-nav" aria-label="LayerNormTf">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.LayerNormTf.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.norm.ScaleNorm" class="md-nav__link">
    ScaleNorm
  </a>
  
    <nav class="md-nav" aria-label="ScaleNorm">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.norm.ScaleNorm.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.regularization" class="md-nav__link">
    slp.modules.regularization
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise" class="md-nav__link">
    GaussianNoise
  </a>
  
    <nav class="md-nav" aria-label="GaussianNoise">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.__repr__" class="md-nav__link">
    __repr__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.regularization.GaussianNoise.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn" class="md-nav__link">
    slp.modules.rnn
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN" class="md-nav__link">
    AttentiveRNN
  </a>
  
    <nav class="md-nav" aria-label="AttentiveRNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.AttentiveRNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN" class="md-nav__link">
    RNN
  </a>
  
    <nav class="md-nav" aria-label="RNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.out_size" class="md-nav__link">
    out_size
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.RNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN" class="md-nav__link">
    TokenRNN
  </a>
  
    <nav class="md-nav" aria-label="TokenRNN">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN.__init__" class="md-nav__link">
    __init__()
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#slp.modules.rnn.TokenRNN.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer" class="md-nav__link">
    slp.modules.transformer
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Decoder" class="md-nav__link">
    Decoder
  </a>
  
    <nav class="md-nav" aria-label="Decoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Decoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.DecoderLayer" class="md-nav__link">
    DecoderLayer
  </a>
  
    <nav class="md-nav" aria-label="DecoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.DecoderLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Encoder" class="md-nav__link">
    Encoder
  </a>
  
    <nav class="md-nav" aria-label="Encoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Encoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderDecoder" class="md-nav__link">
    EncoderDecoder
  </a>
  
    <nav class="md-nav" aria-label="EncoderDecoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderDecoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderLayer" class="md-nav__link">
    EncoderLayer
  </a>
  
    <nav class="md-nav" aria-label="EncoderLayer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.EncoderLayer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer1" class="md-nav__link">
    Sublayer1
  </a>
  
    <nav class="md-nav" aria-label="Sublayer1">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer1.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer2" class="md-nav__link">
    Sublayer2
  </a>
  
    <nav class="md-nav" aria-label="Sublayer2">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer2.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer3" class="md-nav__link">
    Sublayer3
  </a>
  
    <nav class="md-nav" aria-label="Sublayer3">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Sublayer3.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.Transformer" class="md-nav__link">
    Transformer
  </a>
  
    <nav class="md-nav" aria-label="Transformer">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.Transformer.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerSequenceEncoder" class="md-nav__link">
    TransformerSequenceEncoder
  </a>
  
    <nav class="md-nav" aria-label="TransformerSequenceEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerSequenceEncoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerTokenSequenceEncoder" class="md-nav__link">
    TransformerTokenSequenceEncoder
  </a>
  
    <nav class="md-nav" aria-label="TransformerTokenSequenceEncoder">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#slp.modules.transformer.TransformerTokenSequenceEncoder.forward" class="md-nav__link">
    forward()
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.modules.transformer.reset_parameters" class="md-nav__link">
    reset_parameters()
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="generic-modules">Generic Modules</h1>
<p>Modules implemented in slp. These modules can be used as building blocks for more complicated models.</p>


  <div class="doc doc-object doc-module">

<a id="slp.modules.attention"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.attention.Attention" class="doc doc-heading">
        <code>Attention</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.Attention.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Single-Headed Dot-product attention module</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>attention_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>input_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Input features. Defaults to None.
If None input_size is set to attention_size.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attention_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Single-Headed Dot-product attention module</span>

<span class="sd">    Args:</span>
<span class="sd">        attention_size (int): Number of hidden features. Defaults to 512.</span>
<span class="sd">        input_size (Optional[int]): Input features. Defaults to None.</span>
<span class="sd">            If None input_size is set to attention_size.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Attention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dk</span> <span class="o">=</span> <span class="n">input_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.Attention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Single-head scaled dot-product attention forward pass</p>
<p>Outputs the values, where features for each sequence element are weighted by their respective attention scores</p>
<div class="arithmatex">\[a = softmax(\frac{Q}{K^T}){\sqrt{d}}) \dot V\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>keys</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, D] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>queries</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, M, D] Queries tensor. If None queries = keys. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, L] or [B, M, L] zero-one mask for sequence elements. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, M, L])</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">keys</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">queries</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Single-head scaled dot-product attention forward pass</span>

<span class="sd">    Outputs the values, where features for each sequence element are weighted by their respective attention scores</span>

<span class="sd">    $$a = softmax(\frac{Q}{K^T}){\sqrt{d}}) \dot V$$</span>

<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>

<span class="sd">    Args:</span>
<span class="sd">        keys (torch.Tensor): [B, L, D] Keys tensor</span>
<span class="sd">        queries (Optional[torch.Tensor]): Optional [B, M, D] Queries tensor. If None queries = keys. Defaults to None.</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, L] or [B, M, L] zero-one mask for sequence elements. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, M, L])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">queries</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">keys</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">keys</span>

    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>  <span class="c1"># (B, L, A)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>

    <span class="c1"># weights =&gt; (B, L, L)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">v</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.attention.MultiheadAttention" class="doc doc-heading">
        <code>MultiheadAttention</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadAttention.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">nystrom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Multi-Headed Dot-product attention module</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>attention_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>Number of attention heads</p></td>
        <td><code>8</code></td>
      </tr>
      <tr>
        <td><code>input_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Input features. Defaults to None.
If None input_size is set to attention_size.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>nystrom</code></td>
        <td><code>bool</code></td>
        <td><p>Use nystrom method for attention calculation. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark points for nystrom attention. Defaults to 64.</p></td>
        <td><code>64</code></td>
      </tr>
      <tr>
        <td><code>inverse_iterations</code></td>
        <td><code>int</code></td>
        <td><p>Number of iteration to calculate the inverse in nystrom attention. Defaults to 6.</p></td>
        <td><code>6</code></td>
      </tr>
      <tr>
        <td><code>kernel_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Use residual convolution in the output. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attention_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">nystrom</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multi-Headed Dot-product attention module</span>

<span class="sd">    Args:</span>
<span class="sd">        attention_size (int): Number of hidden features. Defaults to 512.</span>
<span class="sd">        num_heads (int): Number of attention heads</span>
<span class="sd">        input_size (Optional[int]): Input features. Defaults to None.</span>
<span class="sd">            If None input_size is set to attention_size.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.1.</span>
<span class="sd">        nystrom (bool, optional): Use nystrom method for attention calculation. Defaults to False.</span>
<span class="sd">        num_landmarks (int, optional): Number of landmark points for nystrom attention. Defaults to 64.</span>
<span class="sd">        inverse_iterations (int, optional): Number of iteration to calculate the inverse in nystrom attention. Defaults to 6.</span>
<span class="sd">        kernel_size (Optional[int], optional): Use residual convolution in the output. Defaults to None.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiheadAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inverse_iterations</span> <span class="o">=</span> <span class="n">inverse_iterations</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span> <span class="o">=</span> <span class="n">num_landmarks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span> <span class="o">=</span> <span class="n">nystrom</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">attention_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">k</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">q</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">v</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">attention_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">kernel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadAttention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Multi-head scaled dot-product attention forward pass</p>
<p>Outputs the values, where features for each sequence element are weighted by their respective attention scores</p>
<p>Each head performs dot-product attention</p>
<div class="arithmatex">\[a_H = softmax(\frac{Q_H \cdot K_H^T}{\sqrt{d}}) \cdot V_H\]</div>
<p>The outputs of multiple heads are concatenated and passed through a feedforward layer.</p>
<div class="arithmatex">\[a = W (a^{(1)}_{H} \mathbin\Vert a^{(2)}_{H} \dots) + b\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>keys</code></td>
        <td><code>torch.Tensor</code></td>
        <td><p>[B, L, D] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>queries</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, M, D] Queries tensor. If None queries = keys. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, M, L] zero-one mask for sequence elements. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>(Reweighted values [B, L, D], attention scores [B, H, M, L])</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">keys</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Multi-head scaled dot-product attention forward pass</span>

<span class="sd">    Outputs the values, where features for each sequence element are weighted by their respective attention scores</span>

<span class="sd">    Each head performs dot-product attention</span>

<span class="sd">    $$a_H = softmax(\frac{Q_H \cdot K_H^T}{\sqrt{d}}) \cdot V_H$$</span>

<span class="sd">    The outputs of multiple heads are concatenated and passed through a feedforward layer.</span>

<span class="sd">    $$a = W (a^{(1)}_{H} \mathbin\Vert a^{(2)}_{H} \dots) + b$$</span>


<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>


<span class="sd">    Args:</span>
<span class="sd">        keys (torch.Tensor): [B, L, D] Keys tensor</span>
<span class="sd">        queries (Optional[torch.Tensor]): Optional [B, M, D] Queries tensor. If None queries = keys. Defaults to None.</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, M, L] zero-one mask for sequence elements. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, H, M, L])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">keys</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span><span class="p">:</span>
        <span class="n">keys</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">pad_for_nystrom</span><span class="p">(</span>
            <span class="n">keys</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="n">queries</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">queries</span> <span class="o">=</span> <span class="n">keys</span>

    <span class="n">values</span> <span class="o">=</span> <span class="n">keys</span>

    <span class="n">k</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">k</span><span class="p">(</span><span class="n">keys</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">q</span><span class="p">(</span><span class="n">queries</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">v</span><span class="p">(</span><span class="n">values</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span><span class="p">:</span>
        <span class="c1"># out = (B, H, L, A/H)</span>
        <span class="c1"># scores = Tuple</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">nystrom_attention</span><span class="p">(</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">inverse_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inverse_iterations</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># out =&gt; (B, H, L, A/H)</span>
        <span class="c1"># scores =&gt; (B, H, L, L)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">v</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>

    <span class="c1"># out =&gt; (B, H, L, A/H)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">merge_heads</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="n">seq_length</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="p">:</span><span class="n">seq_length</span><span class="p">,</span> <span class="p">:]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.attention.MultiheadSelfAttention" class="doc doc-heading">
        <code>MultiheadSelfAttention</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadSelfAttention.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">nystrom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Multi-Headed Dot-product attention module</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>attention_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>Number of attention heads</p></td>
        <td><code>8</code></td>
      </tr>
      <tr>
        <td><code>input_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Input features. Defaults to None.
If None input_size is set to attention_size.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attention_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">nystrom</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Multi-Headed Dot-product attention module</span>

<span class="sd">    Args:</span>
<span class="sd">        attention_size (int): Number of hidden features. Defaults to 512.</span>
<span class="sd">        num_heads (int): Number of attention heads</span>
<span class="sd">        input_size (Optional[int]): Input features. Defaults to None.</span>
<span class="sd">            If None input_size is set to attention_size.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiheadSelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">inverse_iterations</span> <span class="o">=</span> <span class="n">inverse_iterations</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span> <span class="o">=</span> <span class="n">num_landmarks</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span> <span class="o">=</span> <span class="n">nystrom</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span> <span class="o">=</span> <span class="n">num_heads</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">attention_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dk</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">head_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">attention_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kqv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">output</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">attention_size</span><span class="p">,</span> <span class="n">attention_size</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">kernel_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span>
            <span class="n">in_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">out_channels</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
            <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span><span class="p">,</span> <span class="mi">1</span><span class="p">),</span>
            <span class="n">padding</span><span class="o">=</span><span class="p">(</span><span class="n">kernel_size</span> <span class="o">//</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span>
            <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
            <span class="n">groups</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadSelfAttention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Multi-head scaled dot-product attention forward pass</p>
<p>Outputs the values, where features for each sequence element are weighted by their respective attention scores</p>
<p>Each head performs dot-product attention</p>
<div class="arithmatex">\[a_H = softmax(\frac{Q_H \cdot K_H^T}{\sqrt{d}}) \cdot V_H\]</div>
<p>The outputs of multiple heads are concatenated and passed through a feedforward layer.</p>
<div class="arithmatex">\[a = W (a^{(1)}_{H} \mathbin\Vert a^{(2)}_{H} \dots) + b\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>torch.Tensor</code></td>
        <td><p>[B, L, D] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, M, L] zero-one mask for sequence elements. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>(Reweighted values [B, L, D], attention scores [B, H, M, L])</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Multi-head scaled dot-product attention forward pass</span>

<span class="sd">    Outputs the values, where features for each sequence element are weighted by their respective attention scores</span>

<span class="sd">    Each head performs dot-product attention</span>

<span class="sd">    $$a_H = softmax(\frac{Q_H \cdot K_H^T}{\sqrt{d}}) \cdot V_H$$</span>

<span class="sd">    The outputs of multiple heads are concatenated and passed through a feedforward layer.</span>

<span class="sd">    $$a = W (a^{(1)}_{H} \mathbin\Vert a^{(2)}_{H} \dots) + b$$</span>


<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>


<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, D] Keys tensor</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, M, L] zero-one mask for sequence elements. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, H, M, L])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span><span class="p">:</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">pad_for_nystrom</span><span class="p">(</span>
            <span class="n">x</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span>
        <span class="p">)</span>

    <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kqv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">k</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
    <span class="n">q</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>
    <span class="n">v</span> <span class="o">=</span> <span class="n">split_heads</span><span class="p">(</span><span class="n">v</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">num_heads</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">nystrom</span><span class="p">:</span>
        <span class="c1"># out = (B, H, L, A/H)</span>
        <span class="c1"># scores = Tuple</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">nystrom_attention</span><span class="p">(</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">num_landmarks</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">inverse_iterations</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">inverse_iterations</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># out =&gt; (B, H, L, A/H)</span>
        <span class="c1"># scores =&gt; (B, H, L, L)</span>
        <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span>
            <span class="n">k</span><span class="p">,</span>
            <span class="n">q</span><span class="p">,</span>
            <span class="n">v</span><span class="p">,</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
            <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
            <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="kc">None</span> <span class="ow">or</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">ndim</span> <span class="o">&gt;</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">v</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">out</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv</span><span class="p">(</span><span class="n">v</span> <span class="o">*</span> <span class="n">attention_mask</span><span class="p">[:,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:,</span> <span class="kc">None</span><span class="p">])</span>

    <span class="c1"># out =&gt; (B, H, L, A/H)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">merge_heads</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="k">if</span> <span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="o">!=</span> <span class="n">seq_length</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">out</span><span class="p">[:,</span> <span class="o">-</span><span class="n">seq_length</span><span class="p">:,</span> <span class="p">:]</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">output</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.attention.MultiheadTwowayAttention" class="doc doc-heading">
        <code>MultiheadTwowayAttention</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadTwowayAttention.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">8</span><span class="p">,</span> <span class="n">residual</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">nystrom</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Multihead twoway attention for multimodal fusion</p>
<p>This module performs two way attention for two input modality feature sequences.
If att is the MultiheadAttention operation and x, y the input modality sequences,
the operation is summarized as</p>
<div class="arithmatex">\[out = (att(x \rightarrow y), att(y \rightarrow x))\]</div>
<p>If residual is True then a Vilbert-like residual connection is applied</p>
<div class="arithmatex">\[out = (att(x \rightarrow y) + x, att(y \rightarrow x) + y)\]</div>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>attention_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>Number of attention heads</p></td>
        <td><code>8</code></td>
      </tr>
      <tr>
        <td><code>input_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Input features. Defaults to None.
If None input_size is set to attention_size.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>nystrom</code></td>
        <td><code>bool</code></td>
        <td><p>Use nystrom method for attention calculation. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark points for nystrom attention. Defaults to 64.</p></td>
        <td><code>64</code></td>
      </tr>
      <tr>
        <td><code>inverse_iterations</code></td>
        <td><code>int</code></td>
        <td><p>Number of iteration to calculate the inverse in nystrom attention. Defaults to 6.</p></td>
        <td><code>6</code></td>
      </tr>
      <tr>
        <td><code>kernel_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Use residual convolution in the output. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>residual</code></td>
        <td><code>bool</code></td>
        <td><p>Use vilbert-like residual connections for fusion. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attention_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">8</span><span class="p">,</span>
    <span class="n">residual</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">nystrom</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">64</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Multihead twoway attention for multimodal fusion</span>

<span class="sd">    This module performs two way attention for two input modality feature sequences.</span>
<span class="sd">    If att is the MultiheadAttention operation and x, y the input modality sequences,</span>
<span class="sd">    the operation is summarized as</span>

<span class="sd">    $$out = (att(x \rightarrow y), att(y \rightarrow x))$$</span>

<span class="sd">    If residual is True then a Vilbert-like residual connection is applied</span>

<span class="sd">    $$out = (att(x \rightarrow y) + x, att(y \rightarrow x) + y)$$</span>


<span class="sd">    Args:</span>
<span class="sd">        attention_size (int): Number of hidden features. Defaults to 512.</span>
<span class="sd">        num_heads (int): Number of attention heads</span>
<span class="sd">        input_size (Optional[int]): Input features. Defaults to None.</span>
<span class="sd">            If None input_size is set to attention_size.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.1.</span>
<span class="sd">        nystrom (bool, optional): Use nystrom method for attention calculation. Defaults to False.</span>
<span class="sd">        num_landmarks (int, optional): Number of landmark points for nystrom attention. Defaults to 64.</span>
<span class="sd">        inverse_iterations (int, optional): Number of iteration to calculate the inverse in nystrom attention. Defaults to 6.</span>
<span class="sd">        kernel_size (Optional[int], optional): Use residual convolution in the output. Defaults to None.</span>
<span class="sd">        residual (bool, optional): Use vilbert-like residual connections for fusion. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">MultiheadTwowayAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">xy</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span>
        <span class="n">attention_size</span><span class="o">=</span><span class="n">attention_size</span><span class="p">,</span>
        <span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">nystrom</span><span class="o">=</span><span class="n">nystrom</span><span class="p">,</span>
        <span class="n">num_landmarks</span><span class="o">=</span><span class="n">num_landmarks</span><span class="p">,</span>
        <span class="n">inverse_iterations</span><span class="o">=</span><span class="n">inverse_iterations</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">yx</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span>
        <span class="n">attention_size</span><span class="o">=</span><span class="n">attention_size</span><span class="p">,</span>
        <span class="n">input_size</span><span class="o">=</span><span class="n">input_size</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">nystrom</span><span class="o">=</span><span class="n">nystrom</span><span class="p">,</span>
        <span class="n">num_landmarks</span><span class="o">=</span><span class="n">num_landmarks</span><span class="p">,</span>
        <span class="n">inverse_iterations</span><span class="o">=</span><span class="n">inverse_iterations</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">residual</span> <span class="o">=</span> <span class="n">residual</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.MultiheadTwowayAttention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod1</span><span class="p">,</span> <span class="n">mod2</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>x : (B, L, D)
queries : (B, L, D)
values : (B, L, D)</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod1</span><span class="p">,</span> <span class="n">mod2</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    x : (B, L, D)</span>
<span class="sd">    queries : (B, L, D)</span>
<span class="sd">    values : (B, L, D)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out_mod1</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">xy</span><span class="p">(</span><span class="n">mod1</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">mod2</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
    <span class="n">out_mod2</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">yx</span><span class="p">(</span><span class="n">mod2</span><span class="p">,</span> <span class="n">queries</span><span class="o">=</span><span class="n">mod1</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out_mod1</span><span class="p">,</span> <span class="n">out_mod2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># vilbert cross residual</span>

        <span class="c1"># v + attention(v-&gt;a)</span>
        <span class="c1"># a + attention(a-&gt;v)</span>
        <span class="n">out_mod1</span> <span class="o">+=</span> <span class="n">mod2</span>
        <span class="n">out_mod2</span> <span class="o">+=</span> <span class="n">mod1</span>

        <span class="k">return</span> <span class="n">out_mod1</span><span class="p">,</span> <span class="n">out_mod2</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.attention.SelfAttention" class="doc doc-heading">
        <code>SelfAttention</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.SelfAttention.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">attention_size</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">input_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Single-Headed Dot-product self attention module</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>attention_size</code></td>
        <td><code>int</code></td>
        <td><p>Number of hidden features. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>input_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Input features. Defaults to None.
If None input_size is set to attention_size.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">attention_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Single-Headed Dot-product self attention module</span>

<span class="sd">    Args:</span>
<span class="sd">        attention_size (int): Number of hidden features. Defaults to 512.</span>
<span class="sd">        input_size (Optional[int]): Input features. Defaults to None.</span>
<span class="sd">            If None input_size is set to attention_size.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">SelfAttention</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">input_size</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">input_size</span> <span class="o">=</span> <span class="n">attention_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dk</span> <span class="o">=</span> <span class="n">input_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">kqv</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">input_size</span><span class="p">,</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">attention_size</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">dropout</span>
    <span class="n">reset_parameters</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">())</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.attention.SelfAttention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Single-head scaled dot-product attention forward pass</p>
<p>Outputs the values, where features for each sequence element are weighted by their respective attention scores</p>
<div class="arithmatex">\[a = softmax(\frac{Q}{K^T}){\sqrt{d}}) \dot V\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, D] Input tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, L] or [B, M, L] zero-one mask for sequence elements. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, M, L])</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Single-head scaled dot-product attention forward pass</span>

<span class="sd">    Outputs the values, where features for each sequence element are weighted by their respective attention scores</span>

<span class="sd">    $$a = softmax(\frac{Q}{K^T}){\sqrt{d}}) \dot V$$</span>

<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, D] Input tensor</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, L] or [B, M, L] zero-one mask for sequence elements. Defaults to None.</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor]: (Reweighted values [B, L, D], attention scores [B, M, L])</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="nb">list</span><span class="p">(</span><span class="n">attention_mask</span><span class="o">.</span><span class="n">size</span><span class="p">()))</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kqv</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">.</span><span class="n">chunk</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>  <span class="c1"># (B, L, A)</span>

    <span class="c1"># weights =&gt; (B, L, L)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">scores</span> <span class="o">=</span> <span class="n">attention</span><span class="p">(</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="n">v</span><span class="p">,</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">,</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.attention" class="doc doc-heading">
<code class="highlight language-python"><span class="n">attention</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Reweight values using scaled dot product attention</p>
<div class="arithmatex">\[s = softmax(\frac{Q \cdot K^T}{\sqrt{d}}) V\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>k</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>q</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>v</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, M, A] or multi-head [B, H, M, A/H] Values tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dk</code></td>
        <td><code>int</code></td>
        <td><p>Model dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask
tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be
preserved. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>training</code></td>
        <td><code>bool</code></td>
        <td><p>Is module in training phase? Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>[B, M, L] or [B, H, M, L] attention scores</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">attention</span><span class="p">(</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">dk</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Reweight values using scaled dot product attention</span>

<span class="sd">    $$s = softmax(\frac{Q \cdot K^T}{\sqrt{d}}) V$$</span>

<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>

<span class="sd">    Args:</span>
<span class="sd">        k (torch.Tensor): Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</span>
<span class="sd">        q (torch.Tensor): Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</span>
<span class="sd">        v (torch.Tensor): Single head [B, M, A] or multi-head [B, H, M, A/H] Values tensor</span>
<span class="sd">        dk (int): Model dimension</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask</span>
<span class="sd">            tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be</span>
<span class="sd">            preserved. Defaults to None.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.2.</span>
<span class="sd">        training (bool): Is module in training phase? Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, M, L] or [B, H, M, L] attention scores</span>
<span class="sd">    &quot;&quot;&quot;</span>

    <span class="n">scores</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">(</span>
        <span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span>
    <span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">v</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.attention_scores" class="doc doc-heading">
<code class="highlight language-python"><span class="n">attention_scores</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Calculate attention scores for scaled dot product attention</p>
<div class="arithmatex">\[s = softmax(\frac{Q \cdot K^T}{\sqrt{d}})\]</div>
<ul>
<li>B: Batch size</li>
<li>L: Keys Sequence length</li>
<li>M: Queries Sequence length</li>
<li>H: Number of heads</li>
<li>A: Feature dimension</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>k</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>q</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dk</code></td>
        <td><code>int</code></td>
        <td><p>Model dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask
tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be
preserved. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>training</code></td>
        <td><code>bool</code></td>
        <td><p>Is module in training phase? Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: [B, M, L] or [B, H, M, L] attention scores</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">attention_scores</span><span class="p">(</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">dk</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Calculate attention scores for scaled dot product attention</span>

<span class="sd">    $$s = softmax(\frac{Q \cdot K^T}{\sqrt{d}})$$</span>

<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>

<span class="sd">    Args:</span>
<span class="sd">        k (torch.Tensor): Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</span>
<span class="sd">        q (torch.Tensor): Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</span>
<span class="sd">        dk (int): Model dimension</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask</span>
<span class="sd">            tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be</span>
<span class="sd">            preserved. Defaults to None.</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.2.</span>
<span class="sd">        training (bool): Is module in training phase? Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, M, L] or [B, H, M, L] attention scores</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">matmul</span><span class="p">(</span><span class="n">q</span><span class="p">,</span> <span class="n">k</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scores</span> <span class="o">=</span> <span class="n">scores</span> <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="p">)</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e5</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">scores</span><span class="p">,</span> <span class="n">p</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scores</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.merge_heads" class="doc doc-heading">
<code class="highlight language-python"><span class="n">merge_heads</span><span class="p">(</span><span class="n">x</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Merge multiple attention heads into output tensor</p>
<p>(Batch size, Heads, Lengths, Attention size / Heads) =&gt; (Batch size, Length, Attention size)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, H, L, A/H] multi-head tensor</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor:  [B, L, A] merged / reshaped tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">merge_heads</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Merge multiple attention heads into output tensor</span>

<span class="sd">    (Batch size, Heads, Lengths, Attention size / Heads) =&gt; (Batch size, Length, Attention size)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, H, L, A/H] multi-head tensor</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor:  [B, L, A] merged / reshaped tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">_</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="c1"># x =&gt; (B, L, H, A/H)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span><span class="o">.</span><span class="n">contiguous</span><span class="p">()</span>

    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.nystrom_attention" class="doc doc-heading">
<code class="highlight language-python"><span class="n">nystrom_attention</span><span class="p">(</span><span class="n">k</span><span class="p">,</span> <span class="n">q</span><span class="p">,</span> <span class="n">v</span><span class="p">,</span> <span class="n">dk</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">training</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Calculate attention using nystrom approximation</p>
<p>Implementation heavily based on: https://github.com/lucidrains/nystrom-attention</p>
<p>Reference: https://arxiv.org/abs/2102.03902
* B: Batch size
* L: Keys Sequence length
* M: Queries Sequence length
* H: Number of heads
* A: Feature dimension</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>k</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>q</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>v</code></td>
        <td><code>Tensor</code></td>
        <td><p>Single head [B, M, A] or multi-head [B, H, M, A/H] Values tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dk</code></td>
        <td><code>int</code></td>
        <td><p>Model dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark points</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask
tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be
preserved. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>inverse_iterations</code></td>
        <td><code>int</code></td>
        <td><p>Number of iterations for Moore Penrose iterative inverse
approximation</p></td>
        <td><code>6</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability. Defaults to 0.2.</p></td>
        <td><code>0.2</code></td>
      </tr>
      <tr>
        <td><code>training</code></td>
        <td><code>bool</code></td>
        <td><p>Is module in training phase? Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>[B, M, L] or [B, H, M, L] attention scores</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">nystrom_attention</span><span class="p">(</span>
    <span class="n">k</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">q</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">v</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">dk</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
    <span class="n">training</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Calculate attention using nystrom approximation</span>

<span class="sd">    Implementation heavily based on: https://github.com/lucidrains/nystrom-attention</span>

<span class="sd">    Reference: https://arxiv.org/abs/2102.03902</span>
<span class="sd">    * B: Batch size</span>
<span class="sd">    * L: Keys Sequence length</span>
<span class="sd">    * M: Queries Sequence length</span>
<span class="sd">    * H: Number of heads</span>
<span class="sd">    * A: Feature dimension</span>

<span class="sd">    Args:</span>
<span class="sd">        k (torch.Tensor): Single head [B, L, A] or multi-head [B, H, L, A/H] Keys tensor</span>
<span class="sd">        q (torch.Tensor): Single head [B, M, A] or multi-head [B, H, M, A/H] Keys tensor</span>
<span class="sd">        v (torch.Tensor): Single head [B, M, A] or multi-head [B, H, M, A/H] Values tensor</span>
<span class="sd">        dk (int): Model dimension</span>
<span class="sd">        num_landmarks (int): Number of landmark points</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): Optional [B, [H], 1, L] pad mask or [B, [H], M, L] pad mask + subsequent mask</span>
<span class="sd">            tensor with zeros in sequence indices that should be masked and ones in sequence indices that should be</span>
<span class="sd">            preserved. Defaults to None.</span>
<span class="sd">        inverse_iterations (int): Number of iterations for Moore Penrose iterative inverse</span>
<span class="sd">            approximation</span>
<span class="sd">        dropout (float): Drop probability. Defaults to 0.2.</span>
<span class="sd">        training (bool): Is module in training phase? Defaults to True.</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, M, L] or [B, H, M, L] attention scores</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">_</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">head_size</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="n">masked_mean_denom</span> <span class="o">=</span> <span class="n">seq_length</span> <span class="o">//</span> <span class="n">num_landmarks</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">masked_mean_denom</span> <span class="o">=</span> <span class="p">(</span>
            <span class="n">attention_mask</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">seq_length</span> <span class="o">//</span> <span class="n">num_landmarks</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span> <span class="o">+</span> <span class="mf">1e-8</span>  <span class="c1"># type: ignore</span>
        <span class="p">)</span>  <span class="c1"># (B, 1, Landmarks)</span>
        <span class="n">mask_landmarks</span> <span class="o">=</span> <span class="p">(</span><span class="n">masked_mean_denom</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">type</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
        <span class="n">masked_mean_denom</span> <span class="o">=</span> <span class="n">masked_mean_denom</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span>  <span class="c1"># type: ignore</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">*</span> <span class="n">attention_mask</span>  <span class="c1"># (B, H, L, A/H)</span>
        <span class="n">k</span> <span class="o">=</span> <span class="n">k</span> <span class="o">*</span> <span class="n">attention_mask</span>  <span class="c1"># (B, H, L, A/H)</span>
        <span class="n">v</span> <span class="o">=</span> <span class="n">v</span> <span class="o">*</span> <span class="n">attention_mask</span>  <span class="c1"># (B, H, L, A/H)</span>

        <span class="n">scores_1_mask</span> <span class="o">=</span> <span class="n">attention_mask</span> <span class="o">*</span> <span class="n">mask_landmarks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">scores_2_mask</span> <span class="o">=</span> <span class="n">mask_landmarks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">]</span> <span class="o">*</span> <span class="n">mask_landmarks</span><span class="p">[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="p">:]</span>
        <span class="n">scores_3_mask</span> <span class="o">=</span> <span class="n">scores_1_mask</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="o">-</span><span class="mi">2</span><span class="p">)</span>

    <span class="n">q</span> <span class="o">=</span> <span class="n">q</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">dk</span><span class="p">)</span>

    <span class="n">q_landmarks</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>  <span class="c1"># batch_size</span>
        <span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># num_heads</span>
        <span class="n">num_landmarks</span><span class="p">,</span>  <span class="c1"># landmarks</span>
        <span class="n">seq_length</span> <span class="o">//</span> <span class="n">num_landmarks</span><span class="p">,</span>  <span class="c1"># reduced length</span>
        <span class="n">q</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># head_size</span>
    <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span>
    <span class="p">)</span>  <span class="c1"># (B, H, Landmarks, A/H)</span>

    <span class="n">k_landmarks</span> <span class="o">=</span> <span class="n">k</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span>
        <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>  <span class="c1"># batch_size</span>
        <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># num_heads</span>
        <span class="n">num_landmarks</span><span class="p">,</span>  <span class="c1"># landmarks</span>
        <span class="n">seq_length</span> <span class="o">//</span> <span class="n">num_landmarks</span><span class="p">,</span>  <span class="c1"># reduced length</span>
        <span class="n">k</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span>  <span class="c1"># head size</span>
    <span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span>
        <span class="n">dim</span><span class="o">=-</span><span class="mi">2</span>
    <span class="p">)</span>  <span class="c1"># (B, H, Landmarks, A/H)</span>

    <span class="n">k_landmarks</span> <span class="o">=</span> <span class="n">k_landmarks</span> <span class="o">/</span> <span class="n">masked_mean_denom</span>
    <span class="n">q_landmarks</span> <span class="o">=</span> <span class="n">q_landmarks</span> <span class="o">/</span> <span class="n">masked_mean_denom</span>

    <span class="n">scores_1</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">(</span>
        <span class="n">k_landmarks</span><span class="p">,</span>
        <span class="n">q</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>  <span class="c1"># We have already accounted for dk</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">scores_1_mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">scores_2</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">(</span>
        <span class="n">k_landmarks</span><span class="p">,</span>
        <span class="n">q_landmarks</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>  <span class="c1"># We have already accounted for dk</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">scores_2_mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">scores_3</span> <span class="o">=</span> <span class="n">attention_scores</span><span class="p">(</span>
        <span class="n">k</span><span class="p">,</span>
        <span class="n">q_landmarks</span><span class="p">,</span>
        <span class="mi">1</span><span class="p">,</span>  <span class="c1"># We have already accounted for dk</span>
        <span class="n">attention_mask</span><span class="o">=</span><span class="n">scores_3_mask</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">training</span><span class="o">=</span><span class="n">training</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="n">z_star</span> <span class="o">=</span> <span class="n">moore_penrose_pinv</span><span class="p">(</span><span class="n">scores_2</span><span class="p">,</span> <span class="n">num_iter</span><span class="o">=</span><span class="n">inverse_iterations</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="p">(</span><span class="n">scores_1</span> <span class="o">@</span> <span class="n">z_star</span><span class="p">)</span> <span class="o">@</span> <span class="p">(</span><span class="n">scores_3</span> <span class="o">@</span> <span class="n">v</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="p">(</span><span class="n">scores_1</span><span class="p">,</span> <span class="n">scores_2</span><span class="p">,</span> <span class="n">scores_3</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.pad_for_nystrom" class="doc doc-heading">
<code class="highlight language-python"><span class="n">pad_for_nystrom</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Pad inputs and attention_mask to perform Nystrom Attention</p>
<p>Pad to nearest multiple of num_landmarks</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, A] Input tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark points</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>attention_mask</code></td>
        <td><code>Optional[torch.Tensor]</code></td>
        <td><p>[B, L] Padding mask</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, Optional[torch.Tensor]]</code></td>
      <td><p>Tuple[torch.Tensor, Optional[torch.Tensor]]: Padded inputs and attention_mask</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">pad_for_nystrom</span><span class="p">(</span>
    <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">attention_mask</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Optional</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Pad inputs and attention_mask to perform Nystrom Attention</span>

<span class="sd">    Pad to nearest multiple of num_landmarks</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, A] Input tensor</span>
<span class="sd">        num_landmarks (int): Number of landmark points</span>
<span class="sd">        attention_mask (Optional[torch.Tensor]): [B, L] Padding mask</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, Optional[torch.Tensor]]: Padded inputs and attention_mask</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">seq_length</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>

    <span class="n">_</span><span class="p">,</span> <span class="n">remainder</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">seq_length</span> <span class="o">/</span> <span class="n">num_landmarks</span><span class="p">),</span>
        <span class="n">seq_length</span> <span class="o">%</span> <span class="n">num_landmarks</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">remainder</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">padding</span> <span class="o">=</span> <span class="n">num_landmarks</span> <span class="o">-</span> <span class="n">remainder</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">padding</span><span class="p">,</span> <span class="mi">0</span><span class="p">),</span> <span class="n">value</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
            <span class="n">attention_mask</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">pad</span><span class="p">(</span><span class="n">attention_mask</span><span class="p">,</span> <span class="p">(</span><span class="n">padding</span><span class="p">,</span> <span class="mi">0</span><span class="p">))</span>

    <span class="k">return</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.reset_parameters" class="doc doc-heading">
<code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Initialize parameters in the transformer model.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize parameters in the transformer model.&quot;&quot;&quot;</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">:</span>
        <span class="k">if</span> <span class="s2">&quot;weight&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">p</span><span class="p">)</span>

        <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
            <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-function">



<h2 id="slp.modules.attention.split_heads" class="doc doc-heading">
<code class="highlight language-python"><span class="n">split_heads</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Split input tensor into multiple attention heads</p>
<p>(Batch size, Length, Attention size) =&gt; (Batch size, Heads, Lengths, Attention size / Heads)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, A] input tensor</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>number of heads</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: [B, H, L, A/H] Splitted / reshaped tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/attention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">split_heads</span><span class="p">(</span><span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Split input tensor into multiple attention heads</span>

<span class="sd">    (Batch size, Length, Attention size) =&gt; (Batch size, Heads, Lengths, Attention size / Heads)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, A] input tensor</span>
<span class="sd">        num_heads (int): number of heads</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, H, L, A/H] Splitted / reshaped tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">attention_size</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
    <span class="n">head_size</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">attention_size</span> <span class="o">/</span> <span class="n">num_heads</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">batch_size</span><span class="p">,</span> <span class="n">max_length</span><span class="p">,</span> <span class="n">num_heads</span><span class="p">,</span> <span class="n">head_size</span><span class="p">)</span><span class="o">.</span><span class="n">permute</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.twowayattention"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.twowayattention.TwowayAttention" class="doc doc-heading">
        <code>TwowayAttention</code>



</h2>

    <div class="doc doc-contents ">

      <p>Some Information about Attention</p>




  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.twowayattention.TwowayAttention.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod1</span><span class="p">,</span> <span class="n">mod2</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>x : (B, L, D)
queries : (B, L, D)
values : (B, L, D)</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/twowayattention.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">mod1</span><span class="p">,</span> <span class="n">mod2</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    x : (B, L, D)</span>
<span class="sd">    queries : (B, L, D)</span>
<span class="sd">    values : (B, L, D)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">k_mod1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">kx</span><span class="p">(</span><span class="n">mod1</span><span class="p">)</span>
    <span class="n">q_mod2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qy</span><span class="p">(</span><span class="n">mod2</span><span class="p">)</span>
    <span class="n">v_mod1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vx</span><span class="p">(</span><span class="n">mod1</span><span class="p">)</span>

    <span class="n">k_mod2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ky</span><span class="p">(</span><span class="n">mod2</span><span class="p">)</span>  <span class="c1"># (B, L, A)</span>
    <span class="n">q_mod1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">qx</span><span class="p">(</span><span class="n">mod1</span><span class="p">)</span>
    <span class="n">v_mod2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">vy</span><span class="p">(</span><span class="n">mod2</span><span class="p">)</span>

    <span class="c1"># weights =&gt; (B, L, L)</span>

    <span class="n">scores_mod1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">q_mod2</span><span class="p">,</span> <span class="n">k_mod1</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">)</span>
    <span class="n">scores_mod2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">q_mod1</span><span class="p">,</span> <span class="n">k_mod2</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="o">/</span> <span class="n">math</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">dk</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">attention_mask</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">scores_mod1</span> <span class="o">=</span> <span class="n">scores_mod1</span> <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e5</span><span class="p">)</span>
        <span class="n">scores_mod2</span> <span class="o">=</span> <span class="n">scores_mod2</span> <span class="o">+</span> <span class="p">((</span><span class="mi">1</span> <span class="o">-</span> <span class="n">attention_mask</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span> <span class="o">*</span> <span class="o">-</span><span class="mf">1e5</span><span class="p">)</span>
    <span class="n">scores_mod1</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores_mod1</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores_mod1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">scores_mod1</span><span class="p">)</span>
    <span class="n">scores_mod2</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">scores_mod2</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">scores_mod2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">scores_mod2</span><span class="p">)</span>

    <span class="c1"># out =&gt; (B, L, A)</span>
    <span class="n">out_mod1</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">scores_mod1</span><span class="p">,</span> <span class="n">v_mod1</span><span class="p">)</span>
    <span class="n">out_mod2</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">bmm</span><span class="p">(</span><span class="n">scores_mod2</span><span class="p">,</span> <span class="n">v_mod2</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">layernorm</span><span class="p">:</span>
        <span class="n">out_mod1</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lnx</span><span class="p">(</span><span class="n">out_mod1</span><span class="p">)</span>
        <span class="n">out_mod2</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">lny</span><span class="p">(</span><span class="n">out_mod2</span><span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out_mod1</span><span class="p">,</span> <span class="n">out_mod2</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># vilbert cross residual</span>

        <span class="c1"># v + attention(v-&gt;a)</span>
        <span class="c1"># a + attention(a-&gt;v)</span>
        <span class="n">out_mod1</span> <span class="o">+=</span> <span class="n">mod2</span>
        <span class="n">out_mod2</span> <span class="o">+=</span> <span class="n">mod1</span>

        <span class="k">return</span> <span class="n">out_mod1</span><span class="p">,</span> <span class="n">out_mod2</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.classifier"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.classifier.Classifier" class="doc doc-heading">
        <code>Classifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.classifier.Classifier.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">encoder</span><span class="p">,</span> <span class="n">encoded_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.2</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Classifier wrapper module</p>
<p>Stores a Neural Network encoder and adds a classification layer on top.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>encoder</code></td>
        <td><code>Module</code></td>
        <td><p>[description]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>encoded_features</code></td>
        <td><code>int</code></td>
        <td><p>[description]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>num_classes</code></td>
        <td><code>int</code></td>
        <td><p>[description]</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Drop probability</p></td>
        <td><code>0.2</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/classifier.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">encoder</span><span class="p">:</span> <span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">,</span>
    <span class="n">encoded_features</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">num_classes</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.2</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Classifier wrapper module</span>

<span class="sd">    Stores a Neural Network encoder and adds a classification layer on top.</span>

<span class="sd">    Args:</span>
<span class="sd">        encoder (nn.Module): [description]</span>
<span class="sd">        encoded_features (int): [description]</span>
<span class="sd">        num_classes (int): [description]</span>
<span class="sd">        dropout (float): Drop probability</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Classifier</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">encoder</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">clf</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">encoded_features</span><span class="p">,</span> <span class="n">num_classes</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.classifier.Classifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Encode inputs using the encoder network and perform classification</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: [B, *, num_classes] Logits tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/classifier.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Encode inputs using the encoder network and perform classification</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, *, num_classes] Logits tensor</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">encoded</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="o">*</span><span class="n">args</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>  <span class="c1"># type: ignore</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">encoded</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.classifier.MOSEITextClassifier" class="doc doc-heading">
        <code>MOSEITextClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.classifier.MOSEITextClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Encode inputs using the encoder network and perform classification</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>torch.Tensor</code></td>
      <td><p>[B, *, num_classes] Logits tensor</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/classifier.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="p">[</span><span class="s2">&quot;text&quot;</span><span class="p">]</span>

    <span class="k">return</span> <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.classifier.RNNLateFusionClassifier" class="doc doc-heading">
        <code>RNNLateFusionClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.classifier.RNNLateFusionClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/classifier.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">lengths</span><span class="p">):</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modality_encoders</span><span class="p">[</span><span class="n">m</span><span class="p">](</span><span class="n">inputs</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="n">lengths</span><span class="p">[</span><span class="n">m</span><span class="p">])</span> <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modalities</span>
    <span class="p">]</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span><span class="p">(</span><span class="o">*</span><span class="n">encoded</span><span class="p">)</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>





  <div class="doc doc-object doc-class">



<h2 id="slp.modules.classifier.TransformerLateFusionClassifier" class="doc doc-heading">
        <code>TransformerLateFusionClassifier</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.classifier.TransformerLateFusionClassifier.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attention_masks</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/classifier.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">inputs</span><span class="p">,</span> <span class="n">attention_masks</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">attention_masks</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">attention_masks</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span>
            <span class="nb">zip</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">modalities</span><span class="p">,</span> <span class="p">[</span><span class="kc">None</span> <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modalities</span><span class="p">])</span>
        <span class="p">)</span>

    <span class="n">encoded</span> <span class="o">=</span> <span class="p">[</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">modality_encoders</span><span class="p">[</span><span class="n">m</span><span class="p">](</span><span class="n">inputs</span><span class="p">[</span><span class="n">m</span><span class="p">],</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_masks</span><span class="p">[</span><span class="n">m</span><span class="p">])</span>
        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">modalities</span>
    <span class="p">]</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">mmdrop</span><span class="p">(</span><span class="o">*</span><span class="n">encoded</span><span class="p">)</span>
    <span class="n">fused</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cat</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">modality_drop</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fused</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">modality_drop</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">clf</span><span class="p">(</span><span class="n">fused</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>









  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.embed"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.embed.Embed" class="doc doc-heading">
        <code>Embed</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.embed.Embed.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="mf">1.0</span><span class="p">,</span> <span class="n">trainable</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Define the layer of the model and perform the initializations
of the layers (wherever it is necessary)</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>num_embeddings</code></td>
        <td><code>int</code></td>
        <td><p>Total number of embeddings.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>embedding_dim</code></td>
        <td><code>int</code></td>
        <td><p>Embedding dimension.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>Optional[numpy.ndarray]</code></td>
        <td><p>the 2D ndarray with the word vectors.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>noise</code></td>
        <td><code>float</code></td>
        <td><p>Optional additive noise. Defaults to 0.0.</p></td>
        <td><code>0.0</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Embedding dropout probability. Defaults to 0.0.</p></td>
        <td><code>0.0</code></td>
      </tr>
      <tr>
        <td><code>scale</code></td>
        <td><code>float</code></td>
        <td><p>Scale word embeddings by a constant. Defaults to 1.0.</p></td>
        <td><code>1.0</code></td>
      </tr>
      <tr>
        <td><code>trainable</code></td>
        <td><code>bool</code></td>
        <td><p>Finetune embeddings. Defaults to False</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/embed.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">num_embeddings</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">noise</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">scale</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1.0</span><span class="p">,</span>
    <span class="n">trainable</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Define the layer of the model and perform the initializations</span>
<span class="sd">    of the layers (wherever it is necessary)</span>

<span class="sd">    Args:</span>
<span class="sd">        num_embeddings (int): Total number of embeddings.</span>
<span class="sd">        embedding_dim (int): Embedding dimension.</span>
<span class="sd">        embeddings (numpy.ndarray): the 2D ndarray with the word vectors.</span>
<span class="sd">        noise (float): Optional additive noise. Defaults to 0.0.</span>
<span class="sd">        dropout (float): Embedding dropout probability. Defaults to 0.0.</span>
<span class="sd">        scale (float): Scale word embeddings by a constant. Defaults to 1.0.</span>
<span class="sd">        trainable (bool): Finetune embeddings. Defaults to False</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">Embed</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">scale</span> <span class="o">=</span> <span class="n">scale</span>  <span class="c1"># scale embeddings by value. Needed for transformer</span>
    <span class="c1"># define the embedding layer, with the corresponding dimensions</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Embedding</span><span class="p">(</span>
        <span class="n">num_embeddings</span><span class="o">=</span><span class="n">num_embeddings</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="n">embedding_dim</span>
    <span class="p">)</span>

    <span class="k">if</span> <span class="n">embeddings</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Initializing Embedding layer with pre-trained weights.&quot;</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">trainable</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Embeddings are going to be finetuned&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="s2">&quot;Embeddings are frozen&quot;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">init_embeddings</span><span class="p">(</span><span class="n">embeddings</span><span class="p">,</span> <span class="n">trainable</span><span class="p">)</span>

    <span class="c1"># the dropout &quot;layer&quot; for the word embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>

    <span class="c1"># the gaussian noise &quot;layer&quot; for the word embeddings</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">noise</span> <span class="o">=</span> <span class="n">GaussianNoise</span><span class="p">(</span><span class="n">noise</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.embed.Embed.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Embed input tokens</p>
<p>Assign embedding that corresponds to each token.
Optionally add Gaussian noise and embedding dropout and scale embeddings by a constant.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L] Input token ids.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>(torch.Tensor) -&gt; [B, L, E] Embedded tokens.</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/embed.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Embed input tokens</span>

<span class="sd">    Assign embedding that corresponds to each token.</span>
<span class="sd">    Optionally add Gaussian noise and embedding dropout and scale embeddings by a constant.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L] Input token ids.</span>

<span class="sd">    Returns:</span>
<span class="sd">        (torch.Tensor) -&gt; [B, L, E] Embedded tokens.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="o">.</span><span class="n">stddev</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">noise</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="o">.</span><span class="n">p</span> <span class="o">&gt;</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">embeddings</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">dropout</span><span class="p">(</span><span class="n">embeddings</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">embeddings</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">scale</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.embed.Embed.init_embeddings" class="doc doc-heading">
<code class="highlight language-python"><span class="n">init_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">,</span> <span class="n">trainable</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Initialize embeddings matrix with pretrained embeddings</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>weights</code></td>
        <td><code>ndarray</code></td>
        <td><p>pretrained embeddings</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>trainable</code></td>
        <td><code>bool</code></td>
        <td><p>Finetune embeddings?</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/embed.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">init_embeddings</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">weights</span><span class="p">:</span> <span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">trainable</span><span class="p">:</span> <span class="nb">bool</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize embeddings matrix with pretrained embeddings</span>

<span class="sd">    Args:</span>
<span class="sd">        weights (np.ndarray): pretrained embeddings</span>
<span class="sd">        trainable (bool): Finetune embeddings?</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">embedding</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">from_numpy</span><span class="p">(</span><span class="n">weights</span><span class="p">),</span> <span class="n">requires_grad</span><span class="o">=</span><span class="n">trainable</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.embed.PositionalEncoding" class="doc doc-heading">
        <code>PositionalEncoding</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.embed.PositionalEncoding.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="o">=</span><span class="mi">512</span><span class="p">,</span> <span class="n">max_len</span><span class="o">=</span><span class="mi">5000</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Inject some information about the relative or absolute position of the tokens in the sequence.</p>
<p>The positional encodings have the same dimension as
the embeddings, so that the two can be summed. Here, we use sine and cosine
functions of different frequencies.</p>
<p>PE for even positions:</p>
<div class="arithmatex">\[\text{PosEncoder}(pos, 2i) = sin(\frac{pos}{10000^{\frac{2i}{d}}})\]</div>
<p>PE for odd positions:</p>
<div class="arithmatex">\[\text{PosEncoder}(pos, 2i+1) = cos(\frac{pos}{10000^{\frac{2i}{d}}})\]</div>
<p>where <span class="arithmatex">\(pos\)</span> is the word position and <span class="arithmatex">\(i\)</span> is the embedding idx</p>
<p>Implementation modified from pytorch/examples/word_language_model.py</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>embedding_dim</code></td>
        <td><code>int</code></td>
        <td><p>Embedding / model dimension. Defaults to 512.</p></td>
        <td><code>512</code></td>
      </tr>
      <tr>
        <td><code>max_len</code></td>
        <td><code>int</code></td>
        <td><p>Maximum sequence length that can be encoded. Defaults to 5000.</p></td>
        <td><code>5000</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/embed.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">512</span><span class="p">,</span> <span class="n">max_len</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">5000</span><span class="p">):</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Inject some information about the relative or absolute position of the tokens in the sequence.</span>

<span class="sd">    The positional encodings have the same dimension as</span>
<span class="sd">    the embeddings, so that the two can be summed. Here, we use sine and cosine</span>
<span class="sd">    functions of different frequencies.</span>

<span class="sd">    PE for even positions:</span>

<span class="sd">    $$\text{PosEncoder}(pos, 2i) = sin(\frac{pos}{10000^{\frac{2i}{d}}})$$</span>

<span class="sd">    PE for odd positions:</span>

<span class="sd">    $$\text{PosEncoder}(pos, 2i+1) = cos(\frac{pos}{10000^{\frac{2i}{d}}})$$</span>

<span class="sd">    where $pos$ is the word position and $i$ is the embedding idx</span>

<span class="sd">    Implementation modified from pytorch/examples/word_language_model.py</span>

<span class="sd">    Args:</span>
<span class="sd">        embedding_dim (int): Embedding / model dimension. Defaults to 512.</span>
<span class="sd">        max_len (int): Maximum sequence length that can be encoded. Defaults to 5000.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PositionalEncoding</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">max_len</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">)</span>
    <span class="n">position</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">max_len</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">torch</span><span class="o">.</span><span class="n">float</span><span class="p">)</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="n">div_term</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span>
        <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">embedding_dim</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
        <span class="o">*</span> <span class="p">(</span><span class="o">-</span><span class="n">math</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="mf">10000.0</span><span class="p">)</span> <span class="o">/</span> <span class="n">embedding_dim</span><span class="p">)</span>
    <span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">::</span><span class="mi">2</span><span class="p">]</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">position</span> <span class="o">*</span> <span class="n">div_term</span><span class="p">)</span>
    <span class="n">pe</span> <span class="o">=</span> <span class="n">pe</span><span class="o">.</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">register_buffer</span><span class="p">(</span><span class="s2">&quot;pe&quot;</span><span class="p">,</span> <span class="n">pe</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.embed.PositionalEncoding.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Calculate positional embeddings for input and add them to input tensor</p>
<div class="arithmatex">\[out = x + PosEmbed(x)\]</div>
<p>x is assumed to be batch first</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, D] input embeddings</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: Embeddings + positional embeddings</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/embed.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Calculate positional embeddings for input and add them to input tensor</span>

<span class="sd">    $$out = x + PosEmbed(x)$$</span>

<span class="sd">    x is assumed to be batch first</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, D] input embeddings</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: Embeddings + positional embeddings</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">[:,</span> <span class="p">:</span> <span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">),</span> <span class="p">:]</span>  <span class="c1"># type: ignore</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.feedforward"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-class">



<h2 id="slp.modules.feedforward.PositionwiseFF" class="doc doc-heading">
        <code>PositionwiseFF</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedforward.PositionwiseFF.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">gelu</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Transformer Position-wise feed-forward layer</p>
<p>Linear -&gt; LayerNorm -&gt; ReLU -&gt; Linear</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>d_model</code></td>
        <td><code>int</code></td>
        <td><p>Model dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>d_ff</code></td>
        <td><code>int</code></td>
        <td><p>Hidden dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability. Defaults to 0.1.</p></td>
        <td><code>0.1</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedforward.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">d_model</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span> <span class="n">gelu</span><span class="o">=</span><span class="kc">False</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Transformer Position-wise feed-forward layer</span>

<span class="sd">    Linear -&gt; LayerNorm -&gt; ReLU -&gt; Linear</span>

<span class="sd">    Args:</span>
<span class="sd">        d_model (int): Model dimension</span>
<span class="sd">        d_ff (int): Hidden dimension</span>
<span class="sd">        dropout (float): Dropout probability. Defaults to 0.1.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">PositionwiseFF</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_model</span><span class="p">,</span> <span class="n">d_ff</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">ff2</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="n">d_ff</span><span class="p">,</span> <span class="n">d_model</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">activation</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">ReLU</span><span class="p">()</span> <span class="k">if</span> <span class="ow">not</span> <span class="n">gelu</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">GELU</span><span class="p">()</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedforward.PositionwiseFF.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Position-wise FF forward pass</p>
<div class="arithmatex">\[out = W_2 \dot max(0, W_1 \dot x + b_1) + b_2\]</div>
<p>[B, <em>, D] -&gt; [B, </em>, H] -&gt; [B, *, D]</p>
<ul>
<li>B: Batch size</li>
<li>D: Model dim</li>
<li>H: Hidden size &gt; Model dim (Usually <span class="arithmatex">\(H = 2D\)</span>)</li>
</ul>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, *, D] Input features</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td><p>torch.Tensor: [B, *, D] Output features</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/feedforward.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sa">r</span><span class="sd">&quot;&quot;&quot;Position-wise FF forward pass</span>

<span class="sd">    $$out = W_2 \dot max(0, W_1 \dot x + b_1) + b_2$$</span>

<span class="sd">    [B, *, D] -&gt; [B, *, H] -&gt; [B, *, D]</span>

<span class="sd">    * B: Batch size</span>
<span class="sd">    * D: Model dim</span>
<span class="sd">    * H: Hidden size &gt; Model dim (Usually $H = 2D$)</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, *, D] Input features</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, *, D] Output features</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">ff2</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">activation</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">ff1</span><span class="p">(</span><span class="n">x</span><span class="p">))))</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.feedforward.TwoLayer" class="doc doc-heading">
        <code>TwoLayer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.feedforward.TwoLayer.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/feedforward.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">act</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">residual</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">x</span> <span class="o">+</span> <span class="n">out</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.norm"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-class">



<h2 id="slp.modules.norm.LayerNormTf" class="doc doc-heading">
        <code>LayerNormTf</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.norm.LayerNormTf.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">eps</span><span class="o">=</span><span class="mf">1e-12</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Construct a layernorm module in the TF style (epsilon inside the square root).
Link: https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py#L234</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/norm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span> <span class="n">eps</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">1e-12</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Construct a layernorm module in the TF style (epsilon inside the square root).</span>
<span class="sd">    Link: https://github.com/huggingface/pytorch-pretrained-BERT/blob/master/pytorch_pretrained_bert/modeling.py#L234</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">LayerNormTf</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bias</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Parameter</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">hidden_size</span><span class="p">))</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span> <span class="o">=</span> <span class="n">eps</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.norm.LayerNormTf.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Calculate Layernorm the tf way</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/norm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Calculate Layernorm the tf way&quot;&quot;&quot;</span>
    <span class="n">u</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">s</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span><span class="o">.</span><span class="n">pow</span><span class="p">(</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">u</span><span class="p">)</span> <span class="o">/</span> <span class="n">torch</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">s</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">variance_epsilon</span><span class="p">)</span>

    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">weight</span> <span class="o">*</span> <span class="n">x</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">bias</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.norm.ScaleNorm" class="doc doc-heading">
        <code>ScaleNorm</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.norm.ScaleNorm.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/norm.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">):</span>
    <span class="n">scaled_norm</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">g</span> <span class="o">/</span> <span class="n">safe_norm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">dim</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdim</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span><span class="o">.</span><span class="n">clamp</span><span class="p">(</span><span class="nb">min</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">eps</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">scaled_norm</span> <span class="o">*</span> <span class="n">x</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>








  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.regularization"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.regularization.GaussianNoise" class="doc doc-heading">
        <code>GaussianNoise</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.regularization.GaussianNoise.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stddev</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="mf">0.0</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>Additive Gaussian Noise layer</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>stddev</code></td>
        <td><code>float</code></td>
        <td><p>the standard deviation of the distribution</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>mean</code></td>
        <td><code>float</code></td>
        <td><p>the mean of the distribution</p></td>
        <td><code>0.0</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/regularization.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">stddev</span><span class="p">:</span> <span class="nb">float</span><span class="p">,</span> <span class="n">mean</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Additive Gaussian Noise layer</span>

<span class="sd">    Args:</span>
<span class="sd">        stddev (float): the standard deviation of the distribution</span>
<span class="sd">        mean (float): the mean of the distribution</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">stddev</span> <span class="o">=</span> <span class="n">stddev</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">mean</span> <span class="o">=</span> <span class="n">mean</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.regularization.GaussianNoise.__repr__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>String representation of class</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/regularization.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__repr__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;String representation of class&quot;&quot;&quot;</span>
    <span class="k">return</span> <span class="s2">&quot;</span><span class="si">{}</span><span class="s2"> (mean=</span><span class="si">{}</span><span class="s2">, stddev=</span><span class="si">{}</span><span class="s2">)&quot;</span><span class="o">.</span><span class="n">format</span><span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="vm">__class__</span><span class="o">.</span><span class="vm">__name__</span><span class="p">,</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">),</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">stddev</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.regularization.GaussianNoise.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Gaussian noise forward pass</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>Input features.</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tensor</code></td>
      <td></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/regularization.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">)</span> <span class="o">-&gt;</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">:</span>
    <span class="sd">&quot;&quot;&quot;Gaussian noise forward pass</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): Input features.</span>

<span class="sd">    Returns:</span>
<span class="sd">        [type]: [description]</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">training</span><span class="p">:</span>
        <span class="n">noise</span> <span class="o">=</span> <span class="n">Variable</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">new</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span><span class="o">.</span><span class="n">normal_</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">mean</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">stddev</span><span class="p">))</span>
        <span class="k">return</span> <span class="n">x</span> <span class="o">+</span> <span class="n">noise</span>
    <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.rnn"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.rnn.AttentiveRNN" class="doc doc-heading">
        <code>AttentiveRNN</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.AttentiveRNN.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">merge_bi</span><span class="o">=</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">,</span> <span class="n">packed_sequence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nystrom</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">33</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">return_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>RNN with embedding layer and optional attention mechanism</p>
<p>Single-headed scaled dot-product attention is used as an attention mechanism</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>input_size</code></td>
        <td><code>int</code></td>
        <td><p>Input features dimension</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>Hidden features</p></td>
        <td><code>256</code></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Use batch first representation type. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>layers</code></td>
        <td><code>int</code></td>
        <td><p>Number of RNN layers. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use bidirectional RNNs. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>merge_bi</code></td>
        <td><code>str</code></td>
        <td><p>How bidirectional states are merged. Defaults to "cat".</p></td>
        <td><code>&#39;cat&#39;</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability. Defaults to 0.0.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>rnn_type</code></td>
        <td><code>str</code></td>
        <td><p>lstm or gru. Defaults to "lstm".</p></td>
        <td><code>&#39;lstm&#39;</code></td>
      </tr>
      <tr>
        <td><code>packed_sequence</code></td>
        <td><code>bool</code></td>
        <td><p>Use packed sequences. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Maximum sequence length for fixed length padding. If -1 takes the
largest sequence length in this batch</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>attention</code></td>
        <td><code>bool</code></td>
        <td><p>Use attention mechanism. Defaults to False</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>Number of attention heads. If 1 uses single headed attention</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>nystrom</code></td>
        <td><code>bool</code></td>
        <td><p>Use nystrom approximation for multihead attention</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark sequence elements for nystrom attention</p></td>
        <td><code>32</code></td>
      </tr>
      <tr>
        <td><code>kernel_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Kernel size for multihead attention output residual convolution</p></td>
        <td><code>33</code></td>
      </tr>
      <tr>
        <td><code>inverse_iterations</code></td>
        <td><code>int</code></td>
        <td><p>Number of iterations for moore-penrose inverse approximation
in nystrom attention. 6 is a good value</p></td>
        <td><code>6</code></td>
      </tr>
      <tr>
        <td><code>return_hidden</code></td>
        <td><code>bool</code></td>
        <td><p>Return all hidden states. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">merge_bi</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">packed_sequence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">nystrom</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">33</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">return_hidden</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;RNN with embedding layer and optional attention mechanism</span>

<span class="sd">    Single-headed scaled dot-product attention is used as an attention mechanism</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Input features dimension</span>
<span class="sd">        hidden_size (int): Hidden features</span>
<span class="sd">        batch_first (bool): Use batch first representation type. Defaults to True.</span>
<span class="sd">        layers (int): Number of RNN layers. Defaults to 1.</span>
<span class="sd">        bidirectional (bool): Use bidirectional RNNs. Defaults to False.</span>
<span class="sd">        merge_bi (str): How bidirectional states are merged. Defaults to &quot;cat&quot;.</span>
<span class="sd">        dropout (float): Dropout probability. Defaults to 0.0.</span>
<span class="sd">        rnn_type (str): lstm or gru. Defaults to &quot;lstm&quot;.</span>
<span class="sd">        packed_sequence (bool): Use packed sequences. Defaults to True.</span>
<span class="sd">        max_length (int): Maximum sequence length for fixed length padding. If -1 takes the</span>
<span class="sd">            largest sequence length in this batch</span>
<span class="sd">        attention (bool): Use attention mechanism. Defaults to False</span>
<span class="sd">        num_heads (int): Number of attention heads. If 1 uses single headed attention</span>
<span class="sd">        nystrom (bool): Use nystrom approximation for multihead attention</span>
<span class="sd">        num_landmarks (int): Number of landmark sequence elements for nystrom attention</span>
<span class="sd">        kernel_size (int): Kernel size for multihead attention output residual convolution</span>
<span class="sd">        inverse_iterations (int): Number of iterations for moore-penrose inverse approximation</span>
<span class="sd">            in nystrom attention. 6 is a good value</span>
<span class="sd">        return_hidden (bool): Return all hidden states. Defaults to False.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">AttentiveRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">RNN</span><span class="p">(</span>
        <span class="n">input_size</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">merge_bi</span><span class="o">=</span><span class="n">merge_bi</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">packed_sequence</span><span class="o">=</span><span class="n">packed_sequence</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">out_size</span> <span class="o">=</span> <span class="p">(</span>
        <span class="n">hidden_size</span>
        <span class="k">if</span> <span class="ow">not</span> <span class="p">(</span><span class="n">bidirectional</span> <span class="ow">and</span> <span class="n">merge_bi</span> <span class="o">==</span> <span class="s2">&quot;cat&quot;</span><span class="p">)</span>
        <span class="k">else</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">hidden_size</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">return_hidden</span> <span class="o">=</span> <span class="n">return_hidden</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="kc">None</span>

    <span class="k">if</span> <span class="n">attention</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">num_heads</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">Attention</span><span class="p">(</span>
                <span class="n">attention_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_size</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span>
            <span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="o">=</span> <span class="n">MultiheadAttention</span><span class="p">(</span>  <span class="c1"># type: ignore</span>
                <span class="n">attention_size</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">out_size</span><span class="p">,</span>
                <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
                <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
                <span class="n">nystrom</span><span class="o">=</span><span class="n">nystrom</span><span class="p">,</span>
                <span class="n">num_landmarks</span><span class="o">=</span><span class="n">num_landmarks</span><span class="p">,</span>
                <span class="n">inverse_iterations</span><span class="o">=</span><span class="n">inverse_iterations</span><span class="p">,</span>
                <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
            <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.AttentiveRNN.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Attentive RNN forward pass</p>
<p>If self.attention=True then the outputs are the weighted sum of the RNN hidden states with the attention score weights
Else the output is the last hidden state of the RNN.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L] Input token ids</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B] Original sequence lengths</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]</code></td>
      <td><p>Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
    if return_hidden == False: Returns a tensor [B, H] or [B, 2<em>H] of output features to be used for classification
    if return_hidden == True: Returns a tensor [B, H] or [B, 2</em>H] of output features to
        be used for classification, and a tensor of all the hidden states</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Attentive RNN forward pass</span>

<span class="sd">    If self.attention=True then the outputs are the weighted sum of the RNN hidden states with the attention score weights</span>
<span class="sd">    Else the output is the last hidden state of the RNN.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L] Input token ids</span>
<span class="sd">        lengths (torch.Tensor): [B] Original sequence lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:</span>
<span class="sd">            if return_hidden == False: Returns a tensor [B, H] or [B, 2*H] of output features to be used for classification</span>
<span class="sd">            if return_hidden == True: Returns a tensor [B, H] or [B, 2*H] of output features to</span>
<span class="sd">                be used for classification, and a tensor of all the hidden states</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">states</span><span class="p">,</span> <span class="n">last_hidden</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

    <span class="n">out</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span> <span class="o">=</span> <span class="n">last_hidden</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">states</span><span class="p">,</span> <span class="n">_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">attention</span><span class="p">(</span>
            <span class="n">states</span><span class="p">,</span>
            <span class="n">attention_mask</span><span class="o">=</span><span class="n">pad_mask</span><span class="p">(</span>
                <span class="n">lengths</span><span class="p">,</span>
                <span class="n">max_length</span><span class="o">=</span><span class="n">states</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="k">else</span> <span class="n">states</span><span class="o">.</span><span class="n">size</span><span class="p">(</span><span class="mi">0</span><span class="p">),</span>
            <span class="p">),</span>
        <span class="p">)</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">states</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">return_hidden</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">states</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.rnn.RNN" class="doc doc-heading">
        <code>RNN</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">






  <div class="doc doc-object doc-attribute">



<h3 id="slp.modules.rnn.RNN.out_size" class="doc doc-heading">
<code class="highlight language-python"><span class="n">out_size</span><span class="p">:</span> <span class="nb">int</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-property"><code>property</code></small>
      <small class="doc doc-property doc-property-readonly"><code>readonly</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>RNN output features size</p>

<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>int</code></td>
      <td><p>int: RNN output features size</p></td>
    </tr>
  </tbody>
</table>    </div>

  </div>






  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.RNN.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">input_size</span><span class="p">,</span> <span class="n">hidden_size</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">merge_bi</span><span class="o">=</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">,</span> <span class="n">packed_sequence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>LSTM - GRU wrapper with packed sequence support and handling for bidirectional / last output states</p>
<p>It is recommended to run with batch_first=True because the rest of the code is built with this assumption</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>input_size</code></td>
        <td><code>int</code></td>
        <td><p>Input features.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>Hidden features.</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Use batch first representation type. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>layers</code></td>
        <td><code>int</code></td>
        <td><p>Number of RNN layers. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use bidirectional RNNs. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>merge_bi</code></td>
        <td><code>str</code></td>
        <td><p>How bidirectional states are merged. Defaults to "cat".</p></td>
        <td><code>&#39;cat&#39;</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability. Defaults to 0.0.</p></td>
        <td><code>0.0</code></td>
      </tr>
      <tr>
        <td><code>rnn_type</code></td>
        <td><code>str</code></td>
        <td><p>lstm or gru. Defaults to "lstm".</p></td>
        <td><code>&#39;lstm&#39;</code></td>
      </tr>
      <tr>
        <td><code>packed_sequence</code></td>
        <td><code>bool</code></td>
        <td><p>Use packed sequences. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">input_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">merge_bi</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">packed_sequence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;LSTM - GRU wrapper with packed sequence support and handling for bidirectional / last output states</span>

<span class="sd">    It is recommended to run with batch_first=True because the rest of the code is built with this assumption</span>

<span class="sd">    Args:</span>
<span class="sd">        input_size (int): Input features.</span>
<span class="sd">        hidden_size (int): Hidden features.</span>
<span class="sd">        batch_first (bool): Use batch first representation type. Defaults to True.</span>
<span class="sd">        layers (int): Number of RNN layers. Defaults to 1.</span>
<span class="sd">        bidirectional (bool): Use bidirectional RNNs. Defaults to False.</span>
<span class="sd">        merge_bi (str): How bidirectional states are merged. Defaults to &quot;cat&quot;.</span>
<span class="sd">        dropout (float): Dropout probability. Defaults to 0.0.</span>
<span class="sd">        rnn_type (str): lstm or gru. Defaults to &quot;lstm&quot;.</span>
<span class="sd">        packed_sequence (bool): Use packed sequences. Defaults to True.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">RNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">bidirectional</span> <span class="o">=</span> <span class="n">bidirectional</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">hidden_size</span> <span class="o">=</span> <span class="n">hidden_size</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">batch_first</span> <span class="o">=</span> <span class="n">batch_first</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">merge_bi</span> <span class="o">=</span> <span class="n">merge_bi</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">=</span> <span class="n">rnn_type</span><span class="o">.</span><span class="n">lower</span><span class="p">()</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">batch_first</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;You are running RNN with batch_first=False. Make sure this is really what you want&quot;</span>
        <span class="p">)</span>

    <span class="k">if</span> <span class="ow">not</span> <span class="n">packed_sequence</span><span class="p">:</span>
        <span class="n">logger</span><span class="o">.</span><span class="n">warning</span><span class="p">(</span>
            <span class="s2">&quot;You have set packed_sequence=False. Running with packed_sequence=True will be much faster&quot;</span>
        <span class="p">)</span>

    <span class="n">rnn_cls</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">LSTM</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn_type</span> <span class="o">==</span> <span class="s2">&quot;lstm&quot;</span> <span class="k">else</span> <span class="n">nn</span><span class="o">.</span><span class="n">GRU</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span> <span class="o">=</span> <span class="n">rnn_cls</span><span class="p">(</span>
        <span class="n">input_size</span><span class="p">,</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span>
        <span class="n">num_layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">drop</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Dropout</span><span class="p">(</span><span class="n">dropout</span><span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">packed_sequence</span> <span class="o">=</span> <span class="n">packed_sequence</span>

    <span class="k">if</span> <span class="n">packed_sequence</span><span class="p">:</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pack</span> <span class="o">=</span> <span class="n">PackSequence</span><span class="p">(</span><span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">unpack</span> <span class="o">=</span> <span class="n">PadPackedSequence</span><span class="p">(</span>
            <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span>
        <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.RNN.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>RNN forward pass</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L, D] Input features</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B] Original sequence lengths</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Tuple[torch.Tensor, torch.Tensor, Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]]</code></td>
      <td><p>Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: (
    merged forward and backward states [B, L, H] or [B, L, 2<em>H],
    merged last forward and backward state [B, H] or [B, 2</em>H],
    hidden states tuple of [num_layers * num_directions, B, H] for LSTM or tensor [num_layers * num_directions, B, H] for GRU
)</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Tuple</span><span class="p">[</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span>
    <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]],</span>
<span class="p">]:</span>
    <span class="sd">&quot;&quot;&quot;RNN forward pass</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L, D] Input features</span>
<span class="sd">        lengths (torch.Tensor): [B] Original sequence lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        Tuple[torch.Tensor, torch.Tensor, torch.Tensor]: (</span>
<span class="sd">            merged forward and backward states [B, L, H] or [B, L, 2*H],</span>
<span class="sd">            merged last forward and backward state [B, H] or [B, 2*H],</span>
<span class="sd">            hidden states tuple of [num_layers * num_directions, B, H] for LSTM or tensor [num_layers * num_directions, B, H] for GRU</span>
<span class="sd">        )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="o">.</span><span class="n">flatten_parameters</span><span class="p">()</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">packed_sequence</span><span class="p">:</span>
        <span class="c1"># Latest pytorch allows only cpu tensors for packed sequence</span>
        <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="s2">&quot;cpu&quot;</span><span class="p">)</span>
        <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pack</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
    <span class="n">out</span><span class="p">,</span> <span class="n">hidden</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">rnn</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">packed_sequence</span><span class="p">:</span>
        <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">unpack</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">lengths</span> <span class="o">=</span> <span class="n">lengths</span><span class="o">.</span><span class="n">to</span><span class="p">(</span><span class="n">out</span><span class="o">.</span><span class="n">device</span><span class="p">)</span>

    <span class="n">out</span><span class="p">,</span> <span class="n">last_timestep</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">_final_output</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span><span class="p">,</span> <span class="n">last_timestep</span><span class="p">,</span> <span class="n">hidden</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.rnn.TokenRNN" class="doc doc-heading">
        <code>TokenRNN</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">









  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.TokenRNN.__init__" class="doc doc-heading">
<code class="highlight language-python"><span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">hidden_size</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">vocab_size</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings_dim</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">embeddings_dropout</span><span class="o">=</span><span class="mf">0.0</span><span class="p">,</span> <span class="n">finetune_embeddings</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">batch_first</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">layers</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">bidirectional</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">merge_bi</span><span class="o">=</span><span class="s1">&#39;cat&#39;</span><span class="p">,</span> <span class="n">dropout</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">rnn_type</span><span class="o">=</span><span class="s1">&#39;lstm&#39;</span><span class="p">,</span> <span class="n">packed_sequence</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">attention</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">max_length</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">num_heads</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">nystrom</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">num_landmarks</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="mi">33</span><span class="p">,</span> <span class="n">inverse_iterations</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">return_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span></code>

  <span class="doc doc-properties">
      <small class="doc doc-property doc-property-special"><code>special</code></small>
  </span>

</h3>

    <div class="doc doc-contents ">

      <p>RNN with embedding layer and optional attention mechanism</p>
<p>Single-headed scaled dot-product attention is used as an attention mechanism</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>hidden_size</code></td>
        <td><code>int</code></td>
        <td><p>Hidden features</p></td>
        <td><code>256</code></td>
      </tr>
      <tr>
        <td><code>vocab_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Vocabulary size. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings_dim</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Embedding dimension. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings</code></td>
        <td><code>Optional[numpy.ndarray]</code></td>
        <td><p>Embedding matrix. Defaults to None.</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>embeddings_dropout</code></td>
        <td><code>float</code></td>
        <td><p>Embedding dropout probability. Defaults to 0.0.</p></td>
        <td><code>0.0</code></td>
      </tr>
      <tr>
        <td><code>finetune_embeddings</code></td>
        <td><code>bool</code></td>
        <td><p>Finetune embeddings? Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>batch_first</code></td>
        <td><code>bool</code></td>
        <td><p>Use batch first representation type. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>layers</code></td>
        <td><code>int</code></td>
        <td><p>Number of RNN layers. Defaults to 1.</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>bidirectional</code></td>
        <td><code>bool</code></td>
        <td><p>Use bidirectional RNNs. Defaults to False.</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>merge_bi</code></td>
        <td><code>str</code></td>
        <td><p>How bidirectional states are merged. Defaults to "cat".</p></td>
        <td><code>&#39;cat&#39;</code></td>
      </tr>
      <tr>
        <td><code>dropout</code></td>
        <td><code>float</code></td>
        <td><p>Dropout probability. Defaults to 0.0.</p></td>
        <td><code>0.1</code></td>
      </tr>
      <tr>
        <td><code>rnn_type</code></td>
        <td><code>str</code></td>
        <td><p>lstm or gru. Defaults to "lstm".</p></td>
        <td><code>&#39;lstm&#39;</code></td>
      </tr>
      <tr>
        <td><code>packed_sequence</code></td>
        <td><code>bool</code></td>
        <td><p>Use packed sequences. Defaults to True.</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>max_length</code></td>
        <td><code>int</code></td>
        <td><p>Maximum sequence length for fixed length padding. If -1 takes the
largest sequence length in this batch</p></td>
        <td><code>-1</code></td>
      </tr>
      <tr>
        <td><code>attention</code></td>
        <td><code>bool</code></td>
        <td><p>Use attention mechanism. Defaults to False</p></td>
        <td><code>False</code></td>
      </tr>
      <tr>
        <td><code>num_heads</code></td>
        <td><code>int</code></td>
        <td><p>Number of attention heads. If 1 uses single headed attention</p></td>
        <td><code>1</code></td>
      </tr>
      <tr>
        <td><code>nystrom</code></td>
        <td><code>bool</code></td>
        <td><p>Use nystrom approximation for multihead attention</p></td>
        <td><code>True</code></td>
      </tr>
      <tr>
        <td><code>num_landmarks</code></td>
        <td><code>int</code></td>
        <td><p>Number of landmark sequence elements for nystrom attention</p></td>
        <td><code>32</code></td>
      </tr>
      <tr>
        <td><code>kernel_size</code></td>
        <td><code>Optional[int]</code></td>
        <td><p>Kernel size for multihead attention output residual convolution</p></td>
        <td><code>33</code></td>
      </tr>
      <tr>
        <td><code>inverse_iterations</code></td>
        <td><code>int</code></td>
        <td><p>Number of iterations for moore-penrose inverse approximation
in nystrom attention. 6 is a good value</p></td>
        <td><code>6</code></td>
      </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span>
    <span class="n">hidden_size</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">256</span><span class="p">,</span>
    <span class="n">vocab_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings_dim</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">]</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">embeddings_dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.0</span><span class="p">,</span>
    <span class="n">finetune_embeddings</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">batch_first</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">layers</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">bidirectional</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">merge_bi</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;cat&quot;</span><span class="p">,</span>
    <span class="n">dropout</span><span class="p">:</span> <span class="nb">float</span> <span class="o">=</span> <span class="mf">0.1</span><span class="p">,</span>
    <span class="n">rnn_type</span><span class="p">:</span> <span class="nb">str</span> <span class="o">=</span> <span class="s2">&quot;lstm&quot;</span><span class="p">,</span>
    <span class="n">packed_sequence</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">attention</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">False</span><span class="p">,</span>
    <span class="n">max_length</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="o">-</span><span class="mi">1</span><span class="p">,</span>
    <span class="n">num_heads</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">1</span><span class="p">,</span>
    <span class="n">nystrom</span><span class="p">:</span> <span class="nb">bool</span> <span class="o">=</span> <span class="kc">True</span><span class="p">,</span>
    <span class="n">num_landmarks</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">32</span><span class="p">,</span>
    <span class="n">kernel_size</span><span class="p">:</span> <span class="n">Optional</span><span class="p">[</span><span class="nb">int</span><span class="p">]</span> <span class="o">=</span> <span class="mi">33</span><span class="p">,</span>
    <span class="n">inverse_iterations</span><span class="p">:</span> <span class="nb">int</span> <span class="o">=</span> <span class="mi">6</span><span class="p">,</span>
    <span class="n">return_hidden</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;RNN with embedding layer and optional attention mechanism</span>

<span class="sd">    Single-headed scaled dot-product attention is used as an attention mechanism</span>

<span class="sd">    Args:</span>
<span class="sd">        hidden_size (int): Hidden features</span>
<span class="sd">        vocab_size (Optional[int]): Vocabulary size. Defaults to None.</span>
<span class="sd">        embeddings_dim (Optional[int]): Embedding dimension. Defaults to None.</span>
<span class="sd">        embeddings (Optional[np.ndarray]): Embedding matrix. Defaults to None.</span>
<span class="sd">        embeddings_dropout (float): Embedding dropout probability. Defaults to 0.0.</span>
<span class="sd">        finetune_embeddings (bool): Finetune embeddings? Defaults to False.</span>
<span class="sd">        batch_first (bool): Use batch first representation type. Defaults to True.</span>
<span class="sd">        layers (int): Number of RNN layers. Defaults to 1.</span>
<span class="sd">        bidirectional (bool): Use bidirectional RNNs. Defaults to False.</span>
<span class="sd">        merge_bi (str): How bidirectional states are merged. Defaults to &quot;cat&quot;.</span>
<span class="sd">        dropout (float): Dropout probability. Defaults to 0.0.</span>
<span class="sd">        rnn_type (str): lstm or gru. Defaults to &quot;lstm&quot;.</span>
<span class="sd">        packed_sequence (bool): Use packed sequences. Defaults to True.</span>
<span class="sd">        max_length (int): Maximum sequence length for fixed length padding. If -1 takes the</span>
<span class="sd">            largest sequence length in this batch</span>
<span class="sd">        attention (bool): Use attention mechanism. Defaults to False</span>
<span class="sd">        num_heads (int): Number of attention heads. If 1 uses single headed attention</span>
<span class="sd">        nystrom (bool): Use nystrom approximation for multihead attention</span>
<span class="sd">        num_landmarks (int): Number of landmark sequence elements for nystrom attention</span>
<span class="sd">        kernel_size (int): Kernel size for multihead attention output residual convolution</span>
<span class="sd">        inverse_iterations (int): Number of iterations for moore-penrose inverse approximation</span>
<span class="sd">            in nystrom attention. 6 is a good value</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="nb">super</span><span class="p">(</span><span class="n">TokenRNN</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>

    <span class="k">if</span> <span class="n">embeddings</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">finetune_embeddings</span> <span class="o">=</span> <span class="kc">True</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">vocab_size</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;You should either pass an embeddings matrix or vocab size&quot;</span>
        <span class="k">assert</span> <span class="p">(</span>
            <span class="n">embeddings_dim</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span>
        <span class="p">),</span> <span class="s2">&quot;You should either pass an embeddings matrix or embeddings_dim&quot;</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">vocab_size</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
        <span class="n">embeddings_dim</span> <span class="o">=</span> <span class="n">embeddings</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">embed</span> <span class="o">=</span> <span class="n">Embed</span><span class="p">(</span>
        <span class="n">vocab_size</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">embeddings_dim</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">embeddings</span><span class="o">=</span><span class="n">embeddings</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">embeddings_dropout</span><span class="p">,</span>
        <span class="n">scale</span><span class="o">=</span><span class="n">hidden_size</span> <span class="o">**</span> <span class="mf">0.5</span><span class="p">,</span>
        <span class="n">trainable</span><span class="o">=</span><span class="n">finetune_embeddings</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span> <span class="o">=</span> <span class="n">AttentiveRNN</span><span class="p">(</span>
        <span class="n">embeddings_dim</span><span class="p">,</span>  <span class="c1"># type: ignore</span>
        <span class="n">hidden_size</span><span class="p">,</span>
        <span class="n">batch_first</span><span class="o">=</span><span class="n">batch_first</span><span class="p">,</span>
        <span class="n">layers</span><span class="o">=</span><span class="n">layers</span><span class="p">,</span>
        <span class="n">bidirectional</span><span class="o">=</span><span class="n">bidirectional</span><span class="p">,</span>
        <span class="n">merge_bi</span><span class="o">=</span><span class="n">merge_bi</span><span class="p">,</span>
        <span class="n">dropout</span><span class="o">=</span><span class="n">dropout</span><span class="p">,</span>
        <span class="n">rnn_type</span><span class="o">=</span><span class="n">rnn_type</span><span class="p">,</span>
        <span class="n">packed_sequence</span><span class="o">=</span><span class="n">packed_sequence</span><span class="p">,</span>
        <span class="n">attention</span><span class="o">=</span><span class="n">attention</span><span class="p">,</span>
        <span class="n">max_length</span><span class="o">=</span><span class="n">max_length</span><span class="p">,</span>
        <span class="n">num_heads</span><span class="o">=</span><span class="n">num_heads</span><span class="p">,</span>
        <span class="n">nystrom</span><span class="o">=</span><span class="n">nystrom</span><span class="p">,</span>
        <span class="n">num_landmarks</span><span class="o">=</span><span class="n">num_landmarks</span><span class="p">,</span>
        <span class="n">kernel_size</span><span class="o">=</span><span class="n">kernel_size</span><span class="p">,</span>
        <span class="n">inverse_iterations</span><span class="o">=</span><span class="n">inverse_iterations</span><span class="p">,</span>
        <span class="n">return_hidden</span><span class="o">=</span><span class="n">return_hidden</span><span class="p">,</span>
    <span class="p">)</span>

    <span class="bp">self</span><span class="o">.</span><span class="n">out_size</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="o">.</span><span class="n">out_size</span>
</code></pre></div>
        </details>
    </div>

  </div>



  <div class="doc doc-object doc-method">



<h3 id="slp.modules.rnn.TokenRNN.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Token RNN forward pass</p>
<p>If self.attention=True then the outputs are the weighted sum of the RNN hidden states with the attention score weights
Else the output is the last hidden state of the RNN.</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>x</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B, L] Input token ids</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>lengths</code></td>
        <td><code>Tensor</code></td>
        <td><p>[B] Original sequence lengths</p></td>
        <td><em>required</em></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]</code></td>
      <td><p>torch.Tensor: [B, H] or [B, 2*H] Output features to be used for classification</p></td>
    </tr>
  </tbody>
</table>
        <details class="quote">
          <summary>Source code in <code>slp/modules/rnn.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span>
    <span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">lengths</span><span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span>
<span class="p">)</span> <span class="o">-&gt;</span> <span class="n">Union</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">Tuple</span><span class="p">[</span><span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">,</span> <span class="n">torch</span><span class="o">.</span><span class="n">Tensor</span><span class="p">]]:</span>
    <span class="sd">&quot;&quot;&quot;Token RNN forward pass</span>

<span class="sd">    If self.attention=True then the outputs are the weighted sum of the RNN hidden states with the attention score weights</span>
<span class="sd">    Else the output is the last hidden state of the RNN.</span>

<span class="sd">    Args:</span>
<span class="sd">        x (torch.Tensor): [B, L] Input token ids</span>
<span class="sd">        lengths (torch.Tensor): [B] Original sequence lengths</span>

<span class="sd">    Returns:</span>
<span class="sd">        torch.Tensor: [B, H] or [B, 2*H] Output features to be used for classification</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">lengths</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>  <span class="c1"># type: ignore</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>







  </div>

    </div>

  </div>



  <div class="doc doc-object doc-module">

<a id="slp.modules.transformer"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">







  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Decoder" class="doc doc-heading">
        <code>Decoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Decoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>

    <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">:</span>
        <span class="n">target</span> <span class="o">=</span> <span class="n">l</span><span class="p">(</span>
            <span class="n">target</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="n">target_mask</span>
        <span class="p">)</span>

    <span class="k">return</span> <span class="n">target</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.DecoderLayer" class="doc doc-heading">
        <code>DecoderLayer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.DecoderLayer.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">targets</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">in_layer</span><span class="p">(</span><span class="n">targets</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">target_mask</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fuse_layer</span><span class="p">(</span><span class="n">encoded</span><span class="p">,</span> <span class="n">targets</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">out_layer</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Encoder" class="doc doc-heading">
        <code>Encoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Encoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">layer</span> <span class="ow">in</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">layer</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">x</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.EncoderDecoder" class="doc doc-heading">
        <code>EncoderDecoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.EncoderDecoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">encoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">encoder</span><span class="p">(</span><span class="n">source</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">)</span>
    <span class="n">decoded</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">decoder</span><span class="p">(</span>
        <span class="n">target</span><span class="p">,</span> <span class="n">encoded</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="n">target_mask</span>
    <span class="p">)</span>

    <span class="k">return</span> <span class="n">decoded</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.EncoderLayer" class="doc doc-heading">
        <code>EncoderLayer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.EncoderLayer.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l1</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">l2</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Sublayer1" class="doc doc-heading">
        <code>Sublayer1</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Sublayer1.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prenorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prenorm</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_postnorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Sublayer2" class="doc doc-heading">
        <code>Sublayer2</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Sublayer2.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">_prenorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prenorm</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_postnorm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Sublayer3" class="doc doc-heading">
        <code>Sublayer3</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Sublayer3.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">(</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">_prenorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">prenorm</span>
        <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">_postnorm</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span>
    <span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.Transformer" class="doc doc-heading">
        <code>Transformer</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.Transformer.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">source</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="c1"># Adding embeddings + pos embeddings</span>
    <span class="c1"># is done in PositionalEncoding class</span>
    <span class="n">source</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">(</span><span class="n">source</span><span class="p">)</span>
    <span class="n">target</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">(</span><span class="n">target</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_block</span><span class="p">(</span>
        <span class="n">source</span><span class="p">,</span> <span class="n">target</span><span class="p">,</span> <span class="n">source_mask</span><span class="o">=</span><span class="n">source_mask</span><span class="p">,</span> <span class="n">target_mask</span><span class="o">=</span><span class="n">target_mask</span>
    <span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">drop</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">out</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.TransformerSequenceEncoder" class="doc doc-heading">
        <code>TransformerSequenceEncoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.TransformerSequenceEncoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_norm</span><span class="p">:</span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">feature_norm</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>



  <div class="doc doc-object doc-class">



<h2 id="slp.modules.transformer.TransformerTokenSequenceEncoder" class="doc doc-heading">
        <code>TransformerTokenSequenceEncoder</code>



</h2>

    <div class="doc doc-contents ">





  <div class="doc doc-children">










  <div class="doc doc-object doc-method">



<h3 id="slp.modules.transformer.TransformerTokenSequenceEncoder.forward" class="doc doc-heading">
<code class="highlight language-python"><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h3>

    <div class="doc doc-contents ">

      <p>Defines the computation performed at every call.</p>
<p>Should be overridden by all subclasses.</p>
<p>.. note::
    Although the recipe for forward pass needs to be defined within
    this function, one should call the :class:<code>Module</code> instance afterwards
    instead of this since the former takes care of running the
    registered hooks while the latter silently ignores them.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">embed</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">pe</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">transformer_block</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">attention_mask</span><span class="o">=</span><span class="n">attention_mask</span><span class="p">)</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">dim</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">out</span>
</code></pre></div>
        </details>
    </div>

  </div>





  </div>

    </div>

  </div>




  <div class="doc doc-object doc-function">



<h2 id="slp.modules.transformer.reset_parameters" class="doc doc-heading">
<code class="highlight language-python"><span class="n">reset_parameters</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Initialize parameters in the transformer model.</p>

        <details class="quote">
          <summary>Source code in <code>slp/modules/transformer.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">reset_parameters</span><span class="p">(</span><span class="n">named_parameters</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="mf">1.0</span><span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Initialize parameters in the transformer model.&quot;&quot;&quot;</span>

    <span class="k">for</span> <span class="n">name</span><span class="p">,</span> <span class="n">p</span> <span class="ow">in</span> <span class="n">named_parameters</span><span class="p">:</span>
        <span class="k">if</span> <span class="n">p</span><span class="o">.</span><span class="n">dim</span><span class="p">()</span> <span class="o">&gt;</span> <span class="mi">1</span><span class="p">:</span>
            <span class="k">if</span> <span class="s2">&quot;weight&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">xavier_normal_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="n">gain</span><span class="o">=</span><span class="n">gain</span><span class="p">)</span>

            <span class="k">if</span> <span class="s2">&quot;bias&quot;</span> <span class="ow">in</span> <span class="n">name</span><span class="p">:</span>
                <span class="n">nn</span><span class="o">.</span><span class="n">init</span><span class="o">.</span><span class="n">constant_</span><span class="p">(</span><span class="n">p</span><span class="p">,</span> <span class="mf">0.0</span><span class="p">)</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="../plbind/" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Pytorch Lightning Bindings
            </div>
          </div>
        </a>
      
      
        <a href="../multimodal/" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Multimodal Modules
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../assets/javascripts/workers/search.fb4a9340.min.js", "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.5cf3e710.min.js"></script>
      
        <script src="../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>