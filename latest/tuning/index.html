
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
      
      <link rel="shortcut icon" href="../assets/images/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.0.5">
    
    
      
        <title>Hyperparameter tuning - slp Documentation</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.77f3fd56.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.7fa14f5b.min.css">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,400,400i,700%7CRoboto+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Roboto";--md-code-font-family:"Roboto Mono"}</style>
      
    
    
    
      <link rel="stylesheet" href="../assets/_mkdocstrings.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="none" data-md-color-accent="none">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#hyperparameter-tuning" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href=".." title="slp Documentation" class="md-header__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            slp Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Hyperparameter tuning
            
          </span>
        </div>
      </div>
    </div>
    <div class="md-header__options">
      
    </div>
    
      <label class="md-header__button md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
      </label>
      
<div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" data-md-state="active" required>
      <label class="md-search__icon md-icon" for="__search">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.516 6.516 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5z"/></svg>
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
      </label>
      <button type="reset" class="md-search__icon md-icon" aria-label="Clear" tabindex="-1">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41L17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12 19 6.41z"/></svg>
      </button>
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
    
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href=".." title="slp Documentation" class="md-nav__button md-logo" aria-label="slp Documentation">
      
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M12 8a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3 3 3 0 0 0 3 3m0 3.54C9.64 9.35 6.5 8 3 8v11c3.5 0 6.64 1.35 9 3.54 2.36-2.19 5.5-3.54 9-3.54V8c-3.5 0-6.64 1.35-9 3.54z"/></svg>

    </a>
    slp Documentation
  </label>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        slp
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../get-started/" class="md-nav__link">
        Getting started
      </a>
    </li>
  

    
      
      
      

  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Hyperparameter tuning
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Hyperparameter tuning
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slp.util.tuning" class="md-nav__link">
    slp.util.tuning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.tuning.run_tuning" class="md-nav__link">
    run_tuning()
  </a>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../config/" class="md-nav__link">
        Configuration
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../data-utils/" class="md-nav__link">
        Data manipulation
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../plbind/" class="md-nav__link">
        Pytorch Lightning Bindings
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../modules/" class="md-nav__link">
        Generic Modules
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../multimodal/" class="md-nav__link">
        Multimodal Modules
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../utils/" class="md-nav__link">
        SLP utility functions
      </a>
    </li>
  

    
      
      
      

  
  
  
    <li class="md-nav__item">
      <a href="../apiref/" class="md-nav__link">
        API reference
      </a>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#slp.util.tuning" class="md-nav__link">
    slp.util.tuning
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#slp.util.tuning.run_tuning" class="md-nav__link">
    run_tuning()
  </a>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                
                <h1 id="hyperparameter-tuning">Hyperparameter tuning</h1>
<p>We provide easy distributed hyperparameter tuning using Ray Tune. Let's walk through the
<code>examples/mnist_tune.py</code> script for a concrete example.</p>
<p>This script has been modified from <code>examples/mnist.py</code> that was presented in
<a href="../get-started/">Getting started</a>, to perform a distributed hyperparameter parameter tuning
run using Ray Tune and the SLP utilities.</p>
<p>First we refactor the model creation and training into a function, so that each worker is able to
instantiate and train a model.</p>
<pre><code class="language-python">from slp.plbind.trainer import make_trainer_for_ray_tune
...
def train_mnist(config, train=None, val=None):
    # Convert dictionary to omegaconf dictconfig object
    config = OmegaConf.create(config)

    # Create data module
    ldm = PLDataModuleFromDatasets(
        train, val=val, seed=config.seed, no_test_set=True, **config.data
    )

    # Create model, optimizer, criterion, scheduler
    model = Net(**config.model)

    optimizer = getattr(optim, config.optimizer)(model.parameters(), **config.optim)
    criterion = nn.CrossEntropyLoss()

    lr_scheduler = None

    if config.lr_scheduler:
        lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(
            optimizer, **config.lr_schedule
        )

    # Wrap in PLModule, &amp; configure metrics
    lm = PLModule(
        model,
        optimizer,
        criterion,
        lr_scheduler=lr_scheduler,
        metrics={
            &quot;acc&quot;: FromLogits(pl.metrics.classification.Accuracy())
        },  # Will log train_acc and val_acc
        hparams=config,
    )

    # Map Lightning metrics to ray tune metris.
    metrics_map = {&quot;accuracy&quot;: &quot;val_acc&quot;, &quot;validation_loss&quot;: &quot;val_loss&quot;}
    assert (
        config[&quot;tune&quot;][&quot;metric&quot;] in metrics_map.keys()
    ), &quot;Metrics mapping should contain the metric you are trying to optimize&quot;
    # Train model
    trainer = make_trainer_for_ray_tune(metrics_map=metrics_map, **config.trainer)

    trainer.fit(lm, datamodule=ldm)
</code></pre>
<p>This function is pretty similar to the <code>examples/mnist.py</code> code, but there are some important notes to keep in mind here.</p>
<ul>
<li>The function accepts a config dictionary as a positional argument and the train and validation
  datasets as keyword arguments. This is important, because <code>run_tuning</code> expects the input training
  function to have this stub.</li>
<li>We convert the input dict to an omegaconf.DictConfig object for convenience. Unfortunately ray
  tune is not able to pass around OmegaConf configuration objects so we need to convert back and
  forth.</li>
<li>Take note of the <code>metrics_map</code>. This mapping renames the metrics aggregated by our pytorch
  lightning module so that they are logged for ray tune. One of the keys of this dict is going to
  be chosen as the metric to optimize</li>
<li>We use <code>the make_trainer_for_ray_tune</code> To create a trainer that is configured specifically for a
  tuning run</li>
</ul>
<p>Next we define the <code>configure_search_space</code> function, that overrides entries in the configuration
file with ranges, from which ray tune will sample values for the hyperparameters.</p>
<pre><code class="language-python">def configure_search_space(config):
    config[&quot;model&quot;] = {
        &quot;intermediate_hidden&quot;: tune.choice([16, 32, 64, 100, 128, 256, 300, 512])
    }
    config[&quot;optimizer&quot;] = tune.choice([&quot;SGD&quot;, &quot;Adam&quot;, &quot;AdamW&quot;])
    config[&quot;optim&quot;][&quot;lr&quot;] = tune.loguniform(1e-4, 1e-1)
    config[&quot;optim&quot;][&quot;weight_decay&quot;] = tune.loguniform(1e-4, 1e-1)
    config[&quot;data&quot;][&quot;batch_size&quot;] = tune.choice([16, 32, 64, 128])

    return config
</code></pre>
<p>As you can see, we are going to tune the learning rate, weight decay, optimizer,
batch size and the hidden size of our model.</p>
<p><strong>Note</strong>: I considered abstracting this into a configuration
file, but I don't have any use case for this kind of abstraction and the simplicity and flexibility
we get from keeping this in code is more important.</p>
<p>Finally we parse the config file and the CLI arguments and spawn the hyperparameter tuning in the
main function:</p>
<pre><code class="language-python">from slp.util.tuning import run_tuning
...
if __name__ == &quot;__main__&quot;:
    config = ...
    train, val, _ = get_data()
    best_config = run_tuning(
        config,
        &quot;configs/best.mnist.tune.yml&quot;,
        train_mnist,
        configure_search_space,
        train,
        val,
    )
</code></pre>
<p>The <code>run_tuning</code> function accepts
* The parsed configuration as an <code>omegaconf.DictConfig</code> object
* A path to save the best trial configuration as a yaml file
* The <code>train_mnist</code> function
* The <code>configure_search_space</code> function
* The <code>train</code> dataset
* The <code>validation</code> dataset</p>
<p>Note that we create the train and validation splits by hand, so that each trial runs on the same
validation set.</p>
<p>The script can be called with the following arguments:</p>
<pre><code class="language-bash">python examples/minst_tune.py --num-trials 1000 --cpus_per_trial 1 --gpus_per_trial 0.12 --tune-metric accuracy --tune-mode max --epochs 20
</code></pre>
<p>This will spawn 1000 trials over as many gpus as we have available (in our server or in our cluster).
The <code>--gpus_per_trial</code> argument can be a floating point number. In this case we pack 7-8
experiments per GPU (RTX 2080Ti).</p>
<p>Note the <code>--tune-metric</code> argument corresponds to one of the keys in the <code>metrics_map</code> dictionary.
Here we run the tuning to optimize for validation accuracy.</p>
<p>After the run finishes we can use the <code>configs/best.mnist.tune.yml</code> configuration file to train and
evaluate the best model on the test set</p>
<pre><code class="language-bash">python examples/mnist.py --config configs/best.mnist.tune.yml
</code></pre>
<p><strong>Note</strong>: <code>mnist_tune.py</code> can accept any configuration file and command line argument that <code>mnist.py</code> can accept. Run <code>python minst_tune.py --help</code> for more information.</p>


  <div class="doc doc-object doc-module">

<a id="slp.util.tuning"></a>
    <div class="doc doc-contents first">




  <div class="doc doc-children">








  <div class="doc doc-object doc-function">



<h2 id="slp.util.tuning.run_tuning" class="doc doc-heading">
<code class="highlight language-python"><span class="n">run_tuning</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">output_config_file</span><span class="p">,</span> <span class="n">train_fn</span><span class="p">,</span> <span class="n">config_fn</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="kc">None</span><span class="p">)</span></code>


</h2>

    <div class="doc doc-contents ">

      <p>Run distributed hyperparameter tuning using ray tune</p>
<p>Uses Optuna TPE search algorithm and ASHA pruning strategy</p>

<p><strong>Parameters:</strong></p>
<table>
  <thead>
    <tr>
      <th>Name</th>
      <th>Type</th>
      <th>Description</th>
      <th>Default</th>
    </tr>
  </thead>
  <tbody>
      <tr>
        <td><code>config</code></td>
        <td><code>DictConfig</code></td>
        <td><p>The parsed configuration</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>output_config_file</code></td>
        <td><code>str</code></td>
        <td><p>Path to save the optimal configuration that yields the best
result</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>train_fn</code></td>
        <td><code>Callable[[Dict[str, Any], Any, Any], NoneType]</code></td>
        <td><p>Train function that takes the
configuration as a python dict, train dataset and validation dataset and fits the
model. This function is used to create the trainable that will run when calling
ray.tune.run</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>config_fn</code></td>
        <td><code>Callable[[Dict[str, Any]], Dict[str, Any]]</code></td>
        <td><p>Configuration function that
constructs the search space by overriding entries in the input configuration</p></td>
        <td><em>required</em></td>
      </tr>
      <tr>
        <td><code>train</code></td>
        <td><code>Any</code></td>
        <td><p>Torch dataset or corpus that will be used for training</p></td>
        <td><code>None</code></td>
      </tr>
      <tr>
        <td><code>val</code></td>
        <td><code>Any</code></td>
        <td><p>Torch dataset or corpus that will be used for validation</p></td>
        <td><code>None</code></td>
      </tr>
  </tbody>
</table>
<p><strong>Returns:</strong></p>
<table>
  <thead>
    <tr>
      <th>Type</th>
      <th>Description</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td><code>Dict[str, Any]</code></td>
      <td><p>The configuration for the best trial</p></td>
    </tr>
  </tbody>
</table>
<p><strong>Examples:</strong></p>
    <div class="highlight"><pre><span></span><code><span class="o">&gt;&gt;&gt;</span> <span class="c1"># Make search space</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">configure_search_space</span><span class="p">(</span><span class="n">config</span><span class="p">):</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">config</span><span class="p">[</span><span class="s2">&quot;optimizer&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="s2">&quot;SGD&quot;</span><span class="p">,</span> <span class="s2">&quot;Adam&quot;</span><span class="p">,</span> <span class="s2">&quot;AdamW&quot;</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">config</span><span class="p">[</span><span class="s2">&quot;optim&quot;</span><span class="p">][</span><span class="s2">&quot;lr&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">config</span><span class="p">[</span><span class="s2">&quot;optim&quot;</span><span class="p">][</span><span class="s2">&quot;weight_decay&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">loguniform</span><span class="p">(</span><span class="mf">1e-4</span><span class="p">,</span> <span class="mf">1e-1</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">config</span><span class="p">[</span><span class="s2">&quot;data&quot;</span><span class="p">][</span><span class="s2">&quot;batch_size&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">choice</span><span class="p">([</span><span class="mi">16</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">64</span><span class="p">,</span> <span class="mi">128</span><span class="p">])</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="k">return</span> <span class="n">config</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Training function.</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">def</span> <span class="nf">train_fn</span><span class="p">(</span><span class="n">config</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">config</span> <span class="o">=</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">create</span><span class="p">(</span><span class="n">config</span><span class="p">)</span> <span class="c1"># convert dict from ray tune to DictConfig</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">ldm</span> <span class="o">=</span> <span class="n">PLDataModuleFromDatasets</span><span class="p">(</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="n">val</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="n">config</span><span class="o">.</span><span class="n">seed</span><span class="p">,</span> <span class="n">no_test_set</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="o">.</span><span class="n">data</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">model</span> <span class="o">=</span> <span class="n">Net</span><span class="p">(</span><span class="o">**</span><span class="n">config</span><span class="o">.</span><span class="n">model</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">optimizer</span> <span class="o">=</span> <span class="nb">getattr</span><span class="p">(</span><span class="n">optim</span><span class="p">,</span> <span class="n">config</span><span class="o">.</span><span class="n">optimizer</span><span class="p">)(</span><span class="n">model</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="o">**</span><span class="n">config</span><span class="o">.</span><span class="n">optim</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">CrossEntropyLoss</span><span class="p">()</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">lm</span> <span class="o">=</span> <span class="n">PLModule</span><span class="p">(</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">model</span><span class="p">,</span> <span class="n">optimizer</span><span class="p">,</span> <span class="n">criterion</span><span class="p">,</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">hparams</span><span class="o">=</span><span class="n">config</span><span class="p">,</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">metrics</span><span class="o">=</span><span class="p">{</span><span class="s2">&quot;acc&quot;</span><span class="p">:</span> <span class="n">FromLogits</span><span class="p">(</span><span class="n">pl</span><span class="o">.</span><span class="n">metrics</span><span class="o">.</span><span class="n">classification</span><span class="o">.</span><span class="n">Accuracy</span><span class="p">())},</span> <span class="c1"># Logs train_acc and val_acc</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">metrics_map</span> <span class="o">=</span> <span class="p">{</span><span class="s2">&quot;accuracy&quot;</span><span class="p">:</span> <span class="s2">&quot;val_acc&quot;</span><span class="p">,</span> <span class="s2">&quot;validation_loss&quot;</span><span class="p">:</span> <span class="s2">&quot;val_loss&quot;</span><span class="p">}</span>  <span class="c1"># map metrics from pl to ray tune</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">trainer</span> <span class="o">=</span> <span class="n">make_trainer_for_ray_tune</span><span class="p">(</span><span class="n">metrics_map</span><span class="o">=</span><span class="n">metrics_map</span><span class="p">,</span> <span class="o">**</span><span class="n">config</span><span class="o">.</span><span class="n">trainer</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">trainer</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">lm</span><span class="p">,</span> <span class="n">datamodule</span><span class="o">=</span><span class="n">ldm</span><span class="p">)</span>
<span class="o">&gt;&gt;&gt;</span> <span class="c1"># Run optimization</span>
<span class="o">&gt;&gt;&gt;</span> <span class="k">if</span> <span class="vm">__name__</span> <span class="o">==</span> <span class="s2">&quot;__main__&quot;</span><span class="p">:</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">config</span><span class="p">,</span> <span class="n">train_dataset</span><span class="p">,</span> <span class="n">val_dataset</span> <span class="o">=</span> <span class="o">...</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="n">best_config</span> <span class="o">=</span> <span class="n">run_tuning</span><span class="p">(</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">config</span><span class="p">,</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="s2">&quot;configs/best.tuning.config.yml&quot;</span><span class="p">,</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">train_fn</span><span class="p">,</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">configure_search_space</span><span class="p">,</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">train_dataset</span><span class="p">,</span>
<span class="o">&gt;&gt;&gt;</span>         <span class="n">val_dataset</span><span class="p">,</span>
<span class="o">&gt;&gt;&gt;</span>     <span class="p">)</span>
</code></pre></div>

        <details class="quote">
          <summary>Source code in <code>slp/util/tuning.py</code></summary>
          <div class="highlight"><pre><span></span><code><span class="k">def</span> <span class="nf">run_tuning</span><span class="p">(</span>
    <span class="n">config</span><span class="p">:</span> <span class="n">DictConfig</span><span class="p">,</span>
    <span class="n">output_config_file</span><span class="p">:</span> <span class="nb">str</span><span class="p">,</span>
    <span class="n">train_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">Any</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="kc">None</span><span class="p">],</span>
    <span class="n">config_fn</span><span class="p">:</span> <span class="n">Callable</span><span class="p">[[</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span> <span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">]],</span>
    <span class="n">train</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
    <span class="n">val</span><span class="p">:</span> <span class="n">Any</span> <span class="o">=</span> <span class="kc">None</span><span class="p">,</span>
<span class="p">):</span>
    <span class="sd">&quot;&quot;&quot;Run distributed hyperparameter tuning using ray tune</span>

<span class="sd">    Uses Optuna TPE search algorithm and ASHA pruning strategy</span>

<span class="sd">    Args:</span>
<span class="sd">        config (omegaconf.DictConfig): The parsed configuration</span>
<span class="sd">        output_config_file (str): Path to save the optimal configuration that yields the best</span>
<span class="sd">            result</span>
<span class="sd">        train_fn (Callable[[Dict[str, Any], Any, Any], None]): Train function that takes the</span>
<span class="sd">            configuration as a python dict, train dataset and validation dataset and fits the</span>
<span class="sd">            model. This function is used to create the trainable that will run when calling</span>
<span class="sd">            ray.tune.run</span>
<span class="sd">        config_fn (Callable[[Dict[str, Any]], Dict[str, Any]]): Configuration function that</span>
<span class="sd">            constructs the search space by overriding entries in the input configuration</span>
<span class="sd">        train (Dataset): Torch dataset or corpus that will be used for training</span>
<span class="sd">        val (Dataset): Torch dataset or corpus that will be used for validation</span>

<span class="sd">    Returns:</span>
<span class="sd">        Dict[str, Any]: The configuration for the best trial</span>

<span class="sd">    Examples:</span>
<span class="sd">        &gt;&gt;&gt; # Make search space</span>
<span class="sd">        &gt;&gt;&gt; def configure_search_space(config):</span>
<span class="sd">        &gt;&gt;&gt;     config[&quot;optimizer&quot;] = tune.choice([&quot;SGD&quot;, &quot;Adam&quot;, &quot;AdamW&quot;])</span>
<span class="sd">        &gt;&gt;&gt;     config[&quot;optim&quot;][&quot;lr&quot;] = tune.loguniform(1e-4, 1e-1)</span>
<span class="sd">        &gt;&gt;&gt;     config[&quot;optim&quot;][&quot;weight_decay&quot;] = tune.loguniform(1e-4, 1e-1)</span>
<span class="sd">        &gt;&gt;&gt;     config[&quot;data&quot;][&quot;batch_size&quot;] = tune.choice([16, 32, 64, 128])</span>
<span class="sd">        &gt;&gt;&gt;     return config</span>
<span class="sd">        &gt;&gt;&gt; # Training function.</span>
<span class="sd">        &gt;&gt;&gt; def train_fn(config, train=None, val=None):</span>
<span class="sd">        &gt;&gt;&gt;     config = OmegaConf.create(config) # convert dict from ray tune to DictConfig</span>
<span class="sd">        &gt;&gt;&gt;     ldm = PLDataModuleFromDatasets(train, val=val, seed=config.seed, no_test_set=True, **config.data)</span>
<span class="sd">        &gt;&gt;&gt;     model = Net(**config.model)</span>
<span class="sd">        &gt;&gt;&gt;     optimizer = getattr(optim, config.optimizer)(model.parameters(), **config.optim)</span>
<span class="sd">        &gt;&gt;&gt;     criterion = nn.CrossEntropyLoss()</span>
<span class="sd">        &gt;&gt;&gt;     lm = PLModule(</span>
<span class="sd">        &gt;&gt;&gt;         model, optimizer, criterion,</span>
<span class="sd">        &gt;&gt;&gt;         hparams=config,</span>
<span class="sd">        &gt;&gt;&gt;         metrics={&quot;acc&quot;: FromLogits(pl.metrics.classification.Accuracy())}, # Logs train_acc and val_acc</span>
<span class="sd">        &gt;&gt;&gt;     )</span>
<span class="sd">        &gt;&gt;&gt;     metrics_map = {&quot;accuracy&quot;: &quot;val_acc&quot;, &quot;validation_loss&quot;: &quot;val_loss&quot;}  # map metrics from pl to ray tune</span>
<span class="sd">        &gt;&gt;&gt;     trainer = make_trainer_for_ray_tune(metrics_map=metrics_map, **config.trainer)</span>
<span class="sd">        &gt;&gt;&gt;     trainer.fit(lm, datamodule=ldm)</span>
<span class="sd">        &gt;&gt;&gt; # Run optimization</span>
<span class="sd">        &gt;&gt;&gt; if __name__ == &quot;__main__&quot;:</span>
<span class="sd">        &gt;&gt;&gt;     config, train_dataset, val_dataset = ...</span>
<span class="sd">        &gt;&gt;&gt;     best_config = run_tuning(</span>
<span class="sd">        &gt;&gt;&gt;         config,</span>
<span class="sd">        &gt;&gt;&gt;         &quot;configs/best.tuning.config.yml&quot;,</span>
<span class="sd">        &gt;&gt;&gt;         train_fn,</span>
<span class="sd">        &gt;&gt;&gt;         configure_search_space,</span>
<span class="sd">        &gt;&gt;&gt;         train_dataset,</span>
<span class="sd">        &gt;&gt;&gt;         val_dataset,</span>
<span class="sd">        &gt;&gt;&gt;     )</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">config</span> <span class="o">=</span> <span class="n">_extract_wandb_config</span><span class="p">(</span><span class="n">config</span><span class="p">)</span>
    <span class="n">cfg</span> <span class="o">=</span> <span class="n">config_fn</span><span class="p">(</span><span class="n">cast</span><span class="p">(</span><span class="n">Dict</span><span class="p">[</span><span class="nb">str</span><span class="p">,</span> <span class="n">Any</span><span class="p">],</span> <span class="n">OmegaConf</span><span class="o">.</span><span class="n">to_container</span><span class="p">(</span><span class="n">config</span><span class="p">)))</span>
    <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;trainer&quot;</span><span class="p">][</span><span class="s2">&quot;gpus&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">math</span><span class="o">.</span><span class="n">ceil</span><span class="p">(</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;tune&quot;</span><span class="p">][</span><span class="s2">&quot;gpus_per_trial&quot;</span><span class="p">])</span>
    <span class="n">trainable</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">with_parameters</span><span class="p">(</span><span class="n">train_fn</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="n">train</span><span class="p">,</span> <span class="n">val</span><span class="o">=</span><span class="n">val</span><span class="p">)</span>
    <span class="n">metric</span><span class="p">,</span> <span class="n">mode</span> <span class="o">=</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;tune&quot;</span><span class="p">][</span><span class="s2">&quot;metric&quot;</span><span class="p">],</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;tune&quot;</span><span class="p">][</span><span class="s2">&quot;mode&quot;</span><span class="p">]</span>

    <span class="n">analysis</span> <span class="o">=</span> <span class="n">tune</span><span class="o">.</span><span class="n">run</span><span class="p">(</span>
        <span class="n">trainable</span><span class="p">,</span>
        <span class="n">loggers</span><span class="o">=</span><span class="p">[</span>
            <span class="n">WandbLogger</span>
        <span class="p">],</span>  <span class="c1"># WandbLogger logs experiment configurations and metrics reported via tune.report() to W&amp;B Dashboard</span>
        <span class="n">resources_per_trial</span><span class="o">=</span><span class="p">{</span>
            <span class="s2">&quot;cpu&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;tune&quot;</span><span class="p">][</span><span class="s2">&quot;cpus_per_trial&quot;</span><span class="p">],</span>
            <span class="s2">&quot;gpu&quot;</span><span class="p">:</span> <span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;tune&quot;</span><span class="p">][</span><span class="s2">&quot;gpus_per_trial&quot;</span><span class="p">],</span>
        <span class="p">},</span>
        <span class="n">config</span><span class="o">=</span><span class="n">cfg</span><span class="p">,</span>
        <span class="n">max_failures</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span>
        <span class="n">num_samples</span><span class="o">=</span><span class="n">cfg</span><span class="p">[</span><span class="s2">&quot;tune&quot;</span><span class="p">][</span><span class="s2">&quot;num_trials&quot;</span><span class="p">],</span>
        <span class="n">search_alg</span><span class="o">=</span><span class="n">OptunaSearch</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">),</span>
        <span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span>
        <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">,</span>
        <span class="c1"># scheduler=tune.schedulers.ASHAScheduler(metric=metric, mode=mode, reduction_factor=2),</span>
        <span class="n">name</span><span class="o">=</span><span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">cfg</span><span class="p">[</span><span class="s1">&#39;trainer&#39;</span><span class="p">][</span><span class="s1">&#39;experiment_name&#39;</span><span class="p">]</span><span class="si">}</span><span class="s2">-tuning&quot;</span><span class="p">,</span>
    <span class="p">)</span>
    <span class="n">best_config</span> <span class="o">=</span> <span class="n">analysis</span><span class="o">.</span><span class="n">get_best_config</span><span class="p">(</span><span class="n">metric</span><span class="p">,</span> <span class="n">mode</span><span class="p">)</span>
    <span class="n">best_result</span> <span class="o">=</span> <span class="n">analysis</span><span class="o">.</span><span class="n">get_best_trial</span><span class="p">(</span><span class="n">metric</span><span class="o">=</span><span class="n">metric</span><span class="p">,</span> <span class="n">mode</span><span class="o">=</span><span class="n">mode</span><span class="p">)</span><span class="o">.</span><span class="n">last_result</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best hyperparameters found were: </span><span class="si">{</span><span class="n">best_config</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
    <span class="n">logger</span><span class="o">.</span><span class="n">info</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Best score: </span><span class="si">{</span><span class="n">best_result</span><span class="p">[</span><span class="n">metric</span><span class="p">]</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

    <span class="n">best_config</span><span class="p">[</span><span class="s2">&quot;tune&quot;</span><span class="p">][</span><span class="s2">&quot;result&quot;</span><span class="p">]</span> <span class="o">=</span> <span class="n">best_result</span>

    <span class="n">yaml_dump</span><span class="p">(</span><span class="n">best_config</span><span class="p">,</span> <span class="n">output_config_file</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">best_config</span>
</code></pre></div>
        </details>
    </div>

  </div>






  </div>

    </div>

  </div>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href="../get-started/" class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Getting started
            </div>
          </div>
        </a>
      
      
        <a href="../config/" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              Configuration
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
        
      </div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../assets/javascripts/workers/search.fb4a9340.min.js", "version": {"provider": "mike"}}</script>
    
    
      <script src="../assets/javascripts/bundle.5cf3e710.min.js"></script>
      
        <script src="../javascripts/config.js"></script>
      
        <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      
    
  </body>
</html>